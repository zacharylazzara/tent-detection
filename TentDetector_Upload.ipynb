{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TentDetector Upload.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TmJ00OGoP_mr",
        "SZz2bAEBQHxX",
        "dYTTLPwOBzU-"
      ],
      "authorship_tag": "ABX9TyPTUiw6N6TQikrKz8OkrwAc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zacharylazzara/tent-detection/blob/main/TentDetector_Upload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqRutHqon6Du"
      },
      "source": [
        "**Referenced Materials**\n",
        "\n",
        "* https://amaarora.github.io/2020/09/13/unet.html\n",
        "* https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5\n",
        "* https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n",
        "* https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "The majority of this comes from the referenced YouTube video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJ00OGoP_mr"
      },
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfDQrD8JoJwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6402ecda-5751-4e60-f8b8-1a016b77f3b5"
      },
      "source": [
        "# Imports\n",
        "!pip install segmentation-models-pytorch -q\n",
        "!pip install -U albumentations -q\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import csv\n",
        "import random\n",
        "import matplotlib.patches as mpatches\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as plticker\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import pickle\n",
        "import shutil\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 40 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 51 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 61 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 71 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 81 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 88 kB 3.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 78.5 MB/s \n",
            "\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 102 kB 13.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 85.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMwAUsCK4KUe",
        "outputId": "fc8966e4-ecfe-4e16-ade1-2147f1af518f"
      },
      "source": [
        "# Initialize Environment\n",
        "%env SRC_DIR        = sarpol-zahab-tents/\n",
        "%env DATA_DIR       = data/\n",
        "%env TRAIN_IMG_DIR  = data/train_images/\n",
        "%env TRAIN_LBL_DIR  = data/train_labels/\n",
        "%env VAL_IMG_DIR    = data/val_images/\n",
        "%env VAL_LBL_DIR    = data/val_labels/\n",
        "# %env PREDICT_DIR   = predictions/\n",
        "%env OUTPUT_DIR     = output/\n",
        "%env UPLOAD_DIR     = data/uploaded_images/\n",
        "# %env OUTPUT_MASK_DIR    = output/mask/\n",
        "# %env OUTPUT_DATA_DIR    = output/data/\n",
        "\n",
        "SRC_DIR       = os.environ.get(\"SRC_DIR\")\n",
        "DATA_DIR      = os.environ.get(\"DATA_DIR\")\n",
        "TRAIN_IMG_DIR = os.environ.get(\"TRAIN_IMG_DIR\")\n",
        "TRAIN_LBL_DIR = os.environ.get(\"TRAIN_LBL_DIR\")\n",
        "VAL_IMG_DIR   = os.environ.get(\"VAL_IMG_DIR\")\n",
        "VAL_LBL_DIR   = os.environ.get(\"VAL_LBL_DIR\")\n",
        "# PREDICT_DIR   = os.environ.get(\"PREDICT_DIR\")\n",
        "OUTPUT_DIR    = os.environ.get(\"OUTPUT_DIR\")\n",
        "# OUTPUT_MASK_DIR    = os.environ.get(\"OUTPUT_MASK_DIR\")\n",
        "# OUTPUT_DATA_DIR    = os.environ.get(\"OUTPUT_DATA_DIR\")\n",
        "\n",
        "\n",
        "\n",
        "UPLOAD_DIR    = os.environ.get(\"UPLOAD_DIR\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SRC_DIR=sarpol-zahab-tents/\n",
            "env: DATA_DIR=data/\n",
            "env: TRAIN_IMG_DIR=data/train_images/\n",
            "env: TRAIN_LBL_DIR=data/train_labels/\n",
            "env: VAL_IMG_DIR=data/val_images/\n",
            "env: VAL_LBL_DIR=data/val_labels/\n",
            "env: OUTPUT_DIR=output/\n",
            "env: UPLOAD_DIR=data/uploaded_images/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8HWwxdUvigf",
        "outputId": "79edb6a2-68cb-4ca5-c112-ce0ca1bfe4e1"
      },
      "source": [
        "# Initialize Directories\n",
        "%%bash\n",
        "cd /content\n",
        "echo \"Working in Directory: $(pwd)\"\n",
        "\n",
        "if [ -d 'sample_data' ]; then\n",
        "  rm -r sample_data\n",
        "fi\n",
        "\n",
        "if [ ! -d $SRC_DIR ]; then\n",
        " git clone https://github.com/tofighi/sarpol-zahab-tents.git\n",
        "fi\n",
        "\n",
        "if [ ! -d $DATA_DIR ]; then\n",
        "  mkdir -p $DATA_DIR\n",
        "  cp $SRC_DIR/data/sarpol_counts.csv $DATA_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $TRAIN_IMG_DIR ]; then\n",
        "  mkdir -p $TRAIN_IMG_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $TRAIN_LBL_DIR ]; then\n",
        "  mkdir -p $TRAIN_LBL_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $VAL_IMG_DIR ]; then\n",
        "  mkdir -p $VAL_IMG_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $VAL_LBL_DIR ]; then\n",
        "  mkdir -p $VAL_LBL_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $UPLOAD_DIR ]; then\n",
        "  mkdir -p $UPLOAD_DIR\n",
        "fi\n",
        "\n",
        "# if [ ! -d $PREDICT_DIR ]; then\n",
        "#   mkdir -p $PREDICT_DIR\n",
        "# fi\n",
        "\n",
        "# if [ ! -d $OUTPUT_MASK_DIR ]; then\n",
        "#   mkdir -p $OUTPUT_MASK_DIR\n",
        "# fi\n",
        "\n",
        "# if [ ! -d $OUTPUT_DATA_DIR ]; then\n",
        "#   mkdir -p $OUTPUT_DATA_DIR\n",
        "# fi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working in Directory: /content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'sarpol-zahab-tents'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZz2bAEBQHxX"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7WMebFa5Uyr"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Dataset\n",
        "TEST_SIZE = 0.3\n",
        "RANDOM_STATE = 123\n",
        "ALLOW_IRRELEVANT = False # If images don't have tents do we want to throw them out or not?\n",
        "SARPOL = False # do we want to download the very large Sarpol image?\n",
        "\n",
        "GRAYSCALE = False # Print tents as grayscale or not\n",
        "\n",
        "LIVE_VISUALIZE = False\n",
        "\n",
        "# TODO: lets just load all data as validation data to generate the map data and such\n",
        "\n",
        "TRAINING = False\n",
        "DATASET_MODE = False # If this is true, we won't use the uploaded image and will instead use the dataset\n",
        "\n",
        "\n",
        "TENT_CSV          = f\"{DATA_DIR}sarpol_counts.csv\"\n",
        "LOAD_FROM_CSV = True\n",
        "\n",
        "BLOB_LOCALIZATION = False\n",
        "KMEANS_LOCALIZATION = False\n",
        "\n",
        "# Display Limit\n",
        "DISP_LIMIT    = 1 # Maximum number of images to display\n",
        "DISP_RESULTS  = True\n",
        "DISP_SCALE    = 2#150 # Amount to integer divide displayed figure scale by (set to 1 to disable); useful if the notebook keeps crashing\n",
        "\n",
        "# Images\n",
        "IMG_FORMAT    = \"png\"\n",
        "BRIGHTNESS    = 0.5 # set to 1 for no dimming\n",
        "\n",
        "# Checkpoints\n",
        "CHECKPOINT    = \"checkpoint.pt\"\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4 # 1x10^-4 = 0.0001\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE    = 1#5 # Batch size defines the prediction batch; set to 1 if we want individual files\n",
        "NUM_EPOCHS    = 50 #100\n",
        "NUM_WORKERS   = 2\n",
        "IMAGE_HEIGHT  = 512\n",
        "IMAGE_WIDTH   = 512\n",
        "PIN_MEMORY    = True\n",
        "LOAD_MODEL    = True\n",
        "\n",
        "# Convolution Settings\n",
        "C_KERNEL      = 3     # This is the matrix that slides across the image (we define matrix size, so kernel = 3 means 3x3 matrix that slides across the image)\n",
        "C_STRIDE      = 1     # Number of pixels the kernel slides over the input (how many pixels we move the filter at a time)\n",
        "C_PADDING     = 1     # Sometimes the filter doesn't perfectly fit the input image, in which case we can pad with 0s or drop the part of the image that didn't fit (called valid padding)\n",
        "C_BIAS        = False # Bias is false in this case because we're using BatchNorm2d (bias would be canceled by the batch norm, so we set it to false)\n",
        "R_INPLACE     = True  #\n",
        "\n",
        "# UNet Settings\n",
        "IN_CHANNELS   = 3\n",
        "OUT_CHANNELS  = 1 # We're doing binary image segmentation (because our masks are black and white), so we can output a single channel\n",
        "U_FEATURES    = [64, 128, 256, 512] # Features come from the architecture (the number above the boxes)\n",
        "\n",
        "# UNet Pool Settings\n",
        "P_KERNEL      = 2\n",
        "P_STRIDE      = 2\n",
        "\n",
        "# Final layer kernel size\n",
        "F_KERNEL      = 1 # Because we're outputting the final image here\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy; this is used to prevent division by zero, so we want a small value that doesn't have much impact on the results\n",
        "EPSILON = sys.float_info.epsilon"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxu_27u7Qsdq"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-gP_DM19B2B"
      },
      "source": [
        "# Preprocessor Functions\n",
        "def load(path):\n",
        "  return np.array([(np.asarray(Image.open(path)), os.path.basename(path).split(f\".\", 1)[0]) for path in sorted(glob(path))])\n",
        "  \n",
        "def save_data(data, val=False):\n",
        "  img_dir = TRAIN_IMG_DIR\n",
        "  lbl_dir = TRAIN_LBL_DIR\n",
        "\n",
        "  if val:\n",
        "    img_dir = VAL_IMG_DIR\n",
        "    lbl_dir = VAL_LBL_DIR\n",
        "\n",
        "  data[\"img\"].save(f\"{img_dir}/{data['filename']['img']}\")\n",
        "  data[\"lbl\"].save(f\"{lbl_dir}/{data['filename']['lbl']}\")\n",
        "\n",
        "def percent(sample, total):\n",
        "  return f\"({sample}/{total}) = {((sample/total)*100):.2f}%\"\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUAznM-v8VOd",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46904385-ef99-480c-f324-18cd82755cbf"
      },
      "source": [
        "# def load_dataset(path):\n",
        "#   dataset = {}\n",
        "#   for path in sorted(glob(path)):\n",
        "#     mask = np.asarray(Image.open(path).convert('1'))\n",
        "#     if ALLOW_IRRELEVANT:\n",
        "#       dataset[os.path.basename(path).split(f\".\", 1)[0]] = mask\n",
        "#     elif np.sum(mask):\n",
        "#       dataset[os.path.basename(path).split(f\".\", 1)[0]] = mask\n",
        "#   return dataset\n",
        "\n",
        "\n",
        "if not DATASET_MODE:\n",
        "  %cd $UPLOAD_DIR\n",
        "  uploaded = files.upload()\n",
        "  for filename in uploaded.keys():\n",
        "    print(f\"Uploaded file \\\"{filename}\\\"\")\n",
        "  %cd /content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: make this use a dictionary instead, since that'll make it easier to get the image we want\n",
        "\n",
        "MAP = None\n",
        "if SARPOL:\n",
        "  !gdown --id 1-YUbFjwFL2G5r8TudKS0XK56BjJ0KsTK\n",
        "  MAP = load(\"sarpol.png\")[0][0]\n",
        "  print(f\"Sarpol Shape: {MAP.shape}\\n\")\n",
        "\n",
        "if LOAD_MODEL:\n",
        "  !gdown --id 1ulHrgSyNwo1X2WYmQyeb4lIxIIJ4Xs-G"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/uploaded_images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ec296de-26ab-4c12-b69a-a0e1d9871420\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ec296de-26ab-4c12-b69a-a0e1d9871420\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sarpol_001.jpg to sarpol_001.jpg\n",
            "Saving sarpol_002.jpg to sarpol_002.jpg\n",
            "Saving sarpol_003.jpg to sarpol_003.jpg\n",
            "Saving sarpol_004.jpg to sarpol_004.jpg\n",
            "Saving sarpol_005.jpg to sarpol_005.jpg\n",
            "Saving sarpol_006.jpg to sarpol_006.jpg\n",
            "Saving sarpol_007.jpg to sarpol_007.jpg\n",
            "Saving sarpol_008.jpg to sarpol_008.jpg\n",
            "Saving sarpol_009.jpg to sarpol_009.jpg\n",
            "Saving sarpol_010.jpg to sarpol_010.jpg\n",
            "Saving sarpol_011.jpg to sarpol_011.jpg\n",
            "Saving sarpol_012.jpg to sarpol_012.jpg\n",
            "Saving sarpol_013.jpg to sarpol_013.jpg\n",
            "Saving sarpol_014.jpg to sarpol_014.jpg\n",
            "Saving sarpol_015.jpg to sarpol_015.jpg\n",
            "Saving sarpol_016.jpg to sarpol_016.jpg\n",
            "Saving sarpol_017.jpg to sarpol_017.jpg\n",
            "Saving sarpol_018.jpg to sarpol_018.jpg\n",
            "Saving sarpol_019.jpg to sarpol_019.jpg\n",
            "Saving sarpol_020.jpg to sarpol_020.jpg\n",
            "Saving sarpol_021.jpg to sarpol_021.jpg\n",
            "Saving sarpol_022.jpg to sarpol_022.jpg\n",
            "Saving sarpol_023.jpg to sarpol_023.jpg\n",
            "Saving sarpol_024.jpg to sarpol_024.jpg\n",
            "Saving sarpol_025.jpg to sarpol_025.jpg\n",
            "Saving sarpol_026.jpg to sarpol_026.jpg\n",
            "Saving sarpol_027.jpg to sarpol_027.jpg\n",
            "Saving sarpol_028.jpg to sarpol_028.jpg\n",
            "Saving sarpol_029.jpg to sarpol_029.jpg\n",
            "Saving sarpol_030.jpg to sarpol_030.jpg\n",
            "Saving sarpol_031.jpg to sarpol_031.jpg\n",
            "Saving sarpol_032.jpg to sarpol_032.jpg\n",
            "Saving sarpol_033.jpg to sarpol_033.jpg\n",
            "Saving sarpol_034.jpg to sarpol_034.jpg\n",
            "Saving sarpol_035.jpg to sarpol_035.jpg\n",
            "Saving sarpol_036.jpg to sarpol_036.jpg\n",
            "Saving sarpol_037.jpg to sarpol_037.jpg\n",
            "Saving sarpol_038.jpg to sarpol_038.jpg\n",
            "Saving sarpol_039.jpg to sarpol_039.jpg\n",
            "Saving sarpol_040.jpg to sarpol_040.jpg\n",
            "Saving sarpol_041.jpg to sarpol_041.jpg\n",
            "Saving sarpol_042.jpg to sarpol_042.jpg\n",
            "Saving sarpol_043.jpg to sarpol_043.jpg\n",
            "Saving sarpol_044.jpg to sarpol_044.jpg\n",
            "Saving sarpol_045.jpg to sarpol_045.jpg\n",
            "Saving sarpol_046.jpg to sarpol_046.jpg\n",
            "Saving sarpol_047.jpg to sarpol_047.jpg\n",
            "Saving sarpol_048.jpg to sarpol_048.jpg\n",
            "Saving sarpol_049.jpg to sarpol_049.jpg\n",
            "Saving sarpol_050.jpg to sarpol_050.jpg\n",
            "Saving sarpol_051.jpg to sarpol_051.jpg\n",
            "Saving sarpol_052.jpg to sarpol_052.jpg\n",
            "Saving sarpol_053.jpg to sarpol_053.jpg\n",
            "Saving sarpol_054.jpg to sarpol_054.jpg\n",
            "Saving sarpol_055.jpg to sarpol_055.jpg\n",
            "Saving sarpol_056.jpg to sarpol_056.jpg\n",
            "Saving sarpol_057.jpg to sarpol_057.jpg\n",
            "Saving sarpol_058.jpg to sarpol_058.jpg\n",
            "Saving sarpol_059.jpg to sarpol_059.jpg\n",
            "Saving sarpol_060.jpg to sarpol_060.jpg\n",
            "Saving sarpol_061.jpg to sarpol_061.jpg\n",
            "Saving sarpol_062.jpg to sarpol_062.jpg\n",
            "Saving sarpol_063.jpg to sarpol_063.jpg\n",
            "Saving sarpol_064.jpg to sarpol_064.jpg\n",
            "Saving sarpol_065.jpg to sarpol_065.jpg\n",
            "Saving sarpol_066.jpg to sarpol_066.jpg\n",
            "Saving sarpol_067.jpg to sarpol_067.jpg\n",
            "Saving sarpol_068.jpg to sarpol_068.jpg\n",
            "Saving sarpol_069.jpg to sarpol_069.jpg\n",
            "Saving sarpol_070.jpg to sarpol_070.jpg\n",
            "Saving sarpol_071.jpg to sarpol_071.jpg\n",
            "Saving sarpol_072.jpg to sarpol_072.jpg\n",
            "Saving sarpol_073.jpg to sarpol_073.jpg\n",
            "Saving sarpol_074.jpg to sarpol_074.jpg\n",
            "Saving sarpol_075.jpg to sarpol_075.jpg\n",
            "Saving sarpol_076.jpg to sarpol_076.jpg\n",
            "Saving sarpol_077.jpg to sarpol_077.jpg\n",
            "Saving sarpol_078.jpg to sarpol_078.jpg\n",
            "Saving sarpol_079.jpg to sarpol_079.jpg\n",
            "Saving sarpol_080.jpg to sarpol_080.jpg\n",
            "Saving sarpol_081.jpg to sarpol_081.jpg\n",
            "Saving sarpol_082.jpg to sarpol_082.jpg\n",
            "Saving sarpol_083.jpg to sarpol_083.jpg\n",
            "Saving sarpol_084.jpg to sarpol_084.jpg\n",
            "Saving sarpol_085.jpg to sarpol_085.jpg\n",
            "Saving sarpol_086.jpg to sarpol_086.jpg\n",
            "Saving sarpol_087.jpg to sarpol_087.jpg\n",
            "Saving sarpol_088.jpg to sarpol_088.jpg\n",
            "Saving sarpol_089.jpg to sarpol_089.jpg\n",
            "Saving sarpol_090.jpg to sarpol_090.jpg\n",
            "Saving sarpol_091.jpg to sarpol_091.jpg\n",
            "Saving sarpol_092.jpg to sarpol_092.jpg\n",
            "Saving sarpol_093.jpg to sarpol_093.jpg\n",
            "Saving sarpol_094.jpg to sarpol_094.jpg\n",
            "Saving sarpol_095.jpg to sarpol_095.jpg\n",
            "Saving sarpol_096.jpg to sarpol_096.jpg\n",
            "Saving sarpol_097.jpg to sarpol_097.jpg\n",
            "Saving sarpol_098.jpg to sarpol_098.jpg\n",
            "Saving sarpol_099.jpg to sarpol_099.jpg\n",
            "Saving sarpol_100.jpg to sarpol_100.jpg\n",
            "Saving sarpol_101.jpg to sarpol_101.jpg\n",
            "Saving sarpol_102.jpg to sarpol_102.jpg\n",
            "Saving sarpol_103.jpg to sarpol_103.jpg\n",
            "Saving sarpol_104.jpg to sarpol_104.jpg\n",
            "Saving sarpol_105.jpg to sarpol_105.jpg\n",
            "Saving sarpol_106.jpg to sarpol_106.jpg\n",
            "Saving sarpol_107.jpg to sarpol_107.jpg\n",
            "Saving sarpol_108.jpg to sarpol_108.jpg\n",
            "Saving sarpol_109.jpg to sarpol_109.jpg\n",
            "Saving sarpol_110.jpg to sarpol_110.jpg\n",
            "Saving sarpol_111.jpg to sarpol_111.jpg\n",
            "Saving sarpol_112.jpg to sarpol_112.jpg\n",
            "Saving sarpol_113.jpg to sarpol_113.jpg\n",
            "Saving sarpol_114.jpg to sarpol_114.jpg\n",
            "Saving sarpol_115.jpg to sarpol_115.jpg\n",
            "Saving sarpol_116.jpg to sarpol_116.jpg\n",
            "Saving sarpol_117.jpg to sarpol_117.jpg\n",
            "Saving sarpol_118.jpg to sarpol_118.jpg\n",
            "Saving sarpol_119.jpg to sarpol_119.jpg\n",
            "Saving sarpol_120.jpg to sarpol_120.jpg\n",
            "Saving sarpol_121.jpg to sarpol_121.jpg\n",
            "Saving sarpol_122.jpg to sarpol_122.jpg\n",
            "Saving sarpol_123.jpg to sarpol_123.jpg\n",
            "Saving sarpol_124.jpg to sarpol_124.jpg\n",
            "Saving sarpol_125.jpg to sarpol_125.jpg\n",
            "Saving sarpol_126.jpg to sarpol_126.jpg\n",
            "Saving sarpol_127.jpg to sarpol_127.jpg\n",
            "Saving sarpol_128.jpg to sarpol_128.jpg\n",
            "Saving sarpol_129.jpg to sarpol_129.jpg\n",
            "Saving sarpol_130.jpg to sarpol_130.jpg\n",
            "Saving sarpol_131.jpg to sarpol_131.jpg\n",
            "Saving sarpol_132.jpg to sarpol_132.jpg\n",
            "Saving sarpol_133.jpg to sarpol_133.jpg\n",
            "Saving sarpol_134.jpg to sarpol_134.jpg\n",
            "Saving sarpol_135.jpg to sarpol_135.jpg\n",
            "Saving sarpol_136.jpg to sarpol_136.jpg\n",
            "Saving sarpol_137.jpg to sarpol_137.jpg\n",
            "Saving sarpol_138.jpg to sarpol_138.jpg\n",
            "Saving sarpol_139.jpg to sarpol_139.jpg\n",
            "Saving sarpol_140.jpg to sarpol_140.jpg\n",
            "Saving sarpol_141.jpg to sarpol_141.jpg\n",
            "Saving sarpol_142.jpg to sarpol_142.jpg\n",
            "Saving sarpol_143.jpg to sarpol_143.jpg\n",
            "Saving sarpol_144.jpg to sarpol_144.jpg\n",
            "Saving sarpol_145.jpg to sarpol_145.jpg\n",
            "Saving sarpol_146.jpg to sarpol_146.jpg\n",
            "Saving sarpol_147.jpg to sarpol_147.jpg\n",
            "Saving sarpol_148.jpg to sarpol_148.jpg\n",
            "Saving sarpol_149.jpg to sarpol_149.jpg\n",
            "Saving sarpol_150.jpg to sarpol_150.jpg\n",
            "Saving sarpol_151.jpg to sarpol_151.jpg\n",
            "Saving sarpol_152.jpg to sarpol_152.jpg\n",
            "Saving sarpol_153.jpg to sarpol_153.jpg\n",
            "Saving sarpol_154.jpg to sarpol_154.jpg\n",
            "Saving sarpol_155.jpg to sarpol_155.jpg\n",
            "Saving sarpol_156.jpg to sarpol_156.jpg\n",
            "Saving sarpol_157.jpg to sarpol_157.jpg\n",
            "Saving sarpol_158.jpg to sarpol_158.jpg\n",
            "Saving sarpol_159.jpg to sarpol_159.jpg\n",
            "Saving sarpol_160.jpg to sarpol_160.jpg\n",
            "Saving sarpol_161.jpg to sarpol_161.jpg\n",
            "Saving sarpol_162.jpg to sarpol_162.jpg\n",
            "Saving sarpol_163.jpg to sarpol_163.jpg\n",
            "Saving sarpol_164.jpg to sarpol_164.jpg\n",
            "Saving sarpol_165.jpg to sarpol_165.jpg\n",
            "Saving sarpol_166.jpg to sarpol_166.jpg\n",
            "Saving sarpol_167.jpg to sarpol_167.jpg\n",
            "Saving sarpol_168.jpg to sarpol_168.jpg\n",
            "Saving sarpol_169.jpg to sarpol_169.jpg\n",
            "Saving sarpol_170.jpg to sarpol_170.jpg\n",
            "Saving sarpol_171.jpg to sarpol_171.jpg\n",
            "Saving sarpol_172.jpg to sarpol_172.jpg\n",
            "Saving sarpol_173.jpg to sarpol_173.jpg\n",
            "Saving sarpol_174.jpg to sarpol_174.jpg\n",
            "Saving sarpol_175.jpg to sarpol_175.jpg\n",
            "Saving sarpol_176.jpg to sarpol_176.jpg\n",
            "Saving sarpol_177.jpg to sarpol_177.jpg\n",
            "Saving sarpol_178.jpg to sarpol_178.jpg\n",
            "Saving sarpol_179.jpg to sarpol_179.jpg\n",
            "Saving sarpol_180.jpg to sarpol_180.jpg\n",
            "Saving sarpol_181.jpg to sarpol_181.jpg\n",
            "Saving sarpol_182.jpg to sarpol_182.jpg\n",
            "Saving sarpol_183.jpg to sarpol_183.jpg\n",
            "Saving sarpol_184.jpg to sarpol_184.jpg\n",
            "Saving sarpol_185.jpg to sarpol_185.jpg\n",
            "Saving sarpol_186.jpg to sarpol_186.jpg\n",
            "Saving sarpol_187.jpg to sarpol_187.jpg\n",
            "Saving sarpol_188.jpg to sarpol_188.jpg\n",
            "Saving sarpol_189.jpg to sarpol_189.jpg\n",
            "Saving sarpol_190.jpg to sarpol_190.jpg\n",
            "Saving sarpol_191.jpg to sarpol_191.jpg\n",
            "Saving sarpol_192.jpg to sarpol_192.jpg\n",
            "Saving sarpol_193.jpg to sarpol_193.jpg\n",
            "Saving sarpol_194.jpg to sarpol_194.jpg\n",
            "Saving sarpol_195.jpg to sarpol_195.jpg\n",
            "Saving sarpol_196.jpg to sarpol_196.jpg\n",
            "Saving sarpol_197.jpg to sarpol_197.jpg\n",
            "Saving sarpol_198.jpg to sarpol_198.jpg\n",
            "Saving sarpol_199.jpg to sarpol_199.jpg\n",
            "Saving sarpol_200.jpg to sarpol_200.jpg\n",
            "Saving sarpol_201.jpg to sarpol_201.jpg\n",
            "Saving sarpol_202.jpg to sarpol_202.jpg\n",
            "Saving sarpol_203.jpg to sarpol_203.jpg\n",
            "Saving sarpol_204.jpg to sarpol_204.jpg\n",
            "Saving sarpol_205.jpg to sarpol_205.jpg\n",
            "Saving sarpol_206.jpg to sarpol_206.jpg\n",
            "Saving sarpol_207.jpg to sarpol_207.jpg\n",
            "Saving sarpol_208.jpg to sarpol_208.jpg\n",
            "Saving sarpol_209.jpg to sarpol_209.jpg\n",
            "Saving sarpol_210.jpg to sarpol_210.jpg\n",
            "Saving sarpol_211.jpg to sarpol_211.jpg\n",
            "Saving sarpol_212.jpg to sarpol_212.jpg\n",
            "Saving sarpol_213.jpg to sarpol_213.jpg\n",
            "Saving sarpol_214.jpg to sarpol_214.jpg\n",
            "Saving sarpol_215.jpg to sarpol_215.jpg\n",
            "Saving sarpol_216.jpg to sarpol_216.jpg\n",
            "Saving sarpol_217.jpg to sarpol_217.jpg\n",
            "Saving sarpol_218.jpg to sarpol_218.jpg\n",
            "Saving sarpol_219.jpg to sarpol_219.jpg\n",
            "Saving sarpol_220.jpg to sarpol_220.jpg\n",
            "Saving sarpol_221.jpg to sarpol_221.jpg\n",
            "Saving sarpol_222.jpg to sarpol_222.jpg\n",
            "Saving sarpol_223.jpg to sarpol_223.jpg\n",
            "Saving sarpol_224.jpg to sarpol_224.jpg\n",
            "Saving sarpol_225.jpg to sarpol_225.jpg\n",
            "Saving sarpol_226.jpg to sarpol_226.jpg\n",
            "Saving sarpol_227.jpg to sarpol_227.jpg\n",
            "Saving sarpol_228.jpg to sarpol_228.jpg\n",
            "Saving sarpol_229.jpg to sarpol_229.jpg\n",
            "Saving sarpol_230.jpg to sarpol_230.jpg\n",
            "Saving sarpol_231.jpg to sarpol_231.jpg\n",
            "Saving sarpol_232.jpg to sarpol_232.jpg\n",
            "Saving sarpol_233.jpg to sarpol_233.jpg\n",
            "Saving sarpol_234.jpg to sarpol_234.jpg\n",
            "Saving sarpol_235.jpg to sarpol_235.jpg\n",
            "Saving sarpol_236.jpg to sarpol_236.jpg\n",
            "Saving sarpol_237.jpg to sarpol_237.jpg\n",
            "Saving sarpol_238.jpg to sarpol_238.jpg\n",
            "Saving sarpol_239.jpg to sarpol_239.jpg\n",
            "Saving sarpol_240.jpg to sarpol_240.jpg\n",
            "Saving sarpol_241.jpg to sarpol_241.jpg\n",
            "Saving sarpol_242.jpg to sarpol_242.jpg\n",
            "Saving sarpol_243.jpg to sarpol_243.jpg\n",
            "Saving sarpol_244.jpg to sarpol_244.jpg\n",
            "Saving sarpol_245.jpg to sarpol_245.jpg\n",
            "Saving sarpol_246.jpg to sarpol_246.jpg\n",
            "Saving sarpol_247.jpg to sarpol_247.jpg\n",
            "Saving sarpol_248.jpg to sarpol_248.jpg\n",
            "Saving sarpol_249.jpg to sarpol_249.jpg\n",
            "Saving sarpol_250.jpg to sarpol_250.jpg\n",
            "Saving sarpol_251.jpg to sarpol_251.jpg\n",
            "Saving sarpol_252.jpg to sarpol_252.jpg\n",
            "Saving sarpol_253.jpg to sarpol_253.jpg\n",
            "Saving sarpol_254.jpg to sarpol_254.jpg\n",
            "Saving sarpol_255.jpg to sarpol_255.jpg\n",
            "Saving sarpol_256.jpg to sarpol_256.jpg\n",
            "Uploaded file \"sarpol_001.jpg\"\n",
            "Uploaded file \"sarpol_002.jpg\"\n",
            "Uploaded file \"sarpol_003.jpg\"\n",
            "Uploaded file \"sarpol_004.jpg\"\n",
            "Uploaded file \"sarpol_005.jpg\"\n",
            "Uploaded file \"sarpol_006.jpg\"\n",
            "Uploaded file \"sarpol_007.jpg\"\n",
            "Uploaded file \"sarpol_008.jpg\"\n",
            "Uploaded file \"sarpol_009.jpg\"\n",
            "Uploaded file \"sarpol_010.jpg\"\n",
            "Uploaded file \"sarpol_011.jpg\"\n",
            "Uploaded file \"sarpol_012.jpg\"\n",
            "Uploaded file \"sarpol_013.jpg\"\n",
            "Uploaded file \"sarpol_014.jpg\"\n",
            "Uploaded file \"sarpol_015.jpg\"\n",
            "Uploaded file \"sarpol_016.jpg\"\n",
            "Uploaded file \"sarpol_017.jpg\"\n",
            "Uploaded file \"sarpol_018.jpg\"\n",
            "Uploaded file \"sarpol_019.jpg\"\n",
            "Uploaded file \"sarpol_020.jpg\"\n",
            "Uploaded file \"sarpol_021.jpg\"\n",
            "Uploaded file \"sarpol_022.jpg\"\n",
            "Uploaded file \"sarpol_023.jpg\"\n",
            "Uploaded file \"sarpol_024.jpg\"\n",
            "Uploaded file \"sarpol_025.jpg\"\n",
            "Uploaded file \"sarpol_026.jpg\"\n",
            "Uploaded file \"sarpol_027.jpg\"\n",
            "Uploaded file \"sarpol_028.jpg\"\n",
            "Uploaded file \"sarpol_029.jpg\"\n",
            "Uploaded file \"sarpol_030.jpg\"\n",
            "Uploaded file \"sarpol_031.jpg\"\n",
            "Uploaded file \"sarpol_032.jpg\"\n",
            "Uploaded file \"sarpol_033.jpg\"\n",
            "Uploaded file \"sarpol_034.jpg\"\n",
            "Uploaded file \"sarpol_035.jpg\"\n",
            "Uploaded file \"sarpol_036.jpg\"\n",
            "Uploaded file \"sarpol_037.jpg\"\n",
            "Uploaded file \"sarpol_038.jpg\"\n",
            "Uploaded file \"sarpol_039.jpg\"\n",
            "Uploaded file \"sarpol_040.jpg\"\n",
            "Uploaded file \"sarpol_041.jpg\"\n",
            "Uploaded file \"sarpol_042.jpg\"\n",
            "Uploaded file \"sarpol_043.jpg\"\n",
            "Uploaded file \"sarpol_044.jpg\"\n",
            "Uploaded file \"sarpol_045.jpg\"\n",
            "Uploaded file \"sarpol_046.jpg\"\n",
            "Uploaded file \"sarpol_047.jpg\"\n",
            "Uploaded file \"sarpol_048.jpg\"\n",
            "Uploaded file \"sarpol_049.jpg\"\n",
            "Uploaded file \"sarpol_050.jpg\"\n",
            "Uploaded file \"sarpol_051.jpg\"\n",
            "Uploaded file \"sarpol_052.jpg\"\n",
            "Uploaded file \"sarpol_053.jpg\"\n",
            "Uploaded file \"sarpol_054.jpg\"\n",
            "Uploaded file \"sarpol_055.jpg\"\n",
            "Uploaded file \"sarpol_056.jpg\"\n",
            "Uploaded file \"sarpol_057.jpg\"\n",
            "Uploaded file \"sarpol_058.jpg\"\n",
            "Uploaded file \"sarpol_059.jpg\"\n",
            "Uploaded file \"sarpol_060.jpg\"\n",
            "Uploaded file \"sarpol_061.jpg\"\n",
            "Uploaded file \"sarpol_062.jpg\"\n",
            "Uploaded file \"sarpol_063.jpg\"\n",
            "Uploaded file \"sarpol_064.jpg\"\n",
            "Uploaded file \"sarpol_065.jpg\"\n",
            "Uploaded file \"sarpol_066.jpg\"\n",
            "Uploaded file \"sarpol_067.jpg\"\n",
            "Uploaded file \"sarpol_068.jpg\"\n",
            "Uploaded file \"sarpol_069.jpg\"\n",
            "Uploaded file \"sarpol_070.jpg\"\n",
            "Uploaded file \"sarpol_071.jpg\"\n",
            "Uploaded file \"sarpol_072.jpg\"\n",
            "Uploaded file \"sarpol_073.jpg\"\n",
            "Uploaded file \"sarpol_074.jpg\"\n",
            "Uploaded file \"sarpol_075.jpg\"\n",
            "Uploaded file \"sarpol_076.jpg\"\n",
            "Uploaded file \"sarpol_077.jpg\"\n",
            "Uploaded file \"sarpol_078.jpg\"\n",
            "Uploaded file \"sarpol_079.jpg\"\n",
            "Uploaded file \"sarpol_080.jpg\"\n",
            "Uploaded file \"sarpol_081.jpg\"\n",
            "Uploaded file \"sarpol_082.jpg\"\n",
            "Uploaded file \"sarpol_083.jpg\"\n",
            "Uploaded file \"sarpol_084.jpg\"\n",
            "Uploaded file \"sarpol_085.jpg\"\n",
            "Uploaded file \"sarpol_086.jpg\"\n",
            "Uploaded file \"sarpol_087.jpg\"\n",
            "Uploaded file \"sarpol_088.jpg\"\n",
            "Uploaded file \"sarpol_089.jpg\"\n",
            "Uploaded file \"sarpol_090.jpg\"\n",
            "Uploaded file \"sarpol_091.jpg\"\n",
            "Uploaded file \"sarpol_092.jpg\"\n",
            "Uploaded file \"sarpol_093.jpg\"\n",
            "Uploaded file \"sarpol_094.jpg\"\n",
            "Uploaded file \"sarpol_095.jpg\"\n",
            "Uploaded file \"sarpol_096.jpg\"\n",
            "Uploaded file \"sarpol_097.jpg\"\n",
            "Uploaded file \"sarpol_098.jpg\"\n",
            "Uploaded file \"sarpol_099.jpg\"\n",
            "Uploaded file \"sarpol_100.jpg\"\n",
            "Uploaded file \"sarpol_101.jpg\"\n",
            "Uploaded file \"sarpol_102.jpg\"\n",
            "Uploaded file \"sarpol_103.jpg\"\n",
            "Uploaded file \"sarpol_104.jpg\"\n",
            "Uploaded file \"sarpol_105.jpg\"\n",
            "Uploaded file \"sarpol_106.jpg\"\n",
            "Uploaded file \"sarpol_107.jpg\"\n",
            "Uploaded file \"sarpol_108.jpg\"\n",
            "Uploaded file \"sarpol_109.jpg\"\n",
            "Uploaded file \"sarpol_110.jpg\"\n",
            "Uploaded file \"sarpol_111.jpg\"\n",
            "Uploaded file \"sarpol_112.jpg\"\n",
            "Uploaded file \"sarpol_113.jpg\"\n",
            "Uploaded file \"sarpol_114.jpg\"\n",
            "Uploaded file \"sarpol_115.jpg\"\n",
            "Uploaded file \"sarpol_116.jpg\"\n",
            "Uploaded file \"sarpol_117.jpg\"\n",
            "Uploaded file \"sarpol_118.jpg\"\n",
            "Uploaded file \"sarpol_119.jpg\"\n",
            "Uploaded file \"sarpol_120.jpg\"\n",
            "Uploaded file \"sarpol_121.jpg\"\n",
            "Uploaded file \"sarpol_122.jpg\"\n",
            "Uploaded file \"sarpol_123.jpg\"\n",
            "Uploaded file \"sarpol_124.jpg\"\n",
            "Uploaded file \"sarpol_125.jpg\"\n",
            "Uploaded file \"sarpol_126.jpg\"\n",
            "Uploaded file \"sarpol_127.jpg\"\n",
            "Uploaded file \"sarpol_128.jpg\"\n",
            "Uploaded file \"sarpol_129.jpg\"\n",
            "Uploaded file \"sarpol_130.jpg\"\n",
            "Uploaded file \"sarpol_131.jpg\"\n",
            "Uploaded file \"sarpol_132.jpg\"\n",
            "Uploaded file \"sarpol_133.jpg\"\n",
            "Uploaded file \"sarpol_134.jpg\"\n",
            "Uploaded file \"sarpol_135.jpg\"\n",
            "Uploaded file \"sarpol_136.jpg\"\n",
            "Uploaded file \"sarpol_137.jpg\"\n",
            "Uploaded file \"sarpol_138.jpg\"\n",
            "Uploaded file \"sarpol_139.jpg\"\n",
            "Uploaded file \"sarpol_140.jpg\"\n",
            "Uploaded file \"sarpol_141.jpg\"\n",
            "Uploaded file \"sarpol_142.jpg\"\n",
            "Uploaded file \"sarpol_143.jpg\"\n",
            "Uploaded file \"sarpol_144.jpg\"\n",
            "Uploaded file \"sarpol_145.jpg\"\n",
            "Uploaded file \"sarpol_146.jpg\"\n",
            "Uploaded file \"sarpol_147.jpg\"\n",
            "Uploaded file \"sarpol_148.jpg\"\n",
            "Uploaded file \"sarpol_149.jpg\"\n",
            "Uploaded file \"sarpol_150.jpg\"\n",
            "Uploaded file \"sarpol_151.jpg\"\n",
            "Uploaded file \"sarpol_152.jpg\"\n",
            "Uploaded file \"sarpol_153.jpg\"\n",
            "Uploaded file \"sarpol_154.jpg\"\n",
            "Uploaded file \"sarpol_155.jpg\"\n",
            "Uploaded file \"sarpol_156.jpg\"\n",
            "Uploaded file \"sarpol_157.jpg\"\n",
            "Uploaded file \"sarpol_158.jpg\"\n",
            "Uploaded file \"sarpol_159.jpg\"\n",
            "Uploaded file \"sarpol_160.jpg\"\n",
            "Uploaded file \"sarpol_161.jpg\"\n",
            "Uploaded file \"sarpol_162.jpg\"\n",
            "Uploaded file \"sarpol_163.jpg\"\n",
            "Uploaded file \"sarpol_164.jpg\"\n",
            "Uploaded file \"sarpol_165.jpg\"\n",
            "Uploaded file \"sarpol_166.jpg\"\n",
            "Uploaded file \"sarpol_167.jpg\"\n",
            "Uploaded file \"sarpol_168.jpg\"\n",
            "Uploaded file \"sarpol_169.jpg\"\n",
            "Uploaded file \"sarpol_170.jpg\"\n",
            "Uploaded file \"sarpol_171.jpg\"\n",
            "Uploaded file \"sarpol_172.jpg\"\n",
            "Uploaded file \"sarpol_173.jpg\"\n",
            "Uploaded file \"sarpol_174.jpg\"\n",
            "Uploaded file \"sarpol_175.jpg\"\n",
            "Uploaded file \"sarpol_176.jpg\"\n",
            "Uploaded file \"sarpol_177.jpg\"\n",
            "Uploaded file \"sarpol_178.jpg\"\n",
            "Uploaded file \"sarpol_179.jpg\"\n",
            "Uploaded file \"sarpol_180.jpg\"\n",
            "Uploaded file \"sarpol_181.jpg\"\n",
            "Uploaded file \"sarpol_182.jpg\"\n",
            "Uploaded file \"sarpol_183.jpg\"\n",
            "Uploaded file \"sarpol_184.jpg\"\n",
            "Uploaded file \"sarpol_185.jpg\"\n",
            "Uploaded file \"sarpol_186.jpg\"\n",
            "Uploaded file \"sarpol_187.jpg\"\n",
            "Uploaded file \"sarpol_188.jpg\"\n",
            "Uploaded file \"sarpol_189.jpg\"\n",
            "Uploaded file \"sarpol_190.jpg\"\n",
            "Uploaded file \"sarpol_191.jpg\"\n",
            "Uploaded file \"sarpol_192.jpg\"\n",
            "Uploaded file \"sarpol_193.jpg\"\n",
            "Uploaded file \"sarpol_194.jpg\"\n",
            "Uploaded file \"sarpol_195.jpg\"\n",
            "Uploaded file \"sarpol_196.jpg\"\n",
            "Uploaded file \"sarpol_197.jpg\"\n",
            "Uploaded file \"sarpol_198.jpg\"\n",
            "Uploaded file \"sarpol_199.jpg\"\n",
            "Uploaded file \"sarpol_200.jpg\"\n",
            "Uploaded file \"sarpol_201.jpg\"\n",
            "Uploaded file \"sarpol_202.jpg\"\n",
            "Uploaded file \"sarpol_203.jpg\"\n",
            "Uploaded file \"sarpol_204.jpg\"\n",
            "Uploaded file \"sarpol_205.jpg\"\n",
            "Uploaded file \"sarpol_206.jpg\"\n",
            "Uploaded file \"sarpol_207.jpg\"\n",
            "Uploaded file \"sarpol_208.jpg\"\n",
            "Uploaded file \"sarpol_209.jpg\"\n",
            "Uploaded file \"sarpol_210.jpg\"\n",
            "Uploaded file \"sarpol_211.jpg\"\n",
            "Uploaded file \"sarpol_212.jpg\"\n",
            "Uploaded file \"sarpol_213.jpg\"\n",
            "Uploaded file \"sarpol_214.jpg\"\n",
            "Uploaded file \"sarpol_215.jpg\"\n",
            "Uploaded file \"sarpol_216.jpg\"\n",
            "Uploaded file \"sarpol_217.jpg\"\n",
            "Uploaded file \"sarpol_218.jpg\"\n",
            "Uploaded file \"sarpol_219.jpg\"\n",
            "Uploaded file \"sarpol_220.jpg\"\n",
            "Uploaded file \"sarpol_221.jpg\"\n",
            "Uploaded file \"sarpol_222.jpg\"\n",
            "Uploaded file \"sarpol_223.jpg\"\n",
            "Uploaded file \"sarpol_224.jpg\"\n",
            "Uploaded file \"sarpol_225.jpg\"\n",
            "Uploaded file \"sarpol_226.jpg\"\n",
            "Uploaded file \"sarpol_227.jpg\"\n",
            "Uploaded file \"sarpol_228.jpg\"\n",
            "Uploaded file \"sarpol_229.jpg\"\n",
            "Uploaded file \"sarpol_230.jpg\"\n",
            "Uploaded file \"sarpol_231.jpg\"\n",
            "Uploaded file \"sarpol_232.jpg\"\n",
            "Uploaded file \"sarpol_233.jpg\"\n",
            "Uploaded file \"sarpol_234.jpg\"\n",
            "Uploaded file \"sarpol_235.jpg\"\n",
            "Uploaded file \"sarpol_236.jpg\"\n",
            "Uploaded file \"sarpol_237.jpg\"\n",
            "Uploaded file \"sarpol_238.jpg\"\n",
            "Uploaded file \"sarpol_239.jpg\"\n",
            "Uploaded file \"sarpol_240.jpg\"\n",
            "Uploaded file \"sarpol_241.jpg\"\n",
            "Uploaded file \"sarpol_242.jpg\"\n",
            "Uploaded file \"sarpol_243.jpg\"\n",
            "Uploaded file \"sarpol_244.jpg\"\n",
            "Uploaded file \"sarpol_245.jpg\"\n",
            "Uploaded file \"sarpol_246.jpg\"\n",
            "Uploaded file \"sarpol_247.jpg\"\n",
            "Uploaded file \"sarpol_248.jpg\"\n",
            "Uploaded file \"sarpol_249.jpg\"\n",
            "Uploaded file \"sarpol_250.jpg\"\n",
            "Uploaded file \"sarpol_251.jpg\"\n",
            "Uploaded file \"sarpol_252.jpg\"\n",
            "Uploaded file \"sarpol_253.jpg\"\n",
            "Uploaded file \"sarpol_254.jpg\"\n",
            "Uploaded file \"sarpol_255.jpg\"\n",
            "Uploaded file \"sarpol_256.jpg\"\n",
            "/content\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ulHrgSyNwo1X2WYmQyeb4lIxIIJ4Xs-G\n",
            "To: /content/checkpoint.pt\n",
            "100% 373M/373M [00:03<00:00, 94.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Fds7OAxbPY"
      },
      "source": [
        "# Preparing Data\n",
        "\n",
        "# TODO: Ragged nested sequences occuring somewhere in here\n",
        "\n",
        "if DATASET_MODE:\n",
        "  src_imgs = load(f\"{SRC_DIR}data/images/*\")\n",
        "  src_lbls = load(f\"{SRC_DIR}data/labels/*\")\n",
        "\n",
        "  dataset = []\n",
        "  # Dataset Structure:\n",
        "  # [\n",
        "  #   0:{\n",
        "  #     img:  image\n",
        "  #     lbl:  label\n",
        "  #     filename: {img: 0.png, lbl: 0.png}\n",
        "  #   },\n",
        "  #   1:{}, 2:{}, n:{}\n",
        "  # ]\n",
        "\n",
        "  if src_imgs.shape[0] == src_lbls.shape[0]:\n",
        "    n = src_lbls.shape[0]\n",
        "\n",
        "    for index in range(n):\n",
        "      dataset.append({\n",
        "        \"img\":Image.fromarray(src_imgs[index][0]),\n",
        "        \"lbl\":Image.fromarray(src_lbls[index][0]),\n",
        "        \"filename\":{\n",
        "          \"img\":f\"{src_imgs[index][1]}.{IMG_FORMAT}\",\n",
        "          \"lbl\":f\"{src_lbls[index][1]}.{IMG_FORMAT}\"\n",
        "        }\n",
        "      })\n",
        "\n",
        "  relevant = 0\n",
        "  irrelevant = 0\n",
        "  for data in dataset:\n",
        "    if np.asarray(data[\"lbl\"]).sum() > 0:\n",
        "      relevant += 1\n",
        "    else:\n",
        "      irrelevant += 1\n",
        "\n",
        "  total = len(dataset)\n",
        "  print(f\"Dataset Size: {total}\\nRelevant Percentage: {percent(relevant, total)}, Irrelevant Percentage: {percent(irrelevant, total)}\")\n",
        "  print(f\"Allow Irrelevant Data? {'YES' if ALLOW_IRRELEVANT else 'NO'}\")\n",
        "\n",
        "  training_set, validation_set = train_test_split(dataset, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "  training_data = 0\n",
        "  for training in training_set:\n",
        "    if ALLOW_IRRELEVANT or np.asarray(training[\"lbl\"]).sum() > 0:\n",
        "      save_data(training, val=False)\n",
        "      training_data += 1\n",
        "      \n",
        "  validation_data = 0\n",
        "  for testing in validation_set:\n",
        "    if ALLOW_IRRELEVANT or np.asarray(testing[\"lbl\"]).sum() > 0:\n",
        "      save_data(testing, val=True)\n",
        "      validation_data += 1\n",
        "\n",
        "  total_usable = training_data + validation_data\n",
        "  print(f\"\\nTraining Percentage: {percent(training_data, total_usable)}, Validation Percent: {percent(validation_data, total_usable)}\")\n",
        "\n",
        "\n",
        "\n",
        "# TODO: need to generate augmented data too (or at least ensure augmented data expands the dataset and doesn't just replace it), and also re-train the model on the updated version"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ODxRvJdHvno"
      },
      "source": [
        "# Cleanup\n",
        "%%bash\n",
        "if [ -d $SRC_DIR ]; then\n",
        "  rm -r $SRC_DIR\n",
        "fi"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTTLPwOBzU-"
      },
      "source": [
        "# UNet\n",
        "\n",
        "**U-Net Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL5QPK-wA03N"
      },
      "source": [
        "The first half of the architecture is the down-sampling process. At each stage, two 3x3 convolutions are used. The output is then pooled and becomes the input for the next 3x3 convolution and so on, until the bottom of this diagram is reached.\n",
        "\n",
        "The second half of the architecture is the up-sampling process, which is a reflection of the down-sampling process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXGvzzfPpYut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7e7096-12b0-4f72-8ef3-6211a56caf7c"
      },
      "source": [
        "# UNet Model\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "# Referenced https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential (\n",
        "        nn.Conv2d(in_channels, out_channels, C_KERNEL, C_STRIDE, C_PADDING, bias=C_BIAS), # This is a same convolution (input height*width = output height*width)\n",
        "        nn.BatchNorm2d(out_channels), # BatchNorm accelerates training by normalizing inputs by re-centering and re-scaling\n",
        "        nn.ReLU(inplace=R_INPLACE), # ReLU is Rectified Linear Unit; essentially it makes it so negative inputs are discarded and positive inputs are passed through\n",
        "\n",
        "        # Now we do this a second time but with out_channels to out_channels\n",
        "        nn.Conv2d(out_channels, out_channels, C_KERNEL, C_STRIDE, C_PADDING, bias=C_BIAS),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=R_INPLACE),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "# DoubleConv is everything in the first node of the architecture (before pooling)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=U_FEATURES):\n",
        "    super(UNet, self).__init__()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.downs = nn.ModuleList() # Stores the convolutional layers; it's a list, but using ModuleList lets us use BatchNorm2d\n",
        "    self.pool = nn.MaxPool2d(kernel_size=P_KERNEL, stride=P_STRIDE) # Pooling layer will be used inbetween, in forwarding method\n",
        "    # Note that the pooling layer will require out inputs to be perfectly divisible by 2 because we're doing a stride of 2\n",
        "    # Example: 161 x 161 -> MaxPool -> 80 x 80 -> Upsample -> 160 x 160; in this case, we couldn't concatinate the two as they need the same width and height for concat (161x161 is input, 160x160 is output)\n",
        "    # If input is perfectly dividible by 16 then this issue won't happen (16 because its 4 steps, all dividing by 2 (16/(4*2) = 2))\n",
        "    # In order to keep the system general, we can either pad the image or crop the image so that it works even if image size isn't perfectly divisible by 16\n",
        "\n",
        "\n",
        "    # Down-Sampling part of UNet\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature)) # Mapping some input (in the UNet architecture example, it maps 1 to 64 for the first node)\n",
        "      in_channels = feature\n",
        "\n",
        "    # Up-Sampling Part of UNet\n",
        "    # At 10:20 or so in the video he mentions transposed convoltions that may be a better method to this part, but we'll use similar approach to UNet paper for now\n",
        "    # We're using the reversed list of features because we're going from the bottom up now\n",
        "    for feature in reversed(features):\n",
        "      \n",
        "      # In_channels is feature*2 here, output is feature\n",
        "      self.ups.append(\n",
        "          nn.ConvTranspose2d( \n",
        "              feature*2, feature, kernel_size=P_KERNEL, stride=P_STRIDE\n",
        "          )\n",
        "      )\n",
        "      self.ups.append(DoubleConv(feature*2, feature)) # Because we go up then do two convs then go up then do two convs etc\n",
        "\n",
        "    # Bottom Layer\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2) # We're using features[-1] because we want the bottom feature\n",
        "\n",
        "    # Now we do 1x1 conv which doesn't change height or width just number of channels\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=F_KERNEL)\n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = [] # We skip connections for each stage in the architecture, as we use this part later on\n",
        "    \n",
        "    # This loop does all the down-sampling steps until the final step just before the bottom layer (aka bottleneck)\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    skip_connections = skip_connections[::-1] # we wanna go backwards in order when we're doing our concatination; the highest resolution image is the first one; to make things easier, we'll just reverse this list\n",
        "\n",
        "    # We're using a step of 2 here because we're going up then double conv each iteration\n",
        "    for i in range(0, len(self.ups), 2):\n",
        "      x = self.ups[i](x) # doing ConvTranspose2d here\n",
        "      skip_connection = skip_connections[i//2] # // is integer division; we're doing a step of one ordering here\n",
        "\n",
        "      # Dealing with images that are not perfectly dividible\n",
        "      # TODO: perhaps we should look into scaling instead of cropping or padding, or perhaps we should scale before we put the image into the system, so that this is not relevant?\n",
        "      if x.shape != skip_connection.shape:\n",
        "        x = TF.resize(x, size=skip_connections.shape[2:]) # we're taking out height and width here; basically, we're resizing the image if it doesn't fit\n",
        "\n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1) # dim 1 is the channel dimension; we're concationating these things along the channel dimension\n",
        "      x = self.ups[i+1](concat_skip) # running it through a double conv\n",
        "    \n",
        "    return self.final_conv(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Testing the implementation thus far\n",
        "def test():\n",
        "  x = torch.randn((3, 1, 160, 160)) # 3 is number of images (batch size), 1 is number of channels, 160 is image height, 160 is image width\n",
        "  model = UNet(in_channels=1, out_channels=1)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "  print(x.shape)\n",
        "  assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   test()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 160, 160])\n",
            "torch.Size([3, 1, 160, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgE5iT1ORHpM"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXGKRQ6VUz-J"
      },
      "source": [
        "# TentDataset\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "# Data directory should be in the format:\n",
        "# data\n",
        "#   train_images\n",
        "#   train_masks\n",
        "#   val_images\n",
        "#   val_masks\n",
        "def target_count(id):\n",
        "  with open(TENT_CSV) as csvfile:\n",
        "    for row in list(csv.reader(csvfile)):\n",
        "      if row[0].split(\".\", 1)[0] == id:\n",
        "        return int(row[1])\n",
        "  return None\n",
        "\n",
        "# TODO: probably needs to be able to support the case where the target is none?\n",
        "\n",
        "class TentDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir=None, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = os.path.join(self.image_dir, self.images[index])\n",
        "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    id = os.path.basename(image_path).split(\".\")[0] #.replace(f\".{IMG_FORMAT}\", \"\") # TODO: just remove anything after the period\n",
        "\n",
        "    mask = -np.ones(image.shape, dtype=np.float32) #np.array(Image.open(image_path).convert(\"L\"), dtype=np.float32) # Unable to have a None return type in the dataset\n",
        "    count = -1\n",
        "    if self.mask_dir is not None:\n",
        "      mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "      mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) # We use L since mask is grey scale\n",
        "      mask[mask == 255.0] = 1.0 # Preprocessing; we change this cuz we're using a sigmoid on the last activation for probability of white pixel, so this makes it work better?\n",
        "      count = target_count(id)\n",
        "\n",
        "    # TODO: perhaps for the previous line, we should just normalize? as in, just divide the mask by 255.0 to get things in terms of 0 to 1\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image=image, mask=mask)\n",
        "      image = augmentations[\"image\"]\n",
        "      mask = augmentations[\"mask\"]\n",
        "\n",
        "    return image, mask, id, count"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpmSRqG4RFsn"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQKV-QNfXNu"
      },
      "source": [
        "# Utilities\n",
        "def dirs(root=OUTPUT_DIR):\n",
        "  datasets = [\"validation\", \"training\"] if DATASET_MODE else [\"uploaded\"]\n",
        "  dirs = {}\n",
        "  for dataset in datasets:\n",
        "    dataset_dir = f\"{root}{dataset}/\"\n",
        "    dirs[dataset] = {\"features\":       f\"{dataset_dir}features/\",\n",
        "                     \"targets\":        f\"{dataset_dir}targets/\",\n",
        "                     \"estimates\":      f\"{dataset_dir}estimates/\",\n",
        "                     \"visualizations\": f\"{dataset_dir}visualizations/\"}\n",
        "  return dirs\n",
        "\n",
        "DIRECTORIES = dirs()\n",
        "\n",
        "for dataset in DIRECTORIES.values():\n",
        "  for dir in dataset.values():\n",
        "    if not os.path.exists(dir):\n",
        "      os.makedirs(dir)\n",
        "\n",
        "def kmeans_localize(img, k):\n",
        "  centers = None\n",
        "  if img is not None:\n",
        "    img = np.argwhere(img.T == 255)\n",
        "    if k > 0:\n",
        "      kmeans = KMeans(n_clusters=k)\n",
        "      kmeans.fit(img)\n",
        "      y_kmeans = kmeans.predict(img)\n",
        "      centers = kmeans.cluster_centers_\n",
        "  return centers\n",
        "\n",
        "def blob_localize(mask):\n",
        "  params = cv2.SimpleBlobDetector_Params()\n",
        "  params.minArea = 1;\n",
        "  params.maxArea = IMAGE_HEIGHT * IMAGE_WIDTH;\n",
        "  params.minDistBetweenBlobs = 0\n",
        "  params.filterByColor = True\n",
        "  params.filterByArea = True\n",
        "  params.filterByCircularity = False\n",
        "  params.filterByInertia = False\n",
        "  params.filterByConvexity = False\n",
        "  params.minThreshold = 0\n",
        "  params.maxThreshold = 255\n",
        "  params.blobColor = 255\n",
        "  detector = cv2.SimpleBlobDetector_create(params)\n",
        "  detector.empty()\n",
        "  return detector.detect(mask)\n",
        "\n",
        "def save_checkpoint(state, filename=CHECKPOINT):\n",
        "  print(\"=> Saving checkpoint\\n\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "  print(\"=> Loading checkpoint\\n\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(train_dir, val_dir, train_maskdir=None, val_maskdir=None, train_transform=None, val_transform=None, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY):\n",
        "  train_ds = TentDataset(image_dir=train_dir, mask_dir=train_maskdir, transform=train_transform)\n",
        "  train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n",
        "  val_ds = TentDataset(image_dir=val_dir, mask_dir=val_maskdir, transform=val_transform)\n",
        "  val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
        "  return train_loader, val_loader\n",
        "\n",
        "def get_loader(imgdir, maskdir=None, transform=None, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY):\n",
        "  dataset = TentDataset(image_dir=imgdir, mask_dir=maskdir, transform=transform)\n",
        "  loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
        "  return loader\n",
        "\n",
        "def display_visualizations():\n",
        "  print(f\"\\n\\n{DIRECTORIES}\")\n",
        "  for dataset in DIRECTORIES:\n",
        "    vis_dir = f\"{DIRECTORIES[dataset]['visualizations']}\"\n",
        "    print(vis_dir)\n",
        "\n",
        "    targets = load(f\"{vis_dir}targets/*\")\n",
        "    estimates = load(f\"{vis_dir}estimates/*\")\n",
        "\n",
        "    if targets.shape[0] == estimates.shape[0]:\n",
        "      for index in range(targets.shape[0]):\n",
        "        t_id = targets[index][1]\n",
        "        e_id = estimates[index][1]\n",
        "        if t_id == e_id:\n",
        "          fig = plt.figure(figsize=(15, 15))\n",
        "          grid = ImageGrid(fig, 111, nrows_ncols=(1, 2), axes_pad=0.5)\n",
        "\n",
        "          print(f\"\\nLoading: {t_id}\")\n",
        "          grid[0].imshow(targets[index][0])\n",
        "          grid[1].imshow(estimates[index][0])\n",
        "          plt.show()\n",
        "        else:\n",
        "          print(f\"\\nID Mismatch!: {t_id} =/= {e_id}\\n\")\n",
        "          \n",
        "def similarity(estimate, target): # Returns the Jaccard Index of estiamte and target (how similar they are)\n",
        "  return (EPSILON+(estimate * target).sum()) / (EPSILON+(estimate + target).sum())\n",
        "\n",
        "def load_image(id, directory=OUTPUT_DIR):\n",
        "  image = {\"feature\":None, \"target\":None, \"estimate\":None, \"visualization\":None}\n",
        "  for dirs in DIRECTORIES.values():\n",
        "    target_path = f\"{dirs['targets']}{id}.{IMG_FORMAT}\"\n",
        "    estimate_path = f\"{dirs['estimates']}{id}.{IMG_FORMAT}\"\n",
        "    feature_path = f\"{dirs['features']}{id}.{IMG_FORMAT}\"\n",
        "    visualization_path = f\"{dirs['visualizations']}{id}.{IMG_FORMAT}\"\n",
        "\n",
        "    if os.path.exists(target_path):\n",
        "      image[\"target\"] = np.asarray(Image.open(target_path).convert(\"L\")) # use 1 instead of L for binary\n",
        "    \n",
        "    if os.path.exists(estimate_path):\n",
        "      image[\"estimate\"] = np.asarray(Image.open(estimate_path).convert(\"L\")) # use 1 instead of L for binary\n",
        "    \n",
        "    if os.path.exists(feature_path):\n",
        "      image[\"feature\"] = np.asarray(Image.open(feature_path).convert(\"RGB\"))\n",
        "    \n",
        "    if os.path.exists(visualization_path):\n",
        "      image[\"visualization\"] = np.asarray(Image.open(visualization_path).convert(\"RGB\"))\n",
        "  return image\n",
        "\n",
        "def make_visualization(id, directory, accuracy, t_count=None):\n",
        "  # Crashing occurs in here somewhere?\n",
        "\n",
        "\n",
        "  image = load_image(id)\n",
        "  img = image[\"feature\"]\n",
        "  lbl = cv2.threshold(cv2.convertScaleAbs(image[\"target\"]), 1, 255, cv2.THRESH_BINARY)[1]#cv2.threshold(cv2.convertScaleAbs(cv2.cvtColor(image[\"target\"], cv2.COLOR_BGR2GRAY)), 1, 255, cv2.THRESH_BINARY)[1]\n",
        "  prd = cv2.normalize(cv2.convertScaleAbs(image[\"estimate\"]), np.zeros(img.shape, dtype=np.float32), 0, 255, cv2.NORM_MINMAX)#cv2.normalize(cv2.convertScaleAbs(cv2.cvtColor(image[\"estimate\"], cv2.COLOR_BGR2GRAY)), np.zeros(img.shape, dtype=np.float32), 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "\n",
        "  lbl_count = t_count\n",
        "  if lbl is not None and lbl_count is not None and KMEANS_LOCALIZATION:\n",
        "    lbl_kmeans = kmeans_localize(lbl, lbl_count)\n",
        "\n",
        "  overlay = np.zeros(img.shape)\n",
        "  gt_overlay = np.zeros(img.shape)\n",
        "  mask_with_keypoints = prd\n",
        "  \n",
        "  # Adapted from: https://stackoverflow.com/questions/46103731/is-there-a-simple-method-to-highlight-the-mask\n",
        "  contours, hierarchy = cv2.findContours(prd, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  cv2.drawContours(overlay, contours, -1, (1, 0, 0), thickness=cv2.FILLED)\n",
        "\n",
        "  if lbl is not None:\n",
        "    gt_contours, gt_hierarchy = cv2.findContours(lbl, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cv2.drawContours(gt_overlay, gt_contours, -1, (1, 0, 1), thickness=cv2.FILLED)\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  bg = (cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if GRAYSCALE else img)\n",
        "  if GRAYSCALE:\n",
        "    bg = cv2.cvtColor(bg, cv2.COLOR_GRAY2BGR)\n",
        "  \n",
        "  overlay = cv2.normalize(cv2.convertScaleAbs(overlay), np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX)\n",
        "  overlay = cv2.bitwise_xor(bg, overlay)\n",
        "  overlay = cv2.normalize(overlay, np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX) # set 1 to 255 if this doesn't work?\n",
        "\n",
        "  if lbl is not None:\n",
        "    gt_overlay = cv2.normalize(cv2.convertScaleAbs(gt_overlay), np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX)\n",
        "    gt_overlay = cv2.bitwise_xor(bg, gt_overlay)\n",
        "    gt_overlay = cv2.normalize(gt_overlay, np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "  # TODO: Sometimes visualizations lack the background image; maybe it wasn't saved in time or some other issue?\n",
        "  # Might only be a problem on CPU, not sure about GPU yet.\n",
        "\n",
        "  # Target Visualization\n",
        "  if lbl is not None:\n",
        "    accuracy = f\"{accuracy:.2f}\"\n",
        "    target_count = f\"(Tents: {lbl_count})\"\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(f\"{id}: Ground Truth {target_count}\")\n",
        "    plt.imshow(gt_overlay, vmin=0, vmax=1)\n",
        "    green_patch = mpatches.Patch(color=\"lightgreen\", label=\"Target Mask\")\n",
        "    if LOAD_FROM_CSV and KMEANS_LOCALIZATION:\n",
        "      kmeans_patch = mpatches.Patch(color=\"red\", label=\"Tent\")\n",
        "      if KMEANS_LOCALIZATION and lbl_kmeans is not None:\n",
        "        kmeans_patch = plt.scatter(lbl_kmeans[:,0], lbl_kmeans[:,1], s=50, marker='x', c=\"red\", alpha=1, label=\"Tent\")\n",
        "      plt.legend(handles=[green_patch, kmeans_patch], loc=\"upper left\")\n",
        "    else:\n",
        "      plt.legend(handles=[green_patch], loc=\"upper left\")\n",
        "    target_vis_dir = f\"{directory}targets/\"\n",
        "    if not os.path.exists(target_vis_dir):\n",
        "        os.makedirs(target_vis_dir)\n",
        "    plt.savefig(f\"{target_vis_dir}{id}.{IMG_FORMAT}\")\n",
        "    plt.close()\n",
        "\n",
        "  # Estimate Visualization\n",
        "  estimate_count = \"\"\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(f\"{id}: Prediction (Accuracy: {accuracy} {estimate_count})\")\n",
        "  plt.imshow(overlay, vmin=0, vmax=1)\n",
        "  teal_patch = mpatches.Patch(color=\"cyan\", label=\"Prediction Mask\")\n",
        "  plt.legend(handles=[teal_patch], loc=\"upper left\")\n",
        "  estimate_vis_dir = f\"{directory}estimates/\"\n",
        "  if not os.path.exists(estimate_vis_dir):\n",
        "      os.makedirs(estimate_vis_dir)\n",
        "  plt.savefig(f\"{estimate_vis_dir}{id}.{IMG_FORMAT}\")\n",
        "  plt.close()\n",
        "\n",
        "  # Keypoint Visualization\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(f\"{id}: Predicted Mask (Accuracy: {accuracy})\")\n",
        "  plt.imshow(mask_with_keypoints, vmin=0, vmax=1)\n",
        "  keypoint_dir = f\"{directory}keypoints/\"\n",
        "  if not os.path.exists(keypoint_dir):\n",
        "      os.makedirs(keypoint_dir)\n",
        "  plt.savefig(f\"{keypoint_dir}{id}.{IMG_FORMAT}\")\n",
        "  plt.close()\n",
        "\n",
        "def tent_count(id):\n",
        "  # TODO: implement this (convolutional neural network here?)\n",
        "  return None\n",
        "\n",
        "def save_all(loaders, model, loss_fn, directory=OUTPUT_DIR, device=DEVICE):\n",
        "  model.eval()\n",
        "  # TODO: do visualizations here too, then save them to a file; we can then just load from file to display them\n",
        "  # also make a Python pickle which contains accuracies and other such information, along with the path to\n",
        "  # the appropriate file, and if it's validation or training data\n",
        "\n",
        "  # TODO: make sure we're loading mask data in binary format (I don't think we are currently)\n",
        "\n",
        "  results = {}\n",
        "  for dataset, loader in loaders.items():\n",
        "    dirs = DIRECTORIES[dataset]\n",
        "\n",
        "    loader_progress = tqdm(loader)\n",
        "    loader_progress.set_description(f\"Saving {dataset} dataset\")\n",
        "    for feature, target, id, count in loader_progress:\n",
        "      id = id[0]\n",
        "      count = count.item()\n",
        "      if count == -1:\n",
        "        count = None\n",
        "\n",
        "\n",
        "      if target.sum() < 0:\n",
        "        target = None\n",
        "\n",
        "\n",
        "      feature = feature.to(device)\n",
        "      torchvision.utils.save_image(feature, f\"{dirs['features']}{id}.{IMG_FORMAT}\")\n",
        "      with torch.no_grad():\n",
        "        estimate = torch.sigmoid(model(feature))\n",
        "        estimate = (estimate > 0.5).float()\n",
        "\n",
        "      accuracy = None\n",
        "      loss = None\n",
        "      if estimate.shape[0] > 0:\n",
        "        torchvision.utils.save_image(estimate, f\"{dirs['estimates']}{id}.{IMG_FORMAT}\")\n",
        "        if target is not None:\n",
        "          accuracy = similarity(estimate.detach().cpu(), target).item()\n",
        "          loss = loss_fn(estimate.detach().cpu(), target.unsqueeze(1)).item()\n",
        "          torchvision.utils.save_image(target.unsqueeze(1), f\"{dirs['targets']}{id}.{IMG_FORMAT}\") #might not need unsqueeze? or maybe we should use it on estimate too?\n",
        "      \n",
        "      # torchvision.utils.save_image(feature, f\"{dirs['features']}{id}.{IMG_FORMAT}\")\n",
        "      # torchvision.utils.save_image(target.unsqueeze(1), f\"{dirs['targets']}{id}.{IMG_FORMAT}\") #might not need unsqueeze? or maybe we should use it on estimate too?\n",
        "\n",
        "      # accuracy = similarity(estimate.detach().cpu(), target).item()\n",
        "      make_visualization(id, dirs[\"visualizations\"], accuracy, count)\n",
        "\n",
        "      # TODO: kmeans is causing the error when we make visualizations I think?\n",
        "      # at least when running on CPU anyway\n",
        "\n",
        "\n",
        "      # looks like it crashes when it gets a target with no tents?\n",
        "\n",
        "      # Seems like there's an error with the loader; it gives us a target with no tent even though the target has a tent\n",
        "\n",
        "      results[id] = {\n",
        "          \"dataset\":         dataset,\n",
        "          \"target_count\":    count,\n",
        "          \"estimate_count\":  tent_count(id),\n",
        "          \"mask_similarity\": accuracy,\n",
        "          \"loss\": loss}\n",
        "\n",
        "      loader_progress.set_description(f\"Saving [{dataset}]: {id}, {results[id]}\")\n",
        "    \n",
        "  model.train()\n",
        "  with open(f\"{directory}results.pkl\", \"wb\") as output:\n",
        "    pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  shutil.make_archive(\"tent_counts\", \"tar\", directory)\n",
        "  print(\"\\nSaved.\\n\")\n",
        "  return results"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8NEUSCTRAxr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD1wOKe_Z_eJ"
      },
      "source": [
        "# Model Training\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "# This whole function trains one epoch\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "  loop = tqdm(loader) # tqdm gives us a progress bar\n",
        "  loop.set_description(\"Training\")\n",
        "\n",
        "  for batch_idx, (data, targets, id, _) in enumerate(loop):\n",
        "    loop.set_description(f\"Training on {id[0]}\")\n",
        "    data = data.to(device=DEVICE)\n",
        "    targets = targets.float().unsqueeze(1).to(device=DEVICE) #might not need to make it float since it might already be float? Also, unsqueeze is used cuz we're adding a channel\n",
        "\n",
        "    # Forward\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(data)\n",
        "      loss = loss_fn(predictions, targets)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "def main():\n",
        "  model = UNet(in_channels=3, out_channels=1).to(DEVICE) # if we wanted multiclass segmentation we'd change our channels and change our loss function to cross entropy loss\n",
        "  loss_fn = nn.BCEWithLogitsLoss() # We're not doing sigmoid on the output of model which is why we're using this here\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "  loaders = {}\n",
        "\n",
        "  val_transforms = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Normalize(\n",
        "      mean=[0.0, 0.0, 0.0],\n",
        "      std=[1.0, 1.0, 1.0],\n",
        "      max_pixel_value=255.0 # This basically just divides by 255, so we get a value between 0 and 1\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "  ])\n",
        "\n",
        "  if DATASET_MODE:\n",
        "    train_transform = A.Compose([\n",
        "      A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "      A.Rotate(limit=35, p=1.0),\n",
        "      A.HorizontalFlip(p=0.5),\n",
        "      A.VerticalFlip(p=0.1),\n",
        "      A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0 # This basically just divides by 255, so we get a value between 0 and 1\n",
        "      ),\n",
        "      ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    # loaders = {\"validation\":val_loader, \"training\":train_loader}\n",
        "    loaders[\"training\"], loaders[\"validation\"] = get_loaders(\n",
        "        TRAIN_IMG_DIR,\n",
        "        VAL_IMG_DIR,\n",
        "        TRAIN_LBL_DIR,\n",
        "        VAL_LBL_DIR,\n",
        "        train_transform,\n",
        "        val_transforms,\n",
        "        BATCH_SIZE,\n",
        "        NUM_WORKERS,\n",
        "        PIN_MEMORY\n",
        "    )\n",
        "  else:\n",
        "    loaders[\"uploaded\"] = get_loader(UPLOAD_DIR, None, val_transforms)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if LOAD_MODEL:\n",
        "    if DEVICE == \"cuda\":\n",
        "      load_checkpoint(torch.load(CHECKPOINT), model)\n",
        "    else:\n",
        "      load_checkpoint(torch.load(CHECKPOINT, map_location=lambda storage, loc: storage), model) # From https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/4\n",
        "  # check_accuracy(val_loader, model, device=DEVICE) # Because the val_loader is being passed, we know that we're only visualizing validation images and not the images the model was trained on\n",
        "  scaler = torch.cuda.amp.GradScaler() # I think this is where we get the warning about running on CPU; should try to address this warning\n",
        "  \n",
        "  NUM_EPOCHS = NUM_EPOCHS if DATASET_MODE else 1\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    if TRAINING and DATASET_MODE:\n",
        "      train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "      # Save Model\n",
        "      checkpoint = {\"state_dict\":model.state_dict(), \"optimizer\":optimizer.state_dict()}\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "    # Check Accuracy\n",
        "    # check_accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "    # Save Predictions\n",
        "    # save_predictions_as_imgs(val_loader, model, folder=PREDICT_DIR, device=DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: save all predictions (validation and ground truth in separate folders)\n",
        "    # Also save pictures with overlay, and pictures with overlay plus plot?\n",
        "    # also save tent count (heatmap if possible too)\n",
        "\n",
        "\n",
        "    # TODO: use the commented version later; seems to be a problem with saving the training data though?\n",
        "    # save_all({\"validation\":val_loader, \"training\":train_loader}, model)\n",
        "    results = save_all(loaders, model, loss_fn)\n",
        "    \n",
        "    if LIVE_VISUALIZE:\n",
        "      display_visualizations()\n",
        "\n",
        "\n",
        "    #print(f\"\\n==============\\n{results}\\n==============\\n\")\n",
        "\n",
        "    # Generate Map View\n",
        "    # Image.fromarray(stitch(f\"{OUTPUT_MASK_DIR}*\")).save(\"map_predicted_mask.png\") # TODO: generate map\n",
        "    # Image.fromarray(stitch(f\"{VAL_IMG_DIR}*\")).save(\"map_image.png\")\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "# This solves issues when running on Windows (issues relating to NUM_WORKERS); probably not so relevant to Colab.\n",
        "# if __name__ == \"__main__\":\n",
        "#   main()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj7ekOSNAKHC"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qJ6mESs_4Yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f0621b-bcf9-4432-9eda-dd286b5d1605"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving [uploaded]: sarpol_173, {'dataset': 'uploaded', 'target_count': None, 'estimate_count': None, 'mask_similarity': None, 'loss': None}: 100%|██████████| 256/256 [28:43<00:00,  6.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}