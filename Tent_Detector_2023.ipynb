{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zacharylazzara/tent-detection/blob/main/Tent_Detector_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M59yW_AWZ1B"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GocOvAjoz3Zy"
      },
      "source": [
        "##Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rxo20pIfzlxW"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS      = 200\n",
        "BATCH_SIZE    = 8\n",
        "INIT_LR       = 0.0001\n",
        "IMAGE_HEIGHT  = 512\n",
        "IMAGE_WIDTH   = 512\n",
        "TEST_SPLIT    = 0.15\n",
        "U_MODEL_NAME  = 'unet.pth'\n",
        "C_MODEL_NAME  = 'cnn.pth'\n",
        "OUTPUT_FORMAT = 'jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hjSDyxitHih"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PMBNizBrtJZS"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics -q\n",
        "import math\n",
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mean\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.nn import Sequential\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn import ConvTranspose2d\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import functional\n",
        "from torch.nn import BatchNorm2d\n",
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from torch.nn.modules.loss import PoissonNLLLoss\n",
        "from torchmetrics.classification import BinaryJaccardIndex\n",
        "from torch import flatten\n",
        "from torch import cat\n",
        "from torch import randn\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import CenterCrop\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vfi33_GtEj2"
      },
      "source": [
        "##Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI4mFY9uMosP",
        "outputId": "ee40802c-120f-4284-cdac-a8fd35c441d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
        "print(f'Using device: {DEVICE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vorAQM6jtdOu",
        "outputId": "dfb66dab-47cd-4b70-eb86-33b3d0676eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SRC_DIR=sarpol-zahab-tents\n",
            "env: OUTPUT_DIR=output\n"
          ]
        }
      ],
      "source": [
        "# Initialize Environment\n",
        "%env SRC_DIR        = sarpol-zahab-tents\n",
        "%env OUTPUT_DIR     = output\n",
        "\n",
        "SRC_DIR             = os.environ.get('SRC_DIR')\n",
        "OUTPUT_DIR          = os.environ.get('OUTPUT_DIR')\n",
        "\n",
        "DATA_PATH           = Path(f'{SRC_DIR}/data')\n",
        "IMAGES_PATH         = Path(f'{DATA_PATH}/images')\n",
        "MASKS_PATH          = Path(f'{DATA_PATH}/labels')\n",
        "LABELS_PATH         = Path(f'{DATA_PATH}/sarpol_counts.csv')\n",
        "EXT                 = '.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wl8_C1Uas_F3"
      },
      "outputs": [],
      "source": [
        "# Initialize Directories\n",
        "%%bash\n",
        "\n",
        "if [ -d 'sample_data' ]; then\n",
        "  rm -r sample_data\n",
        "fi\n",
        "\n",
        "if [ ! -d $SRC_DIR ]; then\n",
        "  git clone https://github.com/tofighi/sarpol-zahab-tents.git\n",
        "fi\n",
        "\n",
        "if [ ! -d $OUTPUT_DIR ]; then\n",
        "  mkdir -p $OUTPUT_DIR\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMaVfs-TuId3"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65mcYUXjBpMI"
      },
      "source": [
        "##UNet\n",
        "Adapted from: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2IeM8zsTBq5d"
      },
      "outputs": [],
      "source": [
        "# Adapted from: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n",
        "\n",
        "class Block(Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Block, self).__init__()\n",
        "    self.double_conv2d = Sequential(\n",
        "        Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        BatchNorm2d(out_channels),\n",
        "        ReLU(inplace=True),\n",
        "        Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        BatchNorm2d(out_channels),\n",
        "        ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv2d(x)\n",
        "\n",
        "class Encoder(Module):\n",
        "  def __init__(self, channels=(3, 16, 32, 64)):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.encoder_blocks = ModuleList([Block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n",
        "    self.pool = MaxPool2d(2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    block_outputs = []\n",
        "    for block in self.encoder_blocks:\n",
        "      x = block(x)\n",
        "      block_outputs.append(x)\n",
        "      x = self.pool(x)\n",
        "    return block_outputs\n",
        "\n",
        "class Decoder(Module):\n",
        "  def __init__(self, channels=(64, 32, 16)):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.up_convs = ModuleList([ConvTranspose2d(channels[i], channels[i+1], 2, 2) for i in range(len(channels)-1)])\n",
        "    self.decoder_blocks = ModuleList([Block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n",
        "  \n",
        "  def crop(self, encoder_features, x):\n",
        "    (_, _, H, W) = x.shape\n",
        "    return CenterCrop([H, W])(encoder_features)\n",
        "  \n",
        "  def forward(self, x, encoder_features):\n",
        "    for i in range(len(self.up_convs)):\n",
        "      x = self.up_convs[i](x)\n",
        "      encoder_feature = self.crop(encoder_features[i], x)\n",
        "      x = cat([x, encoder_feature], dim=1)\n",
        "      x = self.decoder_blocks[i](x)\n",
        "    return x\n",
        "\n",
        "class UNet(Module):\n",
        "  def __str__(self) -> str:\n",
        "    return 'UNet'\n",
        "\n",
        "  def __init__(self, encoder_channels=(3, 16, 32, 64), decoder_channels=(64, 32, 16), classes=1, retain_dim=True, output_size=(512, 512)):\n",
        "    super(UNet, self).__init__()\n",
        "    self.encoder = Encoder(encoder_channels)\n",
        "    self.decoder = Decoder(decoder_channels)\n",
        "    self.head = Conv2d(decoder_channels[-1], classes, 1)\n",
        "    self.retain_dim = retain_dim\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoder_features = self.encoder(x)\n",
        "    decoder_features = self.decoder(encoder_features[::-1][0], encoder_features[::-1][1:])\n",
        "    map = self.head(decoder_features)\n",
        "    if self.retain_dim:\n",
        "      map = functional.interpolate(map, self.output_size)\n",
        "    return map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGFr3l_-QMM_"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ucYAtGS5QLO8"
      },
      "outputs": [],
      "source": [
        "class CNN(Module):\n",
        "  def __str__(self) -> str:\n",
        "    return 'CNN'\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.model = Sequential(                # input:  1   x 512 x 512\n",
        "        Conv2d(1, 64, 3),                   # output: 64  x 510 x 510\n",
        "        ReLU(),                             #\n",
        "        MaxPool2d(2),                       # output: 64  x 255 x 255\n",
        "        Conv2d(64, 64, 3),                  # output: 64  x 253 x 253\n",
        "        ReLU(),                             #\n",
        "        MaxPool2d(2),                       # output: 64  x 126 x 126\n",
        "        Flatten(),                          # output: 1016064\n",
        "        Linear(1016064, 1),\n",
        "        ReLU(),\n",
        "        Linear(1, 1) # TODO: needs to be poisson output\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x).t().squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3NPIPXp3Pac"
      },
      "source": [
        "##Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U00pSVfc0M4f"
      },
      "outputs": [],
      "source": [
        "# Adapted from: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "  def __init__(self, dataframe, transformations = None):\n",
        "    self.dataframe = dataframe\n",
        "    self.transformations = transformations\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe.index)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image = cv2.cvtColor(cv2.imread(self.dataframe.iloc[index]['image_paths']), cv2.COLOR_BGR2RGB)\n",
        "    mask = cv2.threshold(cv2.imread(self.dataframe.iloc[index]['mask_paths'], cv2.IMREAD_GRAYSCALE), 150, 255, cv2.THRESH_BINARY)[1]\n",
        "    if self.transformations is not None:\n",
        "      image = self.transformations(image)\n",
        "      mask = self.transformations(mask)\n",
        "    return (image, mask, self.dataframe.iloc[index]['labels'], self.dataframe.index[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5vpYefL-2Cx"
      },
      "source": [
        "#Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IRSA6DHOysx"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XYnttTP3gxRX"
      },
      "outputs": [],
      "source": [
        "# Adapted from: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n",
        "\n",
        "def train(model, t_loader, v_loader, loss_func, opt, metric=None, epochs=N_EPOCHS):\n",
        "  history = pd.DataFrame({\n",
        "      't': {'losses':[], 'metrics':[]},\n",
        "      'v': {'losses':[], 'metrics':[]}\n",
        "  })\n",
        "\n",
        "  if metric != None:\n",
        "    metric.to(DEVICE)\n",
        "\n",
        "  loop = tqdm(range(epochs))\n",
        "  for e in loop:\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for (i, (x, y, c, _)) in enumerate(t_loader):\n",
        "      (x, y, c) = (x.to(DEVICE), y.to(DEVICE), c.to(DEVICE))\n",
        "      feature = (y if str(model) == 'CNN' else x)\n",
        "      target = (c if str(model) == 'CNN' else y)\n",
        "\n",
        "      pred = model(feature)\n",
        "      loss = loss_func(pred, target)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      if loss.requires_grad:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      if metric != None:\n",
        "        metric.update(pred, target)\n",
        "        \n",
        "    history['t']['losses'].append(mean(losses))\n",
        "    if metric != None:\n",
        "      history['t']['metrics'].append(metric.compute().cpu().detach().numpy())\n",
        "      metric.reset()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      \n",
        "      losses = []\n",
        "      for (x, y, c, _) in v_loader:\n",
        "        (x, y, c) = (x.to(DEVICE), y.to(DEVICE), c.to(DEVICE))\n",
        "        feature = (y if str(model) == 'CNN' else x)\n",
        "        target = (c if str(model) == 'CNN' else y)\n",
        "\n",
        "        pred = model(feature)\n",
        "        loss = loss_func(pred, target)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if metric != None:\n",
        "          metric.update(pred, target)\n",
        "      \n",
        "      history['v']['losses'].append(mean(losses))\n",
        "      if metric != None:\n",
        "        history['v']['metrics'].append(metric.compute().cpu().detach().numpy())\n",
        "        metric.reset()\n",
        "\n",
        "    loop.set_description(f'E({e+1}/{N_EPOCHS}) Training {model}, Train Loss: {history[\"t\"][\"losses\"][-1]:.4f}, Test Loss: {history[\"v\"][\"losses\"][-1]:.4f}')\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT6QjNVRPTZY"
      },
      "source": [
        "##Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fCq193FpPKr-"
      },
      "outputs": [],
      "source": [
        "def predict(model, loader):\n",
        "  preds = []\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for (epoch, (x, y, _, name)) in enumerate(loader):\n",
        "      (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "      feature = (y if str(model) == 'CNN' else x)\n",
        "      pred = model(feature)\n",
        "      if str(model) == 'CNN':\n",
        "        for batch, c in enumerate(pred.cpu().detach()):\n",
        "          preds.append({'names':name[batch], 'image_paths':str(next(IMAGES_PATH.glob(f'{name[batch]}{EXT}'))), 'mask_paths':None, 'labels':c.numpy()})\n",
        "      else:\n",
        "        for batch, img in enumerate(pred.cpu().detach()):\n",
        "          # Adapted from: https://discuss.pytorch.org/t/converting-tensors-to-images/99482/5\n",
        "          img = torch.sigmoid(img)\n",
        "          threshold = (img.min() + img.max()) * 0.5\n",
        "          img_out = torch.where(img > threshold, 0.9, 0.1)\n",
        "          out_path = f'{OUTPUT_DIR}/{name[batch]}.{OUTPUT_FORMAT}'\n",
        "          save_image(img_out, out_path)\n",
        "\n",
        "          # If we want greyscale use this instead (or perhaps we should use both so we can analyze better)\n",
        "          # img = torch.sigmoid(img)\n",
        "          # img = 1./(img.max()-img.min()) * img + 1.*img.min() / (img.min()-img.max())\n",
        "          # out_path = f'{OUTPUT_DIR}/{name[batch]}.{OUTPUT_FORMAT}'\n",
        "          # save_image(img, out_path)\n",
        "          \n",
        "          preds.append({'names':name[batch], 'image_paths':str(next(IMAGES_PATH.glob(f'{name[batch]}{EXT}'))), 'mask_paths':out_path, 'labels':None})\n",
        "  return pd.DataFrame(preds).set_index('names').fillna(np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drmFROkfN8j8"
      },
      "source": [
        "#Data Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQyhPsJBNNt"
      },
      "source": [
        "##Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ97hPjhBhTu"
      },
      "source": [
        "Creates the DataFrame and loads the image paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1bhM-gJ8uLKN"
      },
      "outputs": [],
      "source": [
        "def load_data(images_path=IMAGES_PATH, masks_path=MASKS_PATH, csv_path=LABELS_PATH):\n",
        "  with open(csv_path) as csv_file:\n",
        "    rows = [row for row in csv.reader(csv_file)]\n",
        "    return pd.DataFrame({\n",
        "        'names'        : [row[0].split('.')[0] for row in rows],\n",
        "        'image_paths'  : [str(next(images_path.glob(row[0]))) for row in rows],\n",
        "        'mask_paths'   : [str(next(masks_path.glob(row[0]))) for row in rows],\n",
        "        'labels'       : [int(row[1]) for row in rows]\n",
        "    }).set_index('names')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qkhL2f1YU8FB",
        "outputId": "0895c144-c3d4-4c77-ae9d-8de6f86067d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              image_paths  \\\n",
              "names                                                       \n",
              "sarpol_001  sarpol-zahab-tents/data/images/sarpol_001.jpg   \n",
              "sarpol_002  sarpol-zahab-tents/data/images/sarpol_002.jpg   \n",
              "sarpol_003  sarpol-zahab-tents/data/images/sarpol_003.jpg   \n",
              "sarpol_004  sarpol-zahab-tents/data/images/sarpol_004.jpg   \n",
              "sarpol_005  sarpol-zahab-tents/data/images/sarpol_005.jpg   \n",
              "...                                                   ...   \n",
              "sarpol_252  sarpol-zahab-tents/data/images/sarpol_252.jpg   \n",
              "sarpol_253  sarpol-zahab-tents/data/images/sarpol_253.jpg   \n",
              "sarpol_254  sarpol-zahab-tents/data/images/sarpol_254.jpg   \n",
              "sarpol_255  sarpol-zahab-tents/data/images/sarpol_255.jpg   \n",
              "sarpol_256  sarpol-zahab-tents/data/images/sarpol_256.jpg   \n",
              "\n",
              "                                               mask_paths  labels  \n",
              "names                                                              \n",
              "sarpol_001  sarpol-zahab-tents/data/labels/sarpol_001.jpg       5  \n",
              "sarpol_002  sarpol-zahab-tents/data/labels/sarpol_002.jpg       0  \n",
              "sarpol_003  sarpol-zahab-tents/data/labels/sarpol_003.jpg       2  \n",
              "sarpol_004  sarpol-zahab-tents/data/labels/sarpol_004.jpg       0  \n",
              "sarpol_005  sarpol-zahab-tents/data/labels/sarpol_005.jpg       0  \n",
              "...                                                   ...     ...  \n",
              "sarpol_252  sarpol-zahab-tents/data/labels/sarpol_252.jpg       0  \n",
              "sarpol_253  sarpol-zahab-tents/data/labels/sarpol_253.jpg       0  \n",
              "sarpol_254  sarpol-zahab-tents/data/labels/sarpol_254.jpg       0  \n",
              "sarpol_255  sarpol-zahab-tents/data/labels/sarpol_255.jpg       0  \n",
              "sarpol_256  sarpol-zahab-tents/data/labels/sarpol_256.jpg       0  \n",
              "\n",
              "[256 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29cc0490-f472-4ab2-be44-6c7e900dc82d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_paths</th>\n",
              "      <th>mask_paths</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>names</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sarpol_001</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_001.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_001.jpg</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_002</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_002.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_002.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_003</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_003.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_003.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_004</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_004.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_004.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_005</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_005.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_005.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_252</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_252.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_252.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_253</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_253.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_253.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_254</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_254.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_254.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_255</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_255.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_255.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sarpol_256</th>\n",
              "      <td>sarpol-zahab-tents/data/images/sarpol_256.jpg</td>\n",
              "      <td>sarpol-zahab-tents/data/labels/sarpol_256.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>256 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29cc0490-f472-4ab2-be44-6c7e900dc82d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29cc0490-f472-4ab2-be44-6c7e900dc82d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29cc0490-f472-4ab2-be44-6c7e900dc82d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwAZOPs5tIwZ"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeyDX72HMc4i"
      },
      "source": [
        "##Create Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YRPIwxAFVkNd"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(load_data(), test_size=TEST_SPLIT, random_state=42)\n",
        "\n",
        "transformations = transforms.Compose([transforms.ToPILImage(), transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)), transforms.ToTensor()])\n",
        "\n",
        "train_dataset = SegmentationDataset(train_data, transformations)\n",
        "test_dataset = SegmentationDataset(test_data, transformations)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\n",
        "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGoAtv-FKDyc",
        "outputId": "bc1ff9fd-8505-4757-c330-00f61c22e5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test to Train Ratio: 39/217 = 0.1797\n"
          ]
        }
      ],
      "source": [
        "test_count = len(test_data)\n",
        "train_count = len(train_data)\n",
        "print(f'Test to Train Ratio: {test_count}/{train_count} = {test_count/train_count:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhIx-cPJjVg"
      },
      "source": [
        "##Load the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vWmNDPG5Jii4"
      },
      "outputs": [],
      "source": [
        "unet = UNet().to(DEVICE)\n",
        "cnn = CNN().to(DEVICE)\n",
        "\n",
        "if False: # TODO: determine the criteria for when we want to load the already created model (of course we also must check if it exists first too)\n",
        "  unet = torch.load(U_MODEL_PATH).to(DEVICE)\n",
        "  cnn = torch.load(C_MODEL_PATH).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxBm2aV6JcFt"
      },
      "source": [
        "##Train the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413,
          "referenced_widgets": [
            "24d05d2919224ae983d3bc75be9772ee",
            "223f1aa68ad94c93bb327cf182df38b7",
            "9ca7fa3e42c64fd6a1e11f0e8e039a6d",
            "b64c040646934f5bb5840e1a417b0db6",
            "63c38b06ada044cea36620052df76180",
            "cc6d9f07771544f4a9ba034be5589139",
            "16f7dbaf30af4accb7c72c2f2f4a7e32",
            "56bff7b9e58f4458a0534ff658e9fb1f",
            "8dd1475e6d6943b39e7708580f9ade15",
            "c79f241356b642d2bcb93585ea44a9f6",
            "135f4e6c7ea845bdada5a3b984e8dc03"
          ]
        },
        "id": "2jtA5i4-g89g",
        "outputId": "828b82c4-1d7a-4a28-8a79-9bfbce2426b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24d05d2919224ae983d3bc75be9772ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0d9032f272c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINIT_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBinaryJaccardIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mc_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoissonNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINIT_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TODO: CNN causing an exception (Assertation Error about can only test a child process; test by using num_workers = 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check https://discuss.pytorch.org/t/error-while-multiprocessing-in-dataloader/46845/7 for guidance on this error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bf066bf2ee0b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, t_loader, v_loader, loss_func, opt, metric, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected all tensors to be on\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/classification/confusion_matrix.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m\"\"\"Update state with predictions and targets.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0m_binary_confusion_matrix_tensor_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_binary_confusion_matrix_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mconfmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_binary_confusion_matrix_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/functional/classification/confusion_matrix.py\u001b[0m in \u001b[0;36m_binary_confusion_matrix_tensor_validation\u001b[0;34m(preds, target, ignore_index)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Check that target only contains {0,1} values or value in ignore_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_values\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0munique_values\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    798\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         output, inverse_indices, counts = torch._unique2(\n\u001b[0m\u001b[1;32m    800\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "u_results = train(unet, train_loader, test_loader, BCEWithLogitsLoss(), Adam(unet.parameters(), lr=INIT_LR), BinaryJaccardIndex())\n",
        "c_results = train(cnn, train_loader, test_loader, PoissonNLLLoss(), Adam(cnn.parameters(), lr=INIT_LR))\n",
        "\n",
        "# TODO: CNN causing an exception (Assertation Error about can only test a child process; test by using num_workers = 0)\n",
        "# Check https://discuss.pytorch.org/t/error-while-multiprocessing-in-dataloader/46845/7 for guidance on this error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85AhAWC3JWij"
      },
      "source": [
        "##Save the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa1u-8hiJVqT"
      },
      "outputs": [],
      "source": [
        "torch.save(unet, f'{OUTPUT_DIR}/{U_MODEL_NAME}')\n",
        "torch.save(cnn, f'{OUTPUT_DIR}/{C_MODEL_NAME}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMaUJCVaSwpr"
      },
      "source": [
        "#Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln7yNeFLObCk"
      },
      "outputs": [],
      "source": [
        "# UNet\n",
        "plt.plot(u_results['t']['losses'], label='train loss')\n",
        "plt.plot(u_results['v']['losses'], label='test loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('UNet')\n",
        "plt.savefig(f'{OUTPUT_DIR}/unet_loss.{OUTPUT_FORMAT}')\n",
        "plt.close()\n",
        "\n",
        "# UNet\n",
        "plt.plot(u_results['t']['metrics'], label='train metrics')\n",
        "plt.plot(u_results['v']['metrics'], label='test metrics')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('UNet')\n",
        "plt.savefig(f'{OUTPUT_DIR}/unet_metrics.{OUTPUT_FORMAT}')\n",
        "plt.close()\n",
        "\n",
        "# CNN\n",
        "plt.plot(c_results['t']['losses'], label='train loss')\n",
        "plt.plot(c_results['v']['losses'], label='test loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "# plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('CNN')\n",
        "plt.savefig(f'{OUTPUT_DIR}/cnn_loss.{OUTPUT_FORMAT}')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80DXIxqj_Dp"
      },
      "source": [
        "#Experimenting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHmYiC6ij8Cj"
      },
      "outputs": [],
      "source": [
        "transformations = transforms.Compose([transforms.ToPILImage(), transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)), transforms.ToTensor()])\n",
        "dataset = SegmentationDataset(load_data(), transformations)\n",
        "loader = DataLoader(dataset, shuffle=False, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\n",
        "\n",
        "u_pred = predict(unet, loader)\n",
        "\n",
        "# Turn predictions into a loader for the CNN to work using UNet's output\n",
        "u_pred_dataset = SegmentationDataset(u_pred, transformations)\n",
        "u_pred_loader = DataLoader(u_pred_dataset, shuffle=False, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count())\n",
        "\n",
        "c_pred = predict(cnn, u_pred_loader)\n",
        "out_pred = u_pred.combine_first(c_pred).sort_index()\n",
        "\n",
        "\n",
        "# TODO: need to turn output into a loader\n",
        "output_dataset = SegmentationDataset(out_pred, transformations)\n",
        "output_loader = DataLoader(output_dataset, shuffle=False, batch_size=int(math.sqrt(len(out_pred))), pin_memory=PIN_MEMORY, num_workers=os.cpu_count()) # Batch of 16 since each row is 16 images long"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_pred"
      ],
      "metadata": {
        "id": "kC5lpEd-WHhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outputs"
      ],
      "metadata": {
        "id": "R2uqbhfs4xxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "\n",
        "*   Tile the images (features, ground truths, and predictions)\n",
        "*   Overlay (tiled features <- tiled ground truths; tiles features <- tiled predictions)\n",
        "*   Maybe even overlay the tent counts? Just as a number with a boarder to start, but eventually as a heatmap?\n",
        "\n",
        "\n",
        "The tiler should concat each row of the image (thus the loader must have a batch size which is the square root of the loader, in this case 16). Then once each image in the row is concatinated it should concat each row with the previous results.\n",
        "\n"
      ],
      "metadata": {
        "id": "ynruPGd17_9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: not tiling right; we seem to be getting diagionals correct but otherwise images\n",
        "# are in the wrong places?\n",
        "\n",
        "def tile(loader, output_path_x, output_path_y):\n",
        "  output_x = output_y = None\n",
        "  for (x, y, _, _) in tqdm(loader):\n",
        "    if output_x == None and output_y == None:\n",
        "      output_x = torch.cat(tuple(x), 2)\n",
        "      output_y = torch.cat(tuple(y), 2)\n",
        "    else:\n",
        "      output_x = torch.cat((output_x, torch.cat(tuple(x), 2)), 1)\n",
        "      output_y = torch.cat((output_y, torch.cat(tuple(y), 2)), 1)\n",
        "  save_image(output_x, output_path_x)\n",
        "  save_image(output_y, output_path_y)\n",
        "  print('Done.')\n",
        "  \n",
        "tile(output_loader, f'{OUTPUT_DIR}/tiled_x.{OUTPUT_FORMAT}', f'{OUTPUT_DIR}/tiled_pred.{OUTPUT_FORMAT}')"
      ],
      "metadata": {
        "id": "5rf4zlID40_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GocOvAjoz3Zy",
        "_hjSDyxitHih",
        "7Vfi33_GtEj2",
        "65mcYUXjBpMI",
        "GGFr3l_-QMM_",
        "F3NPIPXp3Pac",
        "6IRSA6DHOysx",
        "TT6QjNVRPTZY",
        "ndQyhPsJBNNt",
        "DeyDX72HMc4i",
        "enhIx-cPJjVg",
        "85AhAWC3JWij"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMIT8epFvT/1UEhbUbAGatJ",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24d05d2919224ae983d3bc75be9772ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_223f1aa68ad94c93bb327cf182df38b7",
              "IPY_MODEL_9ca7fa3e42c64fd6a1e11f0e8e039a6d",
              "IPY_MODEL_b64c040646934f5bb5840e1a417b0db6"
            ],
            "layout": "IPY_MODEL_63c38b06ada044cea36620052df76180"
          }
        },
        "223f1aa68ad94c93bb327cf182df38b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc6d9f07771544f4a9ba034be5589139",
            "placeholder": "​",
            "style": "IPY_MODEL_16f7dbaf30af4accb7c72c2f2f4a7e32",
            "value": "E(2/200) Training UNet, Train Loss: 0.4524, Test Loss: 0.4315:   1%"
          }
        },
        "9ca7fa3e42c64fd6a1e11f0e8e039a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56bff7b9e58f4458a0534ff658e9fb1f",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dd1475e6d6943b39e7708580f9ade15",
            "value": 2
          }
        },
        "b64c040646934f5bb5840e1a417b0db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c79f241356b642d2bcb93585ea44a9f6",
            "placeholder": "​",
            "style": "IPY_MODEL_135f4e6c7ea845bdada5a3b984e8dc03",
            "value": " 2/200 [00:32&lt;33:33, 10.17s/it]"
          }
        },
        "63c38b06ada044cea36620052df76180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc6d9f07771544f4a9ba034be5589139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f7dbaf30af4accb7c72c2f2f4a7e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56bff7b9e58f4458a0534ff658e9fb1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd1475e6d6943b39e7708580f9ade15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c79f241356b642d2bcb93585ea44a9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "135f4e6c7ea845bdada5a3b984e8dc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}