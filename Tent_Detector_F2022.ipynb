{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dYTTLPwOBzU-",
        "FgE5iT1ORHpM",
        "ME1KAdsil0iw"
      ],
      "authorship_tag": "ABX9TyMvXTKa56iEi9CeWF1qOsOP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zacharylazzara/tent-detection/blob/main/Tent_Detector_F2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqRutHqon6Du"
      },
      "source": [
        "**Referenced Materials**\n",
        "\n",
        "* https://amaarora.github.io/2020/09/13/unet.html\n",
        "* https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5\n",
        "* https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n",
        "* https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "The majority of the UNet implementation comes from the referenced YouTube video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJ00OGoP_mr"
      },
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfDQrD8JoJwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f8e053-a4e2-43f4-af72-3681644e0f6f"
      },
      "source": [
        "# Imports\n",
        "%cd /content\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "!mkdir -p drive/\n",
        "drive.mount('drive/')\n",
        "\n",
        "# !mkdir -p drive/MyDrive/Thesis/content/ # TODO: remove this line its for debugging\n",
        "\n",
        "\n",
        "!pip install segmentation-models-pytorch -q\n",
        "!pip install -U albumentations -q\n",
        "\n",
        "import math\n",
        "import sys\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import csv\n",
        "import random\n",
        "import matplotlib.patches as mpatches\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as plticker\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import pickle\n",
        "import shutil"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[K     |████████████████████████████████| 97 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 39.7 MB/s \n",
            "\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 123 kB 9.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMwAUsCK4KUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c173853b-1a9a-47c7-a503-56342aeaf7b3"
      },
      "source": [
        "# Initialize Environment\n",
        "%cd drive/MyDrive/Thesis/content/\n",
        "\n",
        "%env SRC_DIR        = sarpol-zahab-tents/\n",
        "%env DATA_DIR       = data/\n",
        "%env TRAINING_DIR   = data/training/\n",
        "%env VALIDATION_DIR = data/validation/\n",
        "%env TRAIN_IMG_DIR  = data/training/features/\n",
        "%env TRAIN_LBL_DIR  = data/training/targets/\n",
        "%env VAL_IMG_DIR    = data/validation/features/\n",
        "%env VAL_LBL_DIR    = data/validation/targets/\n",
        "%env OUTPUT_DIR     = output/\n",
        "\n",
        "SRC_DIR             = os.environ.get(\"SRC_DIR\")\n",
        "DATA_DIR            = os.environ.get(\"DATA_DIR\")\n",
        "TRAINING_DIR        = os.environ.get(\"TRAINING_DIR\")\n",
        "VALIDATION_DIR      = os.environ.get(\"VALIDATION_DIR\")\n",
        "TRAIN_IMG_DIR       = os.environ.get(\"TRAIN_IMG_DIR\")\n",
        "TRAIN_LBL_DIR       = os.environ.get(\"TRAIN_LBL_DIR\")\n",
        "VAL_IMG_DIR         = os.environ.get(\"VAL_IMG_DIR\")\n",
        "VAL_LBL_DIR         = os.environ.get(\"VAL_LBL_DIR\")\n",
        "OUTPUT_DIR          = os.environ.get(\"OUTPUT_DIR\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Thesis/content\n",
            "env: SRC_DIR=sarpol-zahab-tents/\n",
            "env: DATA_DIR=data/\n",
            "env: TRAINING_DIR=data/training/\n",
            "env: VALIDATION_DIR=data/validation/\n",
            "env: TRAIN_IMG_DIR=data/training/features/\n",
            "env: TRAIN_LBL_DIR=data/training/targets/\n",
            "env: VAL_IMG_DIR=data/validation/features/\n",
            "env: VAL_LBL_DIR=data/validation/targets/\n",
            "env: OUTPUT_DIR=output/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8HWwxdUvigf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2d362b-ef5b-4bcf-ba2c-c13873e44aff"
      },
      "source": [
        "# Initialize Directories\n",
        "%%bash\n",
        "echo \"Working in Directory: $(pwd)\"\n",
        "\n",
        "if [ -d 'sample_data' ]; then\n",
        "  rm -r sample_data\n",
        "fi\n",
        "\n",
        "if [ ! -d $SRC_DIR ] && [ ! -d $DATA_DIR ]; then\n",
        "  git clone https://github.com/tofighi/sarpol-zahab-tents.git\n",
        "fi\n",
        "\n",
        "if [ ! -d $DATA_DIR ]; then\n",
        "  mkdir -p $DATA_DIR\n",
        "  cp $SRC_DIR/data/sarpol_counts.csv $DATA_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $TRAIN_IMG_DIR ]; then\n",
        "  mkdir -p $TRAIN_IMG_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $TRAIN_LBL_DIR ]; then\n",
        "  mkdir -p $TRAIN_LBL_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $VAL_IMG_DIR ]; then\n",
        "  mkdir -p $VAL_IMG_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $VAL_LBL_DIR ]; then\n",
        "  mkdir -p $VAL_LBL_DIR\n",
        "fi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working in Directory: /content/drive/MyDrive/Thesis/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'sarpol-zahab-tents'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZz2bAEBQHxX"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7WMebFa5Uyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee540c3-b485-43ae-f2ee-b6575d3876b0"
      },
      "source": [
        "# Configuration\n",
        "# AUTO_DOWNLOAD = True\n",
        "\n",
        "# Dataset\n",
        "TEST_SIZE = 0.3\n",
        "RANDOM_STATE = 123\n",
        "ALLOW_IRRELEVANT = True # If images don't have tents do we want to throw them out or not?\n",
        "SARPOL = False # do we want to download the very large Sarpol image?\n",
        "\n",
        "TILE = True # If we want to generate the final big image\n",
        "EPOCH_TILE = True # Tile during epochs instead of after\n",
        "\n",
        "GRAYSCALE = False # Print tents as grayscale or not\n",
        "\n",
        "LIVE_VISUALIZE = False\n",
        "\n",
        "# TODO: lets just load all data as validation data to generate the map data and such\n",
        "\n",
        "RAW_IMAGE_DIR = f\"{SRC_DIR}data/images/\"\n",
        "RAW_LABEL_DIR = f\"{SRC_DIR}data/labels/\"\n",
        "\n",
        "# Overview Directories #\n",
        "OVERVIEWS           = f\"{OUTPUT_DIR}overviews/\"\n",
        "ESTIMATE_OVERVIEWS  = f\"{OVERVIEWS}estimates/\"\n",
        "TARGET_OVERVIEWS    = f\"{OVERVIEWS}targets/\"\n",
        "FEATURE_OVERVIEWS   = f\"{OVERVIEWS}features/\"\n",
        "\n",
        "OVERLAYS            = f\"{OVERVIEWS}overlays/\"\n",
        "\n",
        "ESTIMATE_OVERLAYS   = f\"{OVERLAYS}estimates/\"\n",
        "TARGET_OVERLAYS     = f\"{OVERLAYS}targets/\"\n",
        "\n",
        "HEATMAPS            = f\"{OVERVIEWS}heatmaps/\"\n",
        "ESTIMATE_HEATMAPS   = f\"{HEATMAPS}estimates/\"\n",
        "TARGET_HEATMAPS     = f\"{HEATMAPS}targets/\"\n",
        "########################\n",
        "\n",
        "# Pickle Outputs #######\n",
        "PICKLES       = f\"{OUTPUT_DIR}pickles/\"\n",
        "RAW_PICKLE    = f\"{PICKLES}raw_dataset.pkl\"\n",
        "DATA_PICKLE   = f\"{PICKLES}dataset.pkl\"\n",
        "RESULT_PICKLE = f\"{PICKLES}results.pkl\"\n",
        "\n",
        "RESULT_CSV    = f\"{PICKLES}results.csv\"\n",
        "########################\n",
        "\n",
        "TENT_CSV      = f\"{DATA_DIR}sarpol_counts.csv\"\n",
        "LOAD_FROM_CSV = True\n",
        "\n",
        "BLOB_LOCALIZATION = False\n",
        "KMEANS_LOCALIZATION = False # Causes error in kmeans localization when saving training data?\n",
        "\n",
        "# Display Limit\n",
        "DISP_LIMIT    = 1 # Maximum number of images to display\n",
        "DISP_RESULTS  = True\n",
        "DISP_SCALE    = 2#150 # Amount to integer divide displayed figure scale by (set to 1 to disable); useful if the notebook keeps crashing\n",
        "\n",
        "# Images\n",
        "IMG_FORMAT    = \"png\"\n",
        "BRIGHTNESS    = 0.5 # set to 1 for no dimming\n",
        "\n",
        "# Checkpoints\n",
        "CHECKPOINT    = \"checkpoint.pt\"\n",
        "\n",
        "\n",
        "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4 # 1x10^-4 = 0.0001\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE    = 1#5 # Batch size defines the prediction batch; set to 1 if we want individual files\n",
        "NUM_EPOCHS    = 50 #100\n",
        "NUM_WORKERS   = 2\n",
        "IMAGE_HEIGHT  = 512\n",
        "IMAGE_WIDTH   = 512\n",
        "PIN_MEMORY    = True\n",
        "LOAD_MODEL    = True\n",
        "\n",
        "# Convolution Settings\n",
        "C_KERNEL      = 3     # This is the matrix that slides across the image (we define matrix size, so kernel = 3 means 3x3 matrix that slides across the image)\n",
        "C_STRIDE      = 1     # Number of pixels the kernel slides over the input (how many pixels we move the filter at a time)\n",
        "C_PADDING     = 1     # Sometimes the filter doesn't perfectly fit the input image, in which case we can pad with 0s or drop the part of the image that didn't fit (called valid padding)\n",
        "C_BIAS        = False # Bias is false in this case because we're using BatchNorm2d (bias would be canceled by the batch norm, so we set it to false)\n",
        "R_INPLACE     = True  #\n",
        "\n",
        "# UNet Settings\n",
        "IN_CHANNELS   = 3\n",
        "OUT_CHANNELS  = 1 # We're doing binary image segmentation (because our masks are black and white), so we can output a single channel\n",
        "U_FEATURES    = [64, 128, 256, 512] # Features come from the architecture (the number above the boxes)\n",
        "\n",
        "# UNet Pool Settings\n",
        "P_KERNEL      = 2\n",
        "P_STRIDE      = 2\n",
        "\n",
        "# Final layer kernel size\n",
        "F_KERNEL      = 1 # Because we're outputting the final image here\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy; this is used to prevent division by zero, so we want a small value that doesn't have much impact on the results\n",
        "EPSILON = sys.float_info.epsilon\n",
        "\n",
        "\n",
        "\n",
        "# Define and create directory structures\n",
        "def dirs(root=OUTPUT_DIR):\n",
        "  datasets = [\"validation\", \"training\"]\n",
        "  dirs = {}\n",
        "  for dataset in datasets:\n",
        "    data_dir = VALIDATION_DIR if dataset == 'validation' else TRAINING_DIR\n",
        "    dataset_dir = f\"{root}{dataset}/\"\n",
        "    dirs[dataset] = {\"features\":       f\"{data_dir}features/\",\n",
        "                     \"targets\":        f\"{data_dir}targets/\",\n",
        "                     \"estimates\":      f\"{dataset_dir}estimates/\"}\n",
        "  return dirs\n",
        "DIRECTORIES = dirs()\n",
        "for dataset in DIRECTORIES.values():\n",
        "  for dir in dataset.values():\n",
        "    if not os.path.exists(dir):\n",
        "      os.makedirs(dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUAznM-v8VOd"
      },
      "source": [
        "def load(path):\n",
        "  imgs = []\n",
        "  for filepath in sorted(glob(path)):\n",
        "    with Image.open(filepath) as img:\n",
        "      imgs.append((np.asarray(img), os.path.basename(path).split(f\".\", 1)[0]))\n",
        "  return imgs #np.array([(np.asarray(Image.open(path)), os.path.basename(path).split(f\".\", 1)[0]) for path in sorted(glob(path))])\n",
        "  \n",
        "def load_filenames(path):\n",
        "  return np.array([os.path.basename(path).split(f\".\", 1) for path in sorted(glob(path))])\n",
        "\n",
        "MAP = None\n",
        "if SARPOL:\n",
        "  !gdown --id 1-YUbFjwFL2G5r8TudKS0XK56BjJ0KsTK\n",
        "  MAP = load(\"sarpol.png\")[0][0]\n",
        "  print(f\"Sarpol Shape: {MAP.shape}\\n\")\n",
        "\n",
        "# if LOAD_MODEL:\n",
        "# TODO: if we don't have the checkpoint already, then download it; otherwise use the one we have\n",
        "#   !gdown --id 1ulHrgSyNwo1X2WYmQyeb4lIxIIJ4Xs-G"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxu_27u7Qsdq"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Fds7OAxbPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8ae557-2e72-4301-f06e-6df720fdd252"
      },
      "source": [
        "# Preparing Data\n",
        "def find_square_coordinates(record, row = 0, max_records = 256):\n",
        "  row_length = int(math.sqrt(max_records))\n",
        "  if record >= row_length:\n",
        "    return find_square_coordinates(record - row_length, row + 1, max_records)\n",
        "  else:\n",
        "    return (row, record)\n",
        "\n",
        "def percent(sample, total):\n",
        "  return f\"({sample}/{total}) = {((sample/total)*100):.2f}%\"\n",
        "\n",
        "def populate_dirs(dataset):\n",
        "  print(\"\\nPopulating training and validation directories...\")\n",
        "\n",
        "  # TODO: this should delete everything in the dirs before running, to ensure runs are identical\n",
        "\n",
        "  data_dir = {\"training\":{\"img\":TRAIN_IMG_DIR, \"lbl\":TRAIN_LBL_DIR}, \"validation\":{\"img\":VAL_IMG_DIR, \"lbl\":VAL_LBL_DIR}}\n",
        "\n",
        "  data_loop = tqdm(dataset)\n",
        "  for subset in data_loop:\n",
        "    for record in dataset[subset][\"x\"]:\n",
        "      for dir in record[\"dir\"]:\n",
        "        input = f\"{record['dir'][dir]}{record['id']}.{record['format']}\"\n",
        "        output = f\"{data_dir[subset][dir]}{record['id']}.{IMG_FORMAT}\"\n",
        "        data_loop.set_description(f\"Input: {input}, Output: {output}\")\n",
        "        with Image.open(input) as img:\n",
        "          img.save(output)\n",
        "  print(\"\\n\\nDone.\")\n",
        "\n",
        "raw_dataset = {\"x\":[], \"y\":[]}\n",
        "dataset = {\"training\":{\"x\":[], \"y\":[]}, \"validation\":{\"x\":[], \"y\":[]}}\n",
        "if os.path.exists(PICKLES):\n",
        "  print(\"Loading pickles...\")\n",
        "  with open(RAW_PICKLE, \"rb\") as raw_pickle:\n",
        "    raw_dataset = pickle.load(raw_pickle)\n",
        "  with open(DATA_PICKLE, \"rb\") as data_pickle:\n",
        "    dataset = pickle.load(data_pickle)\n",
        "  print(\"Done.\")\n",
        "else:\n",
        "  src_imgs = load_filenames(f\"{RAW_IMAGE_DIR}*\")\n",
        "  src_lbls = load_filenames(f\"{RAW_LABEL_DIR}*\")\n",
        "  if src_imgs.shape[0] == src_lbls.shape[0]:\n",
        "    n = src_lbls.shape[0]\n",
        "\n",
        "    with open(TENT_CSV, newline='') as csvfile:\n",
        "      raw_dataset[\"y\"] = list(csv.reader(csvfile))\n",
        "    for index in range(n):\n",
        "      if src_imgs[index][0] == src_lbls[index][0]:\n",
        "        id = src_imgs[index][0]\n",
        "        format = src_imgs[index][1]\n",
        "\n",
        "        raw_dataset[\"x\"].append({\n",
        "          \"dir\":{\n",
        "            \"img\":f\"{RAW_IMAGE_DIR}\",\n",
        "            \"lbl\":f\"{RAW_LABEL_DIR}\"\n",
        "          },\n",
        "          \"id\":f\"{id}\",\n",
        "          \"format\":f\"{format}\",\n",
        "          \"tile_coordinates\":find_square_coordinates(index)\n",
        "        })\n",
        "      else:\n",
        "        print(\"ERROR: ID mismatch!\")\n",
        "    os.makedirs(PICKLES)\n",
        "    with open(RAW_PICKLE, \"wb\") as output:\n",
        "      pickle.dump(raw_dataset, output, pickle.HIGHEST_PROTOCOL)\n",
        "  else:\n",
        "    print(\"ERROR: Shape mismatch!\")\n",
        "  dataset[\"training\"][\"x\"], dataset[\"validation\"][\"x\"], dataset[\"training\"][\"y\"], dataset[\"validation\"][\"y\"] = train_test_split(raw_dataset[\"x\"], raw_dataset[\"y\"], test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "  with open(DATA_PICKLE, \"wb\") as output:\n",
        "    pickle.dump(dataset, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "irrelevant = raw_dataset[\"y\"].count(0)\n",
        "total = len(raw_dataset[\"x\"])\n",
        "relevant = total - irrelevant\n",
        "print(f\"\\nRaw Dataset Size: {total}\\nRelevant Percentage: {percent(relevant, total)}, Irrelevant Percentage: {percent(irrelevant, total)}\")\n",
        "print(f\"Allow Irrelevant Data? {'YES' if ALLOW_IRRELEVANT else 'NO'}\")\n",
        "\n",
        "training_data = len(dataset[\"training\"][\"x\"])\n",
        "validation_data = len(dataset[\"validation\"][\"x\"])\n",
        "total_usable = training_data + validation_data\n",
        "print(f\"\\nTraining Percentage: {percent(training_data, total_usable)}, Validation Percent: {percent(validation_data, total_usable)}\")\n",
        "\n",
        "if os.path.exists(SRC_DIR):\n",
        "  populate_dirs(dataset)\n",
        "elif os.path.exists(DATA_DIR):\n",
        "  print(f\"\\nData directory '{DATA_DIR}' has already been populated in a previous run.\")\n",
        "else:\n",
        "  print(f\"\\nSource directory '{SRC_DIR}' not found, unable to populate '{DATA_DIR}'!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Raw Dataset Size: 256\n",
            "Relevant Percentage: (256/256) = 100.00%, Irrelevant Percentage: (0/256) = 0.00%\n",
            "Allow Irrelevant Data? YES\n",
            "\n",
            "Training Percentage: (179/256) = 69.92%, Validation Percent: (77/256) = 30.08%\n",
            "\n",
            "Populating training and validation directories...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input: sarpol-zahab-tents/data/labels/sarpol_245.jpg, Output: data/validation/targets/sarpol_245.png: 100%|██████████| 2/2 [00:34<00:00, 17.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleanup\n",
        "%%bash\n",
        "if [ -d $SRC_DIR ]; then\n",
        "  rm -r $SRC_DIR\n",
        "fi"
      ],
      "metadata": {
        "id": "CKAaKjxzWmfY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTTLPwOBzU-"
      },
      "source": [
        "# UNet\n",
        "\n",
        "**U-Net Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL5QPK-wA03N"
      },
      "source": [
        "The first half of the architecture is the down-sampling process. At each stage, two 3x3 convolutions are used. The output is then pooled and becomes the input for the next 3x3 convolution and so on, until the bottom of this diagram is reached.\n",
        "\n",
        "The second half of the architecture is the up-sampling process, which is a reflection of the down-sampling process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXGvzzfPpYut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04ef147-cb39-4b54-e46e-56acc6ac5b86"
      },
      "source": [
        "# UNet Model\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "# Referenced https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential (\n",
        "        nn.Conv2d(in_channels, out_channels, C_KERNEL, C_STRIDE, C_PADDING, bias=C_BIAS), # This is a same convolution (input height*width = output height*width)\n",
        "        nn.BatchNorm2d(out_channels), # BatchNorm accelerates training by normalizing inputs by re-centering and re-scaling\n",
        "        nn.ReLU(inplace=R_INPLACE), # ReLU is Rectified Linear Unit; essentially it makes it so negative inputs are discarded and positive inputs are passed through\n",
        "\n",
        "        # Now we do this a second time but with out_channels to out_channels\n",
        "        nn.Conv2d(out_channels, out_channels, C_KERNEL, C_STRIDE, C_PADDING, bias=C_BIAS),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=R_INPLACE),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "# DoubleConv is everything in the first node of the architecture (before pooling)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=U_FEATURES):\n",
        "    super(UNet, self).__init__()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.downs = nn.ModuleList() # Stores the convolutional layers; it's a list, but using ModuleList lets us use BatchNorm2d\n",
        "    self.pool = nn.MaxPool2d(kernel_size=P_KERNEL, stride=P_STRIDE) # Pooling layer will be used inbetween, in forwarding method\n",
        "    # Note that the pooling layer will require out inputs to be perfectly divisible by 2 because we're doing a stride of 2\n",
        "    # Example: 161 x 161 -> MaxPool -> 80 x 80 -> Upsample -> 160 x 160; in this case, we couldn't concatinate the two as they need the same width and height for concat (161x161 is input, 160x160 is output)\n",
        "    # If input is perfectly dividible by 16 then this issue won't happen (16 because its 4 steps, all dividing by 2 (16/(4*2) = 2))\n",
        "    # In order to keep the system general, we can either pad the image or crop the image so that it works even if image size isn't perfectly divisible by 16\n",
        "\n",
        "\n",
        "    # Down-Sampling part of UNet\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature)) # Mapping some input (in the UNet architecture example, it maps 1 to 64 for the first node)\n",
        "      in_channels = feature\n",
        "\n",
        "    # Up-Sampling Part of UNet\n",
        "    # At 10:20 or so in the video he mentions transposed convoltions that may be a better method to this part, but we'll use similar approach to UNet paper for now\n",
        "    # We're using the reversed list of features because we're going from the bottom up now\n",
        "    for feature in reversed(features):\n",
        "      \n",
        "      # In_channels is feature*2 here, output is feature\n",
        "      self.ups.append(\n",
        "          nn.ConvTranspose2d( \n",
        "              feature*2, feature, kernel_size=P_KERNEL, stride=P_STRIDE\n",
        "          )\n",
        "      )\n",
        "      self.ups.append(DoubleConv(feature*2, feature)) # Because we go up then do two convs then go up then do two convs etc\n",
        "\n",
        "    # Bottom Layer\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2) # We're using features[-1] because we want the bottom feature\n",
        "\n",
        "    # Now we do 1x1 conv which doesn't change height or width just number of channels\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=F_KERNEL)\n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = [] # We skip connections for each stage in the architecture, as we use this part later on\n",
        "    \n",
        "    # This loop does all the down-sampling steps until the final step just before the bottom layer (aka bottleneck)\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    skip_connections = skip_connections[::-1] # we wanna go backwards in order when we're doing our concatination; the highest resolution image is the first one; to make things easier, we'll just reverse this list\n",
        "\n",
        "    # We're using a step of 2 here because we're going up then double conv each iteration\n",
        "    for i in range(0, len(self.ups), 2):\n",
        "      x = self.ups[i](x) # doing ConvTranspose2d here\n",
        "      skip_connection = skip_connections[i//2] # // is integer division; we're doing a step of one ordering here\n",
        "\n",
        "      # Dealing with images that are not perfectly dividible\n",
        "      # TODO: perhaps we should look into scaling instead of cropping or padding, or perhaps we should scale before we put the image into the system, so that this is not relevant?\n",
        "      if x.shape != skip_connection.shape:\n",
        "        x = TF.resize(x, size=skip_connections.shape[2:]) # we're taking out height and width here; basically, we're resizing the image if it doesn't fit\n",
        "\n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1) # dim 1 is the channel dimension; we're concationating these things along the channel dimension\n",
        "      x = self.ups[i+1](concat_skip) # running it through a double conv\n",
        "    \n",
        "    return self.final_conv(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Testing the implementation thus far\n",
        "def test():\n",
        "  x = torch.randn((3, 1, 160, 160)) # 3 is number of images (batch size), 1 is number of channels, 160 is image height, 160 is image width\n",
        "  model = UNet(in_channels=1, out_channels=1)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "  print(x.shape)\n",
        "  assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   test()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 160, 160])\n",
            "torch.Size([3, 1, 160, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgE5iT1ORHpM"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXGKRQ6VUz-J"
      },
      "source": [
        "# TentDataset\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "# Data directory should be in the format:\n",
        "# data\n",
        "#   train_images\n",
        "#   train_masks\n",
        "#   val_images\n",
        "#   val_masks\n",
        "\n",
        "\n",
        "# TODO: probably needs to be able to support the case where the target is none?\n",
        "\n",
        "class TentDataset(Dataset):\n",
        "  def __init__(self, image_dir=TRAIN_IMG_DIR, mask_dir=TRAIN_LBL_DIR, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = os.path.join(self.image_dir, self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) # We use L since mask is grey scale\n",
        "    mask[mask == 255.0] = 1.0 # Preprocessing; we change this cuz we're using a sigmoid on the last activation for probability of white pixel, so this makes it work better?\n",
        "\n",
        "    id = os.path.basename(image_path).replace(f\".{IMG_FORMAT}\", \"\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image=image, mask=mask)\n",
        "      image = augmentations[\"image\"]\n",
        "      mask = augmentations[\"mask\"]\n",
        "\n",
        "    return image, mask, id"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpmSRqG4RFsn"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQKV-QNfXNu"
      },
      "source": [
        "# Utilities\n",
        "def save_checkpoint(state, filename=CHECKPOINT):\n",
        "  print(\"=> Saving checkpoint\\n\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "  print(\"=> Loading checkpoint\\n\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(train_dir, train_maskdir, val_dir, val_maskdir, batch_size, train_transform, val_transform, num_workers=4, pin_memory=True):\n",
        "  train_ds = TentDataset(image_dir=train_dir, mask_dir=train_maskdir, transform=train_transform)\n",
        "  train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n",
        "  val_ds = TentDataset(image_dir=val_dir, mask_dir=val_maskdir, transform=val_transform)\n",
        "  val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
        "  return train_loader, val_loader\n",
        "\n",
        "def similarity(estimate, target): # Returns the Jaccard Index of estiamte and target (how similar they are)\n",
        "  return (EPSILON+(estimate * target).sum()) / (EPSILON+(estimate + target).sum())\n",
        "\n",
        "def load_image(id, directory=OUTPUT_DIR):\n",
        "  image = {\"record\":None, \"tile_coord\":None, \"feature\":None, \"target\":None, \"estimate\":None, \"visualization\":None}\n",
        "  for dirs in DIRECTORIES.values():\n",
        "    target_path = f\"{dirs['targets']}{id}.{IMG_FORMAT}\"\n",
        "    estimate_path = f\"{dirs['estimates']}{id}.{IMG_FORMAT}\"\n",
        "    feature_path = f\"{dirs['features']}{id}.{IMG_FORMAT}\"\n",
        "    visualization_path = f\"{dirs['visualizations']}{id}.{IMG_FORMAT}\"\n",
        "\n",
        "    image[\"record\"] = int(id.split('_')[1])-1\n",
        "    image[\"tile_coord\"] = find_square_coordinates(image[\"record\"])\n",
        "\n",
        "    if os.path.exists(target_path):\n",
        "      with Image.open(target_path).convert(\"RGBA\") as target_img:\n",
        "        image[\"target\"] = target_img\n",
        "    \n",
        "    if os.path.exists(estimate_path):\n",
        "      with Image.open(estimate_path).convert(\"RGBA\") as estimate_img:\n",
        "        image[\"estimate\"] = estimate_img\n",
        "    \n",
        "    if os.path.exists(feature_path):\n",
        "      with Image.open(feature_path).convert(\"RGBA\") as feature_img:\n",
        "        image[\"feature\"] = feature_img\n",
        "    \n",
        "    if os.path.exists(visualization_path):\n",
        "      with Image.open(visualization_path).convert(\"RGB\") as visualization_img:\n",
        "        image[\"visualization\"] = visualization_img\n",
        "  return image\n",
        "\n",
        "def save_heatmap(msk_path, heatmap_output_path, heatmap_format=IMG_FORMAT):\n",
        "  print(f\"\\nSaving heatmap to '{heatmap_output_path}'...\")\n",
        "  msk = None\n",
        "  with Image.open(msk_path).convert(\"RGB\") as mask:\n",
        "    msk = np.array(mask)\n",
        "  blur = cv2.GaussianBlur(msk, (55, 55), 0)\n",
        "  invert = cv2.bitwise_not(blur) # Invert so that colour map works correctly\n",
        "  heatmap = cv2.applyColorMap(invert, cv2.COLORMAP_JET)\n",
        "  output = Image.fromarray(heatmap)\n",
        "  output.save(f\"{heatmap_output_path}\", format=f\"{heatmap_format}\")\n",
        "\n",
        "def save_overlay(img_path, msk_path, overlay_output_path, overlay_format=IMG_FORMAT):\n",
        "  img = None\n",
        "  msk = None\n",
        "  with Image.open(img_path).convert(\"RGBA\") as image:\n",
        "    img = np.array(image)\n",
        "  with Image.open(msk_path).convert(\"RGBA\") as mask:\n",
        "    msk = np.array(mask)\n",
        "\n",
        "  for channel in range(1, 2):\n",
        "    msk[msk[:,:,channel] > 0, channel] = 0\n",
        "\n",
        "  overlay = cv2.addWeighted(img, 1, msk, 1, 0)\n",
        "\n",
        "  output = Image.fromarray(overlay)\n",
        "  output.save(f\"{overlay_output_path}\", format=f\"{overlay_format}\")\n",
        "\n",
        "# Maybe combine both tent counts and return a dictionary with target and estimate values\n",
        "def gt_tent_count(id):\n",
        "  with open(TENT_CSV) as csvfile:\n",
        "    for row in list(csv.reader(csvfile)):\n",
        "      if row[0].split(\".\", 1)[0] == id:\n",
        "        return int(row[1])\n",
        "  return None\n",
        "\n",
        "def tent_count(id):\n",
        "  # TODO: implement this (convolutional neural network here?)\n",
        "  return None\n",
        "\n",
        "\n",
        "def get_record(id):\n",
        "  return int(id.split('_')[1])-1\n",
        "\n",
        "def save_all(loaders, model, directory=OUTPUT_DIR, device=DEVICE):\n",
        "  model.eval()\n",
        "  results = {}\n",
        "  for dataset, loader in loaders.items():\n",
        "    dirs = DIRECTORIES[dataset]\n",
        "\n",
        "    loader_progress = tqdm(loader)\n",
        "    loader_progress.set_description(f\"Saving {dataset} dataset\")\n",
        "    for feature, target, id in loader_progress:\n",
        "      id = id[0]\n",
        "      feature = feature.to(device)\n",
        "      with torch.no_grad():\n",
        "        estimate = torch.sigmoid(model(feature))\n",
        "        estimate = (estimate > 0.5).float()\n",
        "\n",
        "      torchvision.utils.save_image(estimate, f\"{dirs['estimates']}{id}.{IMG_FORMAT}\")\n",
        "\n",
        "      # NOTE: The line below was messing up the target; we should check that the target is loading correctly when used by the model to verify the problem is now fixed\n",
        "      # torchvision.utils.save_image(target.unsqueeze(1), f\"{dirs['targets']}{id}.{IMG_FORMAT}\") #might not need unsqueeze? or maybe we should use it on estimate too?\n",
        "\n",
        "      accuracy = similarity(estimate.detach().cpu(), target).item()\n",
        "\n",
        "      # TODO: store transformation in here too if possible\n",
        "      # TODO: perhaps results should follow the form of the raw dataset, and maybe just update it?\n",
        "      # TODO: results should probably store paths as well, but storing just IDs will do for now\n",
        "      results[id] = {\n",
        "        \"record\":          get_record(id),\n",
        "        \"tile_coordinates\":find_square_coordinates(get_record(id)),\n",
        "        \"dataset\":         dataset,\n",
        "        \"target_count\":    gt_tent_count(id),\n",
        "        \"estimate_count\":  tent_count(id),\n",
        "        \"mask_similarity\": accuracy\n",
        "      }\n",
        "\n",
        "      loader_progress.set_description(f\"Saving [{dataset}]: {id}, {results[id]}\")\n",
        "    \n",
        "  model.train()\n",
        "  with open(RESULT_PICKLE, \"wb\") as output:\n",
        "    pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  # with open(RESULT_CSV, \"w\") as output:\n",
        "  #   writer = csv.DictWriter(output, fieldnames=results.keys())\n",
        "  #   writer.writeheader()\n",
        "  #   writer.writerows(results)\n",
        "\n",
        "  # TODO: use this instead of the above commented out code (adapt the code to make it work, it comes from https://stackoverflow.com/questions/56018692/converting-pkl-file-to-csv-file)\n",
        "  # with open(RESULT_PICKLE, \"rb\") as input:\n",
        "  #     object = pkl.load(input)\n",
        "  # df = pd.DataFrame(object)\n",
        "  # df.T.to_csv(r'results.csv')\n",
        "\n",
        "  shutil.make_archive(\"tent_counts\", \"tar\", directory)\n",
        "  print(\"Saved.\\n\")\n",
        "\n",
        "def tile_images(output_name, tile_dir, max_records=256):\n",
        "  print(f\"Beginning Image Tiling ({tile_dir})...\")\n",
        "  square_length = int(math.sqrt(max_records))\n",
        "  tile_paths = [[None for x in range(square_length)] for y in range(square_length)]\n",
        "  \n",
        "  for dirs in DIRECTORIES.values():\n",
        "    for path, _, tile_names in os.walk(dirs[tile_dir]):\n",
        "      for tile_name in tile_names:\n",
        "        tile_index = find_square_coordinates(int(tile_name.split('_')[1].split('.')[0])-1)\n",
        "        tile_paths[tile_index[0]][tile_index[1]] = (path+tile_name)\n",
        "  \n",
        "  if any(tile_paths[0]):\n",
        "    row_loop = tqdm(range(len(tile_paths)))\n",
        "    img = None\n",
        "    w = h = 0\n",
        "    for r in row_loop:                      # Rows\n",
        "      for c in range(len(tile_paths[0])):   # Columns\n",
        "        tile = None\n",
        "        with Image.open(tile_paths[r][c]) as img_tile:\n",
        "          tile = img_tile.convert(\"RGB\")\n",
        "\n",
        "        if img == None:\n",
        "          img = tile\n",
        "        if r == 0:\n",
        "          w += tile.width\n",
        "        if c == 0:\n",
        "          h += tile.height\n",
        "\n",
        "        dst = Image.new('RGB', (w, h))\n",
        "        dst.paste(img, (0, 0))\n",
        "        dst.paste(tile, (c*tile.width, r*tile.height))\n",
        "        img = dst\n",
        "        img.save(output_name)\n",
        "\n",
        "        row_loop.set_description(f\"W:{w}, H:{h}, Index: ({r}, {c}), Input: {tile_paths[r][c]}, Output: {output_name}\")\n",
        "    print(f\"Finished Tiling, Output: {output_name}\")\n",
        "  else:\n",
        "    raise Exception(f\"Selected directory '{tile_dir}' is empty!\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Counting"
      ],
      "metadata": {
        "id": "ME1KAdsil0iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://github.com/Thundertung/Book-Price-regression-CNNs/blob/main/Judging%20a%20book%20by%20its%20cover.ipynb\n",
        "# and from https://www.youtube.com/watch?v=nU_T2PPigUQ&t=531s\n",
        "# TODO: adapt for use here\n",
        "\n",
        "if False: # TODO: set to True if you want to run this\n",
        "  from sklearn.preprocessing import LabelEncoder    #For encoding categorical variables\n",
        "  from sklearn.model_selection import train_test_split #For splitting of data\n",
        "  #All tensorflow utilities for creating, training and working with a CNN\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
        "  from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "  from tensorflow.keras.losses import categorical_crossentropy\n",
        "  from tensorflow.keras.optimizers import Adam\n",
        "  from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "  from tensorflow.keras.models import load_model\n",
        "\n",
        "  new_train_set = []\n",
        "  new_train_count = np.empty(len(training_set))\n",
        "  for i in range(len(training_set)):\n",
        "    new_train_set.append(np.asarray(training_set[i]['img']))\n",
        "    new_train_count[i] = training_set[i]['num']\n",
        "\n",
        "  new_train_set = np.array(new_train_set)\n",
        "\n",
        "  print(f\"Training Set: {np.shape(new_train_set)}\")\n",
        "  print(f\"Training Count: {np.shape(new_train_count)}\")\n",
        "\n",
        "  new_val_set = []\n",
        "  new_val_count = np.empty(len(validation_set))\n",
        "  for i in range(len(validation_set)):\n",
        "    new_val_set.append(np.asarray(validation_set[i]['img']))\n",
        "    new_val_count[i] = validation_set[i]['num']\n",
        "\n",
        "  new_val_set = np.array(new_val_set)\n",
        "\n",
        "  print(f\"Val Set: {np.shape(new_val_set)}\")\n",
        "  print(f\"Val Count: {np.shape(new_val_count)}\")\n",
        "\n",
        "  # new_train_set.reshape(new_train_set.shape[0],new_train_set.shape[1],new_train_set.shape[2],new_train_set.shape[3])\n",
        "  # new_train_set.reshape(new_train_set.shape[0],new_train_set.shape[1],new_train_set.shape[2],new_train_set.shape[3])\n",
        "\n",
        "\n",
        "  print(f\"Training Set: {np.shape(new_train_set)}\")\n",
        "\n",
        "  # input_shape = np.shape(new_train_set.reshape(np.shape(new_train_set)[1], np.shape(new_train_set)[2]))\n",
        "  input_shape = np.shape(new_train_set[0])\n",
        "  print(f\"Input Shape: {input_shape}\")\n",
        "\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # First conv\n",
        "  model.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "\n",
        "  # Second conv\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Hidden layer\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu', input_shape = input_shape))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # # model.add(MaxPool2D(strides=(2,2)))\n",
        "  # model.add(Dropout(0.25))\n",
        "  # model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # # model.add(MaxPool2D(strides=(2,2)))\n",
        "  # model.add(Dropout(0.25))\n",
        "\n",
        "  # model.add(Flatten())\n",
        "  # # model.add(Dense(100, activation='relu'))\n",
        "  # model.add(Dropout(0.25))\n",
        "\n",
        "  # # model.add(Dense(1024, activation='relu'))\n",
        "  # model.add(Dropout(0.4))\n",
        "  # model.add(Dense(1, activation='linear'))\n",
        "\n",
        "  learning_rate = 0.001\n",
        "\n",
        "  model.compile(loss = categorical_crossentropy,\n",
        "                optimizer = Adam(learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # TODO: the sets are an array of dictionaries where each dictionary contains the values we want\n",
        "\n",
        "  # new_train_set = []\n",
        "  # new_train_count = np.empty(len(training_set))\n",
        "  # for i in range(len(training_set)):\n",
        "  #   new_train_set.append(np.asarray(training_set[i]['img']))\n",
        "  #   new_train_count[i] = training_set[i]['num']\n",
        "\n",
        "  # print(np.shape(new_train_set))\n",
        "\n",
        "  # new_val_set = []\n",
        "  # new_val_count = np.empty(len(validation_set))\n",
        "  # for i in range(len(validation_set)):\n",
        "  #   new_val_set.append(np.asarray(validation_set[i]['img']))\n",
        "  #   new_val_count[i] = validation_set[i]['num']\n",
        "\n",
        "  #training_set, validation_set, csv_training_set, csv_validation_set = train_test_split(dataset, csv_dataset, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "  model.fit(new_train_set, new_train_count, epochs=15, validation_data=(new_val_set, new_val_count))\n",
        "\n",
        "\n",
        "  # history = model.fit( X_train, Y_train, \n",
        "  #                     epochs = 15, batch_size = 100, \n",
        "  #                     callbacks=[save_best2], verbose=1, \n",
        "  #                    validation_data = (X_val, Y_price_val))\n",
        "else:\n",
        "  print(\"Skipping for now; change if to True if you want to run this block\")"
      ],
      "metadata": {
        "id": "OC260aihvCus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e65d716-ed80-4f5d-b2b7-7b6df967f1d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping for now; change if to True if you want to run this block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8NEUSCTRAxr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD1wOKe_Z_eJ"
      },
      "source": [
        "# Model Training\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "\n",
        "\n",
        "# This whole function trains one epoch\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "  loop = tqdm(loader) # tqdm gives us a progress bar\n",
        "  loop.set_description(\"Training\")\n",
        "\n",
        "  for batch_idx, (data, targets, id) in enumerate(loop):\n",
        "    loop.set_description(f\"Training on {id[0]}\")\n",
        "    data = data.to(device=DEVICE)\n",
        "    targets = targets.float().unsqueeze(1).to(device=DEVICE) #might not need to make it float since it might already be float? Also, unsqueeze is used cuz we're adding a channel\n",
        "\n",
        "    # Forward\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(data)\n",
        "      loss = loss_fn(predictions, targets)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "def main():\n",
        "\n",
        "  # TODO: might need to uncomment the rotations and flips; commented out for now to test why some images are misaligned\n",
        "  train_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    ###### TODO: Need to undo these transformations when we want to stitch the images\n",
        "    A.Rotate(limit=35, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1),\n",
        "    ######\n",
        "    A.Normalize(\n",
        "      mean=[0.0, 0.0, 0.0],\n",
        "      std=[1.0, 1.0, 1.0],\n",
        "      max_pixel_value=255.0 # This basically just divides by 255, so we get a value between 0 and 1\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "  ])\n",
        "\n",
        "  val_transforms = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Normalize(\n",
        "      mean=[0.0, 0.0, 0.0],\n",
        "      std=[1.0, 1.0, 1.0],\n",
        "      max_pixel_value=255.0 # This basically just divides by 255, so we get a value between 0 and 1\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "  ])\n",
        "\n",
        "  model = UNet(in_channels=3, out_channels=1).to(DEVICE) # if we wanted multiclass segmentation we'd change our channels and change our loss function to cross entropy loss\n",
        "  loss_fn = nn.BCEWithLogitsLoss() # We're not doing sigmoid on the output of model which is why we're using this here\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  train_loader, val_loader = get_loaders(\n",
        "      TRAIN_IMG_DIR,\n",
        "      TRAIN_LBL_DIR,\n",
        "      VAL_IMG_DIR,\n",
        "      VAL_LBL_DIR,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      val_transforms,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY\n",
        "  )\n",
        "\n",
        "  if os.path.exists(CHECKPOINT):\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    if DEVICE == \"cuda\":\n",
        "      load_checkpoint(torch.load(CHECKPOINT), model)\n",
        "    else:\n",
        "      load_checkpoint(torch.load(CHECKPOINT, map_location=lambda storage, loc: storage), model) # From https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/4\n",
        "  else:\n",
        "    print(\"No checkpoint found!\\n\")\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler() # I think this is where we get the warning about running on CPU; should try to address this warning\n",
        "  \n",
        "\n",
        "  if not os.path.exists(OVERVIEWS):\n",
        "    # TODO: ideally we should be doing this when we initialize all directories instead of here\n",
        "    os.makedirs(OVERVIEWS)\n",
        "    os.makedirs(OVERLAYS)\n",
        "    os.makedirs(ESTIMATE_OVERVIEWS)\n",
        "    os.makedirs(ESTIMATE_OVERLAYS)\n",
        "    os.makedirs(TARGET_OVERVIEWS)\n",
        "    os.makedirs(TARGET_OVERLAYS)\n",
        "    os.makedirs(ESTIMATE_HEATMAPS)\n",
        "    os.makedirs(TARGET_HEATMAPS)\n",
        "  feature_overview_path = f\"{OVERVIEWS}features_overview.{IMG_FORMAT}\"\n",
        "  if EPOCH_TILE or TILE:\n",
        "    gt_overview_path = f\"{TARGET_OVERVIEWS}gt_overview.{IMG_FORMAT}\"\n",
        "    gt_overlay_path = f\"{TARGET_OVERLAYS}gt_overlay.{IMG_FORMAT}\"\n",
        "    gt_heatmap_path = f\"{TARGET_HEATMAPS}gt_heatmap.{IMG_FORMAT}\"\n",
        "\n",
        "    # TODO: make a function for this? since we repeat ourselves here\n",
        "    if os.path.exists(feature_overview_path):\n",
        "      print(f\"{feature_overview_path} already exists, skipping...\")\n",
        "    else:\n",
        "      tile_images(feature_overview_path, \"features\") # Feature tiles\n",
        "    \n",
        "    if os.path.exists(gt_overview_path):\n",
        "      print(f\"{gt_overview_path} already exists, skipping...\")\n",
        "    else:\n",
        "      tile_images(gt_overview_path, \"targets\") # Ground-truth tiles\n",
        "    \n",
        "    if os.path.exists(gt_overlay_path):\n",
        "      print(f\"{gt_overlay_path} already exists, skipping...\")\n",
        "    else:\n",
        "      save_overlay(feature_overview_path, gt_overview_path, gt_overlay_path)\n",
        "\n",
        "    if os.path.exists(gt_heatmap_path):\n",
        "      print(f\"{gt_heatmap_path} already exists, skipping...\")\n",
        "    else:\n",
        "      save_heatmap(gt_overview_path, gt_heatmap_path)\n",
        "  epoch_offset = 0\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    if epoch > 0: print(\"\\n\") \n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "    # Save Model\n",
        "    checkpoint = {\"state_dict\":model.state_dict(), \"optimizer\":optimizer.state_dict()}\n",
        "    save_checkpoint(checkpoint)\n",
        "    save_all({\"validation\":val_loader, \"training\":train_loader}, model)\n",
        "\n",
        "    if EPOCH_TILE:\n",
        "      overview_path = f\"{ESTIMATE_OVERVIEWS}overview_{epoch}.{IMG_FORMAT}\"\n",
        "      overlay_path = f\"{ESTIMATE_OVERLAYS}overlay_{epoch}.{IMG_FORMAT}\"\n",
        "      heatmap_path = f\"{ESTIMATE_HEATMAPS}heatmap_{epoch}.{IMG_FORMAT}\"\n",
        "      while os.path.exists(overview_path): # Will only run on the first pass; this ensures we don't overwrite old overviews\n",
        "        print(f\"Path '{overview_path}' already exists.\")\n",
        "        epoch_offset += 1\n",
        "        print(f\"Offsetting epoch {epoch} by +{epoch_offset}\")\n",
        "        overview_path = f\"{ESTIMATE_OVERVIEWS}overview_{epoch+epoch_offset}.{IMG_FORMAT}\"\n",
        "        overlay_path = f\"{ESTIMATE_OVERLAYS}overlay_{epoch+epoch_offset}.{IMG_FORMAT}\"\n",
        "        heatmap_path = f\"{ESTIMATE_HEATMAPS}heatmap_{epoch+epoch_offset}.{IMG_FORMAT}\"\n",
        "      tile_images(overview_path, \"estimates\") #\"ve_tiles\") # Estimate tiles\n",
        "      save_overlay(feature_overview_path, overview_path, overlay_path)\n",
        "      save_heatmap(overview_path, heatmap_path)\n",
        "\n",
        "      # if AUTO_DOWNLOAD:\n",
        "      #   archive = f'/content/epoch_{epoch}.zip'\n",
        "      #   shutil.make_archive(archive.split('.')[0], archive.split('.')[1], '.')\n",
        "      #   files.download(archive)\n",
        "      #   os.remove(archive_path)\n",
        "  if TILE and not EPOCH_TILE:\n",
        "    tile_images(f\"{ESTIMATE_OVERVIEWS}overview.{IMG_FORMAT}\", \"estimates\") #\"ve_tiles\") # Estimate tiles"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj7ekOSNAKHC"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qJ6mESs_4Yp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "642a722d-a23f-4b17-bca4-ba31732dc83e"
      },
      "source": [
        "# TODO:\n",
        "# - Stitch the black and white images (outputs from validation and estimation), then overlay the estimate and ground truth\n",
        "# overviews (rather than going tile by tile)\n",
        "#\n",
        "# - Report the accuracy for the overview\n",
        "#\n",
        "# - Estimate tents using pixel values instead of CNN if we can't get CNN running\n",
        "#\n",
        "# - Clean up the directory structure of the output if there's time\n",
        "#\n",
        "# - Don't worry about memory leak for now, it's not priority\n",
        "#\n",
        "# NOTE: Priority is completing manuscript by November 15th. For the findings we just compare the overviews and accuracies and such\n",
        "# between prediction and ground truth.\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found!\n",
            "\n",
            "Beginning Image Tiling (features)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W:8192, H:8192, Index: (15, 15), Input: data/training/features/sarpol_256.png, Output: output/overviews/features_overview.png: 100%|██████████| 16/16 [52:22<00:00, 196.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Tiling, Output: output/overviews/features_overview.png\n",
            "Beginning Image Tiling (targets)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W:8192, H:8192, Index: (15, 15), Input: data/training/targets/sarpol_256.png, Output: output/overviews/targets/gt_overview.png: 100%|██████████| 16/16 [05:15<00:00, 19.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Tiling, Output: output/overviews/targets/gt_overview.png\n",
            "\n",
            "Saving heatmap to 'output/overviews/heatmaps/targets/gt_heatmap.png'...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training on sarpol_181: 100%|██████████| 179/179 [00:29<00:00,  6.12it/s, loss=0.741]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving [validation]: sarpol_245, {'record': 244, 'tile_coordinates': (15, 4), 'dataset': 'validation', 'target_count': 2, 'estimate_count': None, 'mask_similarity': 0.5414519906044006}: 100%|██████████| 77/77 [00:07<00:00,  9.91it/s]\n",
            "Saving [training]: sarpol_089, {'record': 88, 'tile_coordinates': (5, 8), 'dataset': 'training', 'target_count': 0, 'estimate_count': None, 'mask_similarity': 6.4024857816016215e-21}: 100%|██████████| 179/179 [00:17<00:00, 10.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved.\n",
            "\n",
            "Beginning Image Tiling (estimates)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W:8192, H:8192, Index: (15, 15), Input: output/training/estimates/sarpol_256.png, Output: output/overviews/estimates/overview_0.png: 100%|██████████| 16/16 [07:21<00:00, 27.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Tiling, Output: output/overviews/estimates/overview_0.png\n",
            "\n",
            "Saving heatmap to 'output/overviews/heatmaps/estimates/heatmap_0.png'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_886dac77-db3a-439b-980f-b878e7e7f081\", \"epoch_0.zip\", 933080263)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training on sarpol_019: 100%|██████████| 179/179 [00:21<00:00,  8.21it/s, loss=0.591]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving [validation]: sarpol_245, {'record': 244, 'tile_coordinates': (15, 4), 'dataset': 'validation', 'target_count': 2, 'estimate_count': None, 'mask_similarity': 0.6084622740745544}: 100%|██████████| 77/77 [00:07<00:00, 10.43it/s]\n",
            "Saving [training]: sarpol_068, {'record': 67, 'tile_coordinates': (4, 3), 'dataset': 'training', 'target_count': 0, 'estimate_count': None, 'mask_similarity': 1.0992307000470975e-18}: 100%|██████████| 179/179 [00:16<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved.\n",
            "\n",
            "Beginning Image Tiling (estimates)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W:8192, H:8192, Index: (15, 15), Input: output/training/estimates/sarpol_256.png, Output: output/overviews/estimates/overview_1.png: 100%|██████████| 16/16 [06:41<00:00, 25.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Tiling, Output: output/overviews/estimates/overview_1.png\n",
            "\n",
            "Saving heatmap to 'output/overviews/heatmaps/estimates/heatmap_1.png'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b6837cb-5ff1-4501-b202-9d1b66c5f913\", \"epoch_1.zip\", 1146688932)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training on sarpol_132: 100%|██████████| 179/179 [00:21<00:00,  8.23it/s, loss=0.569]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving [validation]: sarpol_245, {'record': 244, 'tile_coordinates': (15, 4), 'dataset': 'validation', 'target_count': 2, 'estimate_count': None, 'mask_similarity': 0.8160625696182251}: 100%|██████████| 77/77 [00:07<00:00, 10.43it/s]\n",
            "Saving [training]: sarpol_118, {'record': 117, 'tile_coordinates': (7, 5), 'dataset': 'training', 'target_count': 116, 'estimate_count': None, 'mask_similarity': 0.9360716342926025}: 100%|██████████| 179/179 [00:16<00:00, 10.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved.\n",
            "\n",
            "Beginning Image Tiling (estimates)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W:8192, H:2560, Index: (4, 9), Input: output/training/estimates/sarpol_074.png, Output: output/overviews/estimates/overview_2.png:  25%|██▌       | 4/16 [00:35<01:47,  8.95s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-88d58f7cff8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDone.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-85b4be6df8dc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0moverlay_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{ESTIMATE_OVERLAYS}overlay_{epoch+epoch_offset}.{IMG_FORMAT}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mheatmap_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{ESTIMATE_HEATMAPS}heatmap_{epoch+epoch_offset}.{IMG_FORMAT}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0mtile_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverview_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"estimates\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\"ve_tiles\") # Estimate tiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m       \u001b[0msave_overlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_overview_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverview_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlay_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0msave_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverview_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmap_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4e52fda61c68>\u001b[0m in \u001b[0;36mtile_images\u001b[0;34m(output_name, tile_dir, max_records)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mrow_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"W:{w}, H:{h}, Index: ({r}, {c}), Input: {tile_paths[r][c]}, Output: {output_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2135\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"IEND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}