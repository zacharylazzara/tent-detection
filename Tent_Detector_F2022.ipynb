{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TmJ00OGoP_mr",
        "dYTTLPwOBzU-"
      ],
      "authorship_tag": "ABX9TyMrWmOfQCSz9VRXB1WjYKx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zacharylazzara/tent-detection/blob/main/Tent_Detector_F2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqRutHqon6Du"
      },
      "source": [
        "**Referenced Materials**\n",
        "\n",
        "* https://amaarora.github.io/2020/09/13/unet.html\n",
        "* https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5\n",
        "* https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n",
        "* https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "The majority of this comes from the referenced YouTube video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJ00OGoP_mr"
      },
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfDQrD8JoJwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192ffbb1-a90b-449f-8936-41e3623a1a67"
      },
      "source": [
        "# Imports\n",
        "!pip install segmentation-models-pytorch -q\n",
        "!pip install -U albumentations -q\n",
        "\n",
        "import math\n",
        "import sys\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import csv\n",
        "import random\n",
        "import matplotlib.patches as mpatches\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as plticker\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import pickle\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 68.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n",
            "\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 123 kB 22.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMwAUsCK4KUe",
        "outputId": "fd0ba32c-849b-411c-f825-5c16a93a1a97"
      },
      "source": [
        "# Initialize Environment\n",
        "%env SRC_DIR       = sarpol-zahab-tents/\n",
        "%env DATA_DIR      = data/\n",
        "%env TRAIN_IMG_DIR = data/train_images/\n",
        "%env TRAIN_LBL_DIR = data/train_labels/\n",
        "%env VAL_IMG_DIR   = data/val_images/\n",
        "%env VAL_LBL_DIR   = data/val_labels/\n",
        "# %env PREDICT_DIR   = predictions/\n",
        "%env OUTPUT_DIR    = output/\n",
        "# %env OUTPUT_MASK_DIR    = output/mask/\n",
        "# %env OUTPUT_DATA_DIR    = output/data/\n",
        "\n",
        "SRC_DIR       = os.environ.get(\"SRC_DIR\")\n",
        "DATA_DIR      = os.environ.get(\"DATA_DIR\")\n",
        "TRAIN_IMG_DIR = os.environ.get(\"TRAIN_IMG_DIR\")\n",
        "TRAIN_LBL_DIR = os.environ.get(\"TRAIN_LBL_DIR\")\n",
        "VAL_IMG_DIR   = os.environ.get(\"VAL_IMG_DIR\")\n",
        "VAL_LBL_DIR   = os.environ.get(\"VAL_LBL_DIR\")\n",
        "# PREDICT_DIR   = os.environ.get(\"PREDICT_DIR\")\n",
        "OUTPUT_DIR    = os.environ.get(\"OUTPUT_DIR\")\n",
        "# OUTPUT_MASK_DIR    = os.environ.get(\"OUTPUT_MASK_DIR\")\n",
        "# OUTPUT_DATA_DIR    = os.environ.get(\"OUTPUT_DATA_DIR\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SRC_DIR=sarpol-zahab-tents/\n",
            "env: DATA_DIR=data/\n",
            "env: TRAIN_IMG_DIR=data/train_images/\n",
            "env: TRAIN_LBL_DIR=data/train_labels/\n",
            "env: VAL_IMG_DIR=data/val_images/\n",
            "env: VAL_LBL_DIR=data/val_labels/\n",
            "env: OUTPUT_DIR=output/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8HWwxdUvigf",
        "outputId": "1f3b91c1-aa08-4b0f-fc63-bbfb72885cd7"
      },
      "source": [
        "# Initialize Directories\n",
        "%%bash\n",
        "echo \"Working in Directory: $(pwd)\"\n",
        "\n",
        "if [ -d 'sample_data' ]; then\n",
        "  rm -r sample_data\n",
        "fi\n",
        "\n",
        "if [ ! -d $SRC_DIR ]; then\n",
        " git clone https://github.com/tofighi/sarpol-zahab-tents.git\n",
        "fi\n",
        "\n",
        "if [ ! -d $DATA_DIR ]; then\n",
        "  mkdir -p $DATA_DIR\n",
        "  cp $SRC_DIR/data/sarpol_counts.csv $DATA_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $TRAIN_IMG_DIR ]; then\n",
        "  mkdir -p $TRAIN_IMG_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $TRAIN_LBL_DIR ]; then\n",
        "  mkdir -p $TRAIN_LBL_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $VAL_IMG_DIR ]; then\n",
        "  mkdir -p $VAL_IMG_DIR\n",
        "fi\n",
        "\n",
        "if [ ! -d $VAL_LBL_DIR ]; then\n",
        "  mkdir -p $VAL_LBL_DIR\n",
        "fi\n",
        "\n",
        "# if [ ! -d $PREDICT_DIR ]; then\n",
        "#   mkdir -p $PREDICT_DIR\n",
        "# fi\n",
        "\n",
        "# if [ ! -d $OUTPUT_MASK_DIR ]; then\n",
        "#   mkdir -p $OUTPUT_MASK_DIR\n",
        "# fi\n",
        "\n",
        "# if [ ! -d $OUTPUT_DATA_DIR ]; then\n",
        "#   mkdir -p $OUTPUT_DATA_DIR\n",
        "# fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working in Directory: /content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'sarpol-zahab-tents'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZz2bAEBQHxX"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7WMebFa5Uyr"
      },
      "source": [
        "# Configuration\n",
        "# Dataset\n",
        "TEST_SIZE = 0.3\n",
        "RANDOM_STATE = 123\n",
        "ALLOW_IRRELEVANT = True # If images don't have tents do we want to throw them out or not?\n",
        "SARPOL = False # do we want to download the very large Sarpol image?\n",
        "\n",
        "GRAYSCALE = False # Print tents as grayscale or not\n",
        "\n",
        "LIVE_VISUALIZE = False\n",
        "\n",
        "# TODO: lets just load all data as validation data to generate the map data and such\n",
        "\n",
        "\n",
        "\n",
        "TENT_CSV      = f\"{DATA_DIR}sarpol_counts.csv\"\n",
        "LOAD_FROM_CSV = True\n",
        "\n",
        "BLOB_LOCALIZATION = False\n",
        "KMEANS_LOCALIZATION = False # Causes error in kmeans localization when saving training data?\n",
        "\n",
        "# Display Limit\n",
        "DISP_LIMIT    = 1 # Maximum number of images to display\n",
        "DISP_RESULTS  = True\n",
        "DISP_SCALE    = 2#150 # Amount to integer divide displayed figure scale by (set to 1 to disable); useful if the notebook keeps crashing\n",
        "\n",
        "# Images\n",
        "IMG_FORMAT    = \"png\"\n",
        "BRIGHTNESS    = 0.5 # set to 1 for no dimming\n",
        "\n",
        "# Checkpoints\n",
        "CHECKPOINT    = \"checkpoint.pt\"\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4 # 1x10^-4 = 0.0001\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE    = 1#5 # Batch size defines the prediction batch; set to 1 if we want individual files\n",
        "NUM_EPOCHS    = 1 #50 #100\n",
        "NUM_WORKERS   = 2\n",
        "IMAGE_HEIGHT  = 512\n",
        "IMAGE_WIDTH   = 512\n",
        "PIN_MEMORY    = True\n",
        "LOAD_MODEL    = True\n",
        "\n",
        "# Convolution Settings\n",
        "C_KERNEL      = 3     # This is the matrix that slides across the image (we define matrix size, so kernel = 3 means 3x3 matrix that slides across the image)\n",
        "C_STRIDE      = 1     # Number of pixels the kernel slides over the input (how many pixels we move the filter at a time)\n",
        "C_PADDING     = 1     # Sometimes the filter doesn't perfectly fit the input image, in which case we can pad with 0s or drop the part of the image that didn't fit (called valid padding)\n",
        "C_BIAS        = False # Bias is false in this case because we're using BatchNorm2d (bias would be canceled by the batch norm, so we set it to false)\n",
        "R_INPLACE     = True  #\n",
        "\n",
        "# UNet Settings\n",
        "IN_CHANNELS   = 3\n",
        "OUT_CHANNELS  = 1 # We're doing binary image segmentation (because our masks are black and white), so we can output a single channel\n",
        "U_FEATURES    = [64, 128, 256, 512] # Features come from the architecture (the number above the boxes)\n",
        "\n",
        "# UNet Pool Settings\n",
        "P_KERNEL      = 2\n",
        "P_STRIDE      = 2\n",
        "\n",
        "# Final layer kernel size\n",
        "F_KERNEL      = 1 # Because we're outputting the final image here\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy; this is used to prevent division by zero, so we want a small value that doesn't have much impact on the results\n",
        "EPSILON = sys.float_info.epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAznM-v8VOd",
        "outputId": "8e995e66-922d-4499-c20c-2d730dcb12ee"
      },
      "source": [
        "# def load_dataset(path):\n",
        "#   dataset = {}\n",
        "#   for path in sorted(glob(path)):\n",
        "#     mask = np.asarray(Image.open(path).convert('1'))\n",
        "#     if ALLOW_IRRELEVANT:\n",
        "#       dataset[os.path.basename(path).split(f\".\", 1)[0]] = mask\n",
        "#     elif np.sum(mask):\n",
        "#       dataset[os.path.basename(path).split(f\".\", 1)[0]] = mask\n",
        "#   return dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: make this use a dictionary instead, since that'll make it easier to get the image we want\n",
        "def load(path):\n",
        "  return np.array([(np.asarray(Image.open(path)), os.path.basename(path).split(f\".\", 1)[0]) for path in sorted(glob(path))])\n",
        "  \n",
        "MAP = None\n",
        "if SARPOL:\n",
        "  !gdown --id 1-YUbFjwFL2G5r8TudKS0XK56BjJ0KsTK\n",
        "  MAP = load(\"sarpol.png\")[0][0]\n",
        "  print(f\"Sarpol Shape: {MAP.shape}\\n\")\n",
        "\n",
        "if LOAD_MODEL:\n",
        "  !gdown --id 1ulHrgSyNwo1X2WYmQyeb4lIxIIJ4Xs-G"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ulHrgSyNwo1X2WYmQyeb4lIxIIJ4Xs-G\n",
            "To: /content/checkpoint.pt\n",
            "100% 373M/373M [00:03<00:00, 93.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxu_27u7Qsdq"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Fds7OAxbPY",
        "outputId": "0b663510-f10d-4779-a9e8-24c08601dd49"
      },
      "source": [
        "# Preparing Data\n",
        "# csv_src = f\"{SRC_DIR}data/sarpol_counts.csv\"\n",
        "# if os.path.exists(csv_src):\n",
        "#   tent_counts = []\n",
        "#   with open(csv_src) as csvfile:\n",
        "#     for row in csv.reader(csvfile):\n",
        "#       tent_counts.append([int(row[1])])\n",
        "\n",
        "#   with open(TENT_CSV, 'w') as csvfile:\n",
        "#     csv.writer(csvfile).writerows(tent_counts)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: ragged array problem somewhere here\n",
        "\n",
        "\n",
        "src_imgs = load(f\"{SRC_DIR}data/images/*\")\n",
        "src_lbls = load(f\"{SRC_DIR}data/labels/*\")\n",
        "\n",
        "dataset = []\n",
        "# Dataset Structure:\n",
        "# [\n",
        "#   0:{\n",
        "#     img:  image\n",
        "#     lbl:  label\n",
        "#     filename: {img: 0.png, lbl: 0.png}\n",
        "#   },\n",
        "#   1:{}, 2:{}, n:{}\n",
        "# ]\n",
        "\n",
        "if src_imgs.shape[0] == src_lbls.shape[0]:\n",
        "  n = src_lbls.shape[0]\n",
        "\n",
        "  for index in range(n):\n",
        "    dataset.append({\n",
        "      \"img\":Image.fromarray(src_imgs[index][0]),\n",
        "      \"lbl\":Image.fromarray(src_lbls[index][0]),\n",
        "      \"filename\":{\n",
        "        \"img\":f\"{src_imgs[index][1]}.{IMG_FORMAT}\",\n",
        "        \"lbl\":f\"{src_lbls[index][1]}.{IMG_FORMAT}\"\n",
        "      },\n",
        "      \"num\": 0\n",
        "    })\n",
        "\n",
        "def find_square_coordinate(record, row = 0, max_records = 256):\n",
        "  row_length = int(math.sqrt(max_records))\n",
        "  if record >= row_length:\n",
        "    return find_square_coordinate(record - row_length, row + 1, max_records)\n",
        "  else:\n",
        "    return (row, record)\n",
        "\n",
        "def save_data(x, y, val=False):\n",
        "  img_dir = TRAIN_IMG_DIR\n",
        "  lbl_dir = TRAIN_LBL_DIR\n",
        "\n",
        "  if val:\n",
        "    img_dir = VAL_IMG_DIR\n",
        "    lbl_dir = VAL_LBL_DIR\n",
        "\n",
        "  x[\"img\"].save(f\"{img_dir}/{x['filename']['img']}\")\n",
        "  x[\"lbl\"].save(f\"{lbl_dir}/{x['filename']['lbl']}\")\n",
        "  x[\"num\"] = y[1]\n",
        "  x[\"record\"] = int(x['filename']['img'].split('.')[0].replace('sarpol_', ''))-1\n",
        "  x[\"tile_coord\"] = find_square_coordinate(x['record'])\n",
        "\n",
        "relevant = 0\n",
        "irrelevant = 0\n",
        "for data in dataset:\n",
        "  if np.asarray(data[\"lbl\"]).sum() > 0:\n",
        "    relevant += 1\n",
        "  else:\n",
        "    irrelevant += 1\n",
        "\n",
        "def percent(sample, total):\n",
        "  return f\"({sample}/{total}) = {((sample/total)*100):.2f}%\"\n",
        "\n",
        "total = len(dataset)\n",
        "print(f\"Dataset Size: {total}\\nRelevant Percentage: {percent(relevant, total)}, Irrelevant Percentage: {percent(irrelevant, total)}\")\n",
        "print(f\"Allow Irrelevant Data? {'YES' if ALLOW_IRRELEVANT else 'NO'}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: looks like we need to include our csv file in the train test split as Y variable?\n",
        "# use sarpol_counts.csv; image names don't match up so we need sarpol_xxx.jpg from the csv?\n",
        "\n",
        "with open(TENT_CSV, newline='') as csvfile:\n",
        "    csv_dataset = list(csv.reader(csvfile))\n",
        "training_set, validation_set, csv_training_set, csv_validation_set = train_test_split(dataset, csv_dataset, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "training_data = 0\n",
        "for i in range(len(training_set)):\n",
        "  if ALLOW_IRRELEVANT or np.asarray(training_set[i][\"lbl\"]).sum() > 0:\n",
        "    save_data(training_set[i], csv_training_set[i], val=False) # TODO: we'll need to ensure training and csv sets stay in sync\n",
        "    training_data += 1\n",
        "# for training in training_set:\n",
        "#   if ALLOW_IRRELEVANT or np.asarray(training[\"lbl\"]).sum() > 0:\n",
        "#     save_data(training, val=False) # TODO: we'll need to ensure training and csv sets stay in sync\n",
        "#     training_data += 1\n",
        "    \n",
        "validation_data = 0\n",
        "for i in range(len(validation_set)):\n",
        "  if ALLOW_IRRELEVANT or np.asarray(validation_set[i][\"lbl\"]).sum() > 0:\n",
        "    save_data(validation_set[i], csv_validation_set[i], val=True)\n",
        "    validation_data += 1\n",
        "# for testing in validation_set:\n",
        "#   if ALLOW_IRRELEVANT or np.asarray(testing[\"lbl\"]).sum() > 0:\n",
        "#     save_data(testing, val=True)\n",
        "#     validation_data += 1\n",
        "\n",
        "\n",
        "total_usable = training_data + validation_data\n",
        "print(f\"\\nTraining Percentage: {percent(training_data, total_usable)}, Validation Percent: {percent(validation_data, total_usable)}\")\n",
        "\n",
        "\n",
        "#debug\n",
        "# print(training_set)\n",
        "\n",
        "# TODO: need to generate augmented data too (or at least ensure augmented data expands the dataset and doesn't just replace it), and also re-train the model on the updated version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: 256\n",
            "Relevant Percentage: (90/256) = 35.16%, Irrelevant Percentage: (166/256) = 64.84%\n",
            "Allow Irrelevant Data? YES\n",
            "\n",
            "Training Percentage: (179/256) = 69.92%, Validation Percent: (77/256) = 30.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ODxRvJdHvno"
      },
      "source": [
        "# Cleanup\n",
        "%%bash\n",
        "if [ -d $SRC_DIR ]; then\n",
        "  rm -r $SRC_DIR\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTTLPwOBzU-"
      },
      "source": [
        "# UNet\n",
        "\n",
        "**U-Net Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL5QPK-wA03N"
      },
      "source": [
        "The first half of the architecture is the down-sampling process. At each stage, two 3x3 convolutions are used. The output is then pooled and becomes the input for the next 3x3 convolution and so on, until the bottom of this diagram is reached.\n",
        "\n",
        "The second half of the architecture is the up-sampling process, which is a reflection of the down-sampling process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXGvzzfPpYut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf91368-d59c-4be6-ae8a-f17d24d797a5"
      },
      "source": [
        "# UNet Model\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "# Referenced https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential (\n",
        "        nn.Conv2d(in_channels, out_channels, C_KERNEL, C_STRIDE, C_PADDING, bias=C_BIAS), # This is a same convolution (input height*width = output height*width)\n",
        "        nn.BatchNorm2d(out_channels), # BatchNorm accelerates training by normalizing inputs by re-centering and re-scaling\n",
        "        nn.ReLU(inplace=R_INPLACE), # ReLU is Rectified Linear Unit; essentially it makes it so negative inputs are discarded and positive inputs are passed through\n",
        "\n",
        "        # Now we do this a second time but with out_channels to out_channels\n",
        "        nn.Conv2d(out_channels, out_channels, C_KERNEL, C_STRIDE, C_PADDING, bias=C_BIAS),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=R_INPLACE),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "# DoubleConv is everything in the first node of the architecture (before pooling)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=U_FEATURES):\n",
        "    super(UNet, self).__init__()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.downs = nn.ModuleList() # Stores the convolutional layers; it's a list, but using ModuleList lets us use BatchNorm2d\n",
        "    self.pool = nn.MaxPool2d(kernel_size=P_KERNEL, stride=P_STRIDE) # Pooling layer will be used inbetween, in forwarding method\n",
        "    # Note that the pooling layer will require out inputs to be perfectly divisible by 2 because we're doing a stride of 2\n",
        "    # Example: 161 x 161 -> MaxPool -> 80 x 80 -> Upsample -> 160 x 160; in this case, we couldn't concatinate the two as they need the same width and height for concat (161x161 is input, 160x160 is output)\n",
        "    # If input is perfectly dividible by 16 then this issue won't happen (16 because its 4 steps, all dividing by 2 (16/(4*2) = 2))\n",
        "    # In order to keep the system general, we can either pad the image or crop the image so that it works even if image size isn't perfectly divisible by 16\n",
        "\n",
        "\n",
        "    # Down-Sampling part of UNet\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature)) # Mapping some input (in the UNet architecture example, it maps 1 to 64 for the first node)\n",
        "      in_channels = feature\n",
        "\n",
        "    # Up-Sampling Part of UNet\n",
        "    # At 10:20 or so in the video he mentions transposed convoltions that may be a better method to this part, but we'll use similar approach to UNet paper for now\n",
        "    # We're using the reversed list of features because we're going from the bottom up now\n",
        "    for feature in reversed(features):\n",
        "      \n",
        "      # In_channels is feature*2 here, output is feature\n",
        "      self.ups.append(\n",
        "          nn.ConvTranspose2d( \n",
        "              feature*2, feature, kernel_size=P_KERNEL, stride=P_STRIDE\n",
        "          )\n",
        "      )\n",
        "      self.ups.append(DoubleConv(feature*2, feature)) # Because we go up then do two convs then go up then do two convs etc\n",
        "\n",
        "    # Bottom Layer\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2) # We're using features[-1] because we want the bottom feature\n",
        "\n",
        "    # Now we do 1x1 conv which doesn't change height or width just number of channels\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=F_KERNEL)\n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = [] # We skip connections for each stage in the architecture, as we use this part later on\n",
        "    \n",
        "    # This loop does all the down-sampling steps until the final step just before the bottom layer (aka bottleneck)\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    skip_connections = skip_connections[::-1] # we wanna go backwards in order when we're doing our concatination; the highest resolution image is the first one; to make things easier, we'll just reverse this list\n",
        "\n",
        "    # We're using a step of 2 here because we're going up then double conv each iteration\n",
        "    for i in range(0, len(self.ups), 2):\n",
        "      x = self.ups[i](x) # doing ConvTranspose2d here\n",
        "      skip_connection = skip_connections[i//2] # // is integer division; we're doing a step of one ordering here\n",
        "\n",
        "      # Dealing with images that are not perfectly dividible\n",
        "      # TODO: perhaps we should look into scaling instead of cropping or padding, or perhaps we should scale before we put the image into the system, so that this is not relevant?\n",
        "      if x.shape != skip_connection.shape:\n",
        "        x = TF.resize(x, size=skip_connections.shape[2:]) # we're taking out height and width here; basically, we're resizing the image if it doesn't fit\n",
        "\n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1) # dim 1 is the channel dimension; we're concationating these things along the channel dimension\n",
        "      x = self.ups[i+1](concat_skip) # running it through a double conv\n",
        "    \n",
        "    return self.final_conv(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Testing the implementation thus far\n",
        "def test():\n",
        "  x = torch.randn((3, 1, 160, 160)) # 3 is number of images (batch size), 1 is number of channels, 160 is image height, 160 is image width\n",
        "  model = UNet(in_channels=1, out_channels=1)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "  print(x.shape)\n",
        "  assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 160, 160])\n",
            "torch.Size([3, 1, 160, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgE5iT1ORHpM"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXGKRQ6VUz-J"
      },
      "source": [
        "# TentDataset\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "# Data directory should be in the format:\n",
        "# data\n",
        "#   train_images\n",
        "#   train_masks\n",
        "#   val_images\n",
        "#   val_masks\n",
        "\n",
        "\n",
        "# TODO: probably needs to be able to support the case where the target is none?\n",
        "\n",
        "class TentDataset(Dataset):\n",
        "  def __init__(self, image_dir=TRAIN_IMG_DIR, mask_dir=TRAIN_LBL_DIR, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = os.path.join(self.image_dir, self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) # We use L since mask is grey scale\n",
        "    mask[mask == 255.0] = 1.0 # Preprocessing; we change this cuz we're using a sigmoid on the last activation for probability of white pixel, so this makes it work better?\n",
        "\n",
        "    id = os.path.basename(image_path).replace(f\".{IMG_FORMAT}\", \"\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image=image, mask=mask)\n",
        "      image = augmentations[\"image\"]\n",
        "      mask = augmentations[\"mask\"]\n",
        "\n",
        "    return image, mask, id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpmSRqG4RFsn"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQKV-QNfXNu"
      },
      "source": [
        "# Utilities\n",
        "def dirs(root=OUTPUT_DIR):\n",
        "  datasets = [\"validation\", \"training\"]\n",
        "  dirs = {}\n",
        "  for dataset in datasets:\n",
        "    dataset_dir = f\"{root}{dataset}/\"\n",
        "    dirs[dataset] = {\"features\":       f\"{dataset_dir}features/\",\n",
        "                     \"targets\":        f\"{dataset_dir}targets/\",\n",
        "                     \"estimates\":      f\"{dataset_dir}estimates/\",\n",
        "                     \"visualizations\": f\"{dataset_dir}visualizations/\",\n",
        "\n",
        "                     \"v_keypoints\":    f\"{dataset_dir}visualizations/keypoints/\",\n",
        "                     \"v_targets\":      f\"{dataset_dir}visualizations/targets/\",\n",
        "                     \"v_estimates\":    f\"{dataset_dir}visualizations/estimates/\",\n",
        "\n",
        "                     \"vt_tiles\":       f\"{dataset_dir}visualizations/targets/tiles/\",\n",
        "                     \"ve_tiles\":       f\"{dataset_dir}visualizations/estimates/tiles/\"}\n",
        "  return dirs\n",
        "\n",
        "DIRECTORIES = dirs()\n",
        "\n",
        "for dataset in DIRECTORIES.values():\n",
        "  for dir in dataset.values():\n",
        "    if not os.path.exists(dir):\n",
        "      os.makedirs(dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def stitch(path):\n",
        "\n",
        "  # imgs = load(f\"{path}*\") # TODO: need to make sure this is sorted by id\n",
        "  imgs = np.array([np.asarray(Image.open(path)) for path in sorted(glob(path))])\n",
        "\n",
        "  rows = imgs.size//2\n",
        "  \n",
        "\n",
        "  imgs = np.reshape(imgs, (rows, -1))\n",
        "\n",
        "  print(imgs)\n",
        "\n",
        "  return cv2.vconcat([cv2.hconcat(img) for img in imgs])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # count = imgs.size\n",
        "  # width = IMAGE_WIDTH * count #imgs[0][0].shape[0] * count\n",
        "  # height = IMAGE_HEIGHT * count #imgs[0][0].shape[1] * count\n",
        "\n",
        "  # TODO: need to know the row length\n",
        "\n",
        "  #n_rows = count // 2 # since this is a square image?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # row = []\n",
        "  # columns = []\n",
        "\n",
        "  # r_index = 0\n",
        "  # i_index = 0\n",
        "  # for img in imgs:\n",
        "  #   if i_index < 1:\n",
        "  #     row.append(img)\n",
        "  #     i_index += 1\n",
        "  #   else:\n",
        "  #     columns.append(cv2.hconcat(row))\n",
        "  #     # row = []\n",
        "  #     r_index += 1\n",
        "  #     i_index = 0\n",
        "\n",
        "  # result = cv2.vconcat(columns)\n",
        "  # result = cv2.hconcat(row)\n",
        "\n",
        "  # print(result)\n",
        "\n",
        "  \n",
        "  # map = Image.new('RGB', (width, height))\n",
        "  # for index, img_id in enumerate(imgs):\n",
        "  #   img = img_id[0]\n",
        "  #   id = img_id[1]\n",
        "  #   numeric_id = int(id.split(f\"_\", 1)[1])\n",
        "\n",
        "  #   cv2.hconcat()\n",
        "\n",
        "\n",
        "    # map.paste(im=img, box=(width*(index),height*(index)))\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    #index > 0 and int(imgs[index-1][1].split(f\"_\", 1)[1])+1 == numeric_id:\n",
        "      # make sure this image is next before we stitch it in\n",
        "      \n",
        "      \n",
        "  # return result\n",
        "\n",
        "\n",
        "\n",
        "# If we want to use kmeans to count number of tents in the binary mask, we\n",
        "# can just find out the number of pixels in a given dot, then get the total number\n",
        "# of white pixels in the scene and divide by that number?\n",
        "\n",
        "def kmeans_localize(img, k):\n",
        "  img = np.argwhere(img.T == 255)\n",
        "  centers = None\n",
        "  if k > 0:\n",
        "    kmeans = KMeans(n_clusters=k)\n",
        "    kmeans.fit(img)\n",
        "    y_kmeans = kmeans.predict(img)\n",
        "    centers = kmeans.cluster_centers_\n",
        "  return centers\n",
        "\n",
        "def blob_localize(mask):\n",
        "  params = cv2.SimpleBlobDetector_Params()\n",
        "  params.minArea = 1;\n",
        "  params.maxArea = IMAGE_HEIGHT * IMAGE_WIDTH;\n",
        "  params.minDistBetweenBlobs = 0\n",
        "  params.filterByColor = True\n",
        "  params.filterByArea = True\n",
        "  params.filterByCircularity = False\n",
        "  params.filterByInertia = False\n",
        "  params.filterByConvexity = False\n",
        "  params.minThreshold = 0\n",
        "  params.maxThreshold = 255\n",
        "  params.blobColor = 255\n",
        "  detector = cv2.SimpleBlobDetector_create(params)\n",
        "  detector.empty()\n",
        "  return detector.detect(mask)\n",
        "\n",
        "def load_csv(id):\n",
        "  with open(TENT_CSV) as csvfile:\n",
        "    for row in list(csv.reader(csvfile)):\n",
        "      if row[0].split(\".\", 1)[0] == id:\n",
        "        return int(row[1])\n",
        "  return None\n",
        "\n",
        "# def visualize_old(image_data):\n",
        "#   print(\"Displaying results. Please wait, this may take some time...\")\n",
        "#   for resolution in image_data: # This will only run once, since we only use one resolution\n",
        "#     cols = 3\n",
        "#     rows = len(image_data[resolution])\n",
        "\n",
        "#     fig = plt.figure(figsize=(resolution[0]//DISP_SCALE, resolution[1]//DISP_SCALE)) # The figure needs to be big enough to accomodate all the images; maybe we need to get resolution elsewhere?\n",
        "#     grid = ImageGrid(fig, 111, nrows_ncols=(rows, cols), axes_pad=0.5)\n",
        "\n",
        "#     g_index = -1\n",
        "\n",
        "#     for record in image_data[resolution]:\n",
        "#       id = record[\"id\"]\n",
        "#       img = record[\"img\"]\n",
        "#       lbl = cv2.threshold(cv2.convertScaleAbs(cv2.cvtColor(record[\"lbl\"], cv2.COLOR_BGR2GRAY)), 1, 255, cv2.THRESH_BINARY)[1]\n",
        "#       prd = cv2.normalize(cv2.convertScaleAbs(cv2.cvtColor(record[\"prd\"], cv2.COLOR_BGR2GRAY)), np.zeros(img.shape, dtype=np.float32), 0, 255, cv2.NORM_MINMAX)\n",
        "      \n",
        "#       if BLOB_LOCALIZATION:\n",
        "#         lbl_keypoints = blob_localize(lbl)\n",
        "#         prd_keypoints = blob_localize(prd)\n",
        "\n",
        "#         lbl_points = []\n",
        "#         prd_points = []\n",
        "\n",
        "#         for point in lbl_keypoints:\n",
        "#           lbl_points.append(np.array([point.pt[0], point.pt[1]]))\n",
        "\n",
        "#         for point in prd_keypoints:\n",
        "#           prd_points.append(np.array([point.pt[0], point.pt[1]]))\n",
        "\n",
        "#         lbl_points = np.array(lbl_points)\n",
        "#         prd_points = np.array(prd_points)\n",
        "\n",
        "#       if LOAD_FROM_CSV:\n",
        "#         lbl_count = load_csv(id) # TODO: need to get picture ID somehow\n",
        "#         if lbl_count is not None and KMEANS_LOCALIZATION:\n",
        "#           lbl_kmeans = kmeans_localize(lbl, lbl_count)\n",
        "\n",
        "#       overlay = np.zeros(img.shape)\n",
        "#       gt_overlay = np.zeros(img.shape)\n",
        "\n",
        "#       if BLOB_LOCALIZATION:\n",
        "#         mask_with_keypoints = np.zeros(img.shape)\n",
        "#         mask_with_keypoints = cv2.drawKeypoints(prd, prd_keypoints, mask_with_keypoints, (255,0,0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "#       else:\n",
        "#         mask_with_keypoints = prd\n",
        "        \n",
        "#       # Adapted from: https://stackoverflow.com/questions/46103731/is-there-a-simple-method-to-highlight-the-mask\n",
        "#       contours, hierarchy = cv2.findContours(prd, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "#       gt_contours, gt_hierarchy = cv2.findContours(lbl, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "      \n",
        "#       cv2.drawContours(overlay, contours, -1, (1, 0, 0), thickness=cv2.FILLED)\n",
        "#       cv2.drawContours(gt_overlay, gt_contours, -1, (1, 0, 1), thickness=cv2.FILLED)\n",
        "\n",
        "#       bg = (cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if GRAYSCALE else img)\n",
        "#       if GRAYSCALE:\n",
        "#         bg = cv2.cvtColor(bg, cv2.COLOR_GRAY2BGR)\n",
        "      \n",
        "#       overlay = overlay.astype(np.float32)\n",
        "#       gt_overlay = gt_overlay.astype(np.float32)\n",
        "#       overlay = cv2.bitwise_xor(bg, overlay)\n",
        "#       gt_overlay = cv2.bitwise_xor(bg, gt_overlay)\n",
        "#       overlay = cv2.normalize(overlay, np.zeros(img.shape, dtype=np.uint8), 0, 1, cv2.NORM_MINMAX)\n",
        "#       gt_overlay = cv2.normalize(gt_overlay, np.zeros(img.shape, dtype=np.uint8), 0, 1, cv2.NORM_MINMAX)\n",
        "\n",
        "#       # Saving Data\n",
        "#       # Image.fromarray(prd).save(f\"{OUTPUT_MASK_DIR}{id}.{IMG_FORMAT}\")\n",
        "#       # Image.fromarray(overlay).save(f\"{OUTPUT_DATA_DIR}{id}.{IMG_FORMAT}\")\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "#       # Will clean this up later; it can probably be simplified a lot\n",
        "#       if BLOB_LOCALIZATION:\n",
        "#         gt_count = f\"(Detected: {len(lbl_points)}, Actual: {lbl_count})\"\n",
        "#         pt_count = f\"(Detected: {len(prd_points)})\"\n",
        "#       else:\n",
        "#         gt_count = f\"(Tents: {lbl_count})\"\n",
        "#         pt_count = \"\"\n",
        "#       g_index += 1\n",
        "#       grid[g_index].set_title(f\"{id}: Ground Truth {gt_count}\")\n",
        "#       grid[g_index].imshow(gt_overlay, vmin=0, vmax=1)\n",
        "#       if BLOB_LOCALIZATION and lbl_points.size:\n",
        "#         gt = grid[g_index].scatter(lbl_points[:,0], lbl_points[:,1], s=50, marker='o', c=\"orange\", alpha=1, label=\"Tent (Blob Localization)\")\n",
        "#       green_patch = mpatches.Patch(color=\"lightgreen\", label=\"Ground Truth Mask\")\n",
        "#       if LOAD_FROM_CSV:\n",
        "#         gmeans = mpatches.Patch(color=\"red\", label=\"Tent (KMeans Localization)\")\n",
        "#         if KMEANS_LOCALIZATION and lbl_kmeans is not None:\n",
        "#           gmeans = grid[g_index].scatter(lbl_kmeans[:,0], lbl_kmeans[:,1], s=50, marker='x', c=\"red\", alpha=1, label=\"Tent (KMeans Localization)\")\n",
        "#         if BLOB_LOCALIZATION:\n",
        "#           grid[g_index].legend(handles=[gt, green_patch, gmeans], loc=\"upper left\")\n",
        "#         else:\n",
        "#           grid[g_index].legend(handles=[green_patch, gmeans], loc=\"upper left\")\n",
        "#       else:\n",
        "#         grid[g_index].legend(handles=[green_patch], loc=\"upper left\")\n",
        "\n",
        "#       g_index += 1\n",
        "#       grid[g_index].set_title(f\"{id}: Prediction {pt_count}\")\n",
        "#       grid[g_index].imshow(overlay, vmin=0, vmax=1)\n",
        "#       teal_patch = mpatches.Patch(color=\"cyan\", label=\"Prediction Mask\")\n",
        "#       if BLOB_LOCALIZATION and prd_points.size:\n",
        "#         pt = grid[g_index].scatter(prd_points[:,0], prd_points[:,1], s=50, marker='x', c=\"red\", alpha=1, label=\"Tent (Blob Localization)\")\n",
        "#         grid[g_index].legend(handles=[pt, teal_patch], loc=\"upper left\")\n",
        "#       else:\n",
        "#         grid[g_index].legend(handles=[teal_patch], loc=\"upper left\")\n",
        "\n",
        "#       accuracy = record[\"accuracy\"]\n",
        "#       g_index += 1\n",
        "#       grid[g_index].set_title(f\"{id}: Predicted Mask (Accuracy: {accuracy:.2f})\")\n",
        "#       grid[g_index].imshow(mask_with_keypoints, vmin=0, vmax=1)\n",
        "#     plt.show()\n",
        "\n",
        "def save_checkpoint(state, filename=CHECKPOINT):\n",
        "  print(\"=> Saving checkpoint\\n\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "  print(\"=> Loading checkpoint\\n\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(train_dir, train_maskdir, val_dir, val_maskdir, batch_size, train_transform, val_transform, num_workers=4, pin_memory=True):\n",
        "  train_ds = TentDataset(image_dir=train_dir, mask_dir=train_maskdir, transform=train_transform)\n",
        "  train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n",
        "  val_ds = TentDataset(image_dir=val_dir, mask_dir=val_maskdir, transform=val_transform)\n",
        "  val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
        "  return train_loader, val_loader\n",
        "\n",
        "# def check_accuracy(loader, model, device=DEVICE):\n",
        "#   model.eval()\n",
        "\n",
        "#   # image_data = {}\n",
        "#   # image_data[(IMAGE_WIDTH, IMAGE_HEIGHT)] = []\n",
        "\n",
        "#   accuracy = {}\n",
        "\n",
        "#   with torch.no_grad():\n",
        "#     for x, y, id in loader:\n",
        "#       x = x.to(device)\n",
        "#       y = y.to(device).unsqueeze(1) # label doesn't have a channel cuz its' greyscale so we have to do this so accuracy works properly\n",
        "#       preds = torch.sigmoid(model(x))\n",
        "#       preds = (preds > 0.5).float()\n",
        "\n",
        "#       jaccard_index = (EPSILON+(preds * y).sum()) / (EPSILON+(preds + y).sum())\n",
        "\n",
        "#       # img = x.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
        "#       # lbl = np.stack((y.squeeze().permute(0, 1).detach().cpu().numpy(), )*3, axis=-1)\n",
        "#       # prd = np.stack((preds.squeeze().permute(0, 1).detach().cpu().numpy(), )*3, axis=-1)\n",
        "\n",
        "#       j_index = jaccard_index.detach().cpu().numpy()\n",
        "#       # image_data[(img.shape[0], img.shape[1])].append({\"img\":img, \"lbl\":lbl, \"prd\":prd, \"accuracy\":j_index, \"id\":id[0]})\n",
        "\n",
        "#       accuracy[id] = j_index\n",
        "\n",
        "#   # Display\n",
        "#   # visualize(image_data)\n",
        "\n",
        "#   model.train()\n",
        "\n",
        "#   return accuracy\n",
        "\n",
        "# def save_predictions_as_imgs(loader, model, folder=PREDICT_DIR, device=DEVICE):\n",
        "#   model.eval()\n",
        "#   for x, y, id in loader:\n",
        "#     x = x.to(device=device)\n",
        "#     with torch.no_grad():\n",
        "#       preds = torch.sigmoid(model(x))\n",
        "#       preds = (preds > 0.5).float()\n",
        "#     torchvision.utils.save_image(preds, f\"{folder}/pred_{id}.{IMG_FORMAT}\")\n",
        "#     torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{id}.{IMG_FORMAT}\")\n",
        "  \n",
        "#   model.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def display_visualizations():\n",
        "  print(f\"\\n\\n{DIRECTORIES}\")\n",
        "  for dataset in DIRECTORIES:\n",
        "    vis_dir = f\"{DIRECTORIES[dataset]['visualizations']}\"\n",
        "    print(vis_dir)\n",
        "\n",
        "    targets = load(f\"{vis_dir}targets/*\")\n",
        "    estimates = load(f\"{vis_dir}estimates/*\")\n",
        "\n",
        "    if targets.shape[0] == estimates.shape[0]:\n",
        "      for index in range(targets.shape[0]):\n",
        "        t_id = targets[index][1]\n",
        "        e_id = estimates[index][1]\n",
        "        if t_id == e_id:\n",
        "          fig = plt.figure(figsize=(15, 15))\n",
        "          grid = ImageGrid(fig, 111, nrows_ncols=(1, 2), axes_pad=0.5)\n",
        "\n",
        "          print(f\"\\nLoading: {t_id}\")\n",
        "          grid[0].imshow(targets[index][0])\n",
        "          grid[1].imshow(estimates[index][0])\n",
        "          plt.show()\n",
        "        else:\n",
        "          print(f\"\\nID Mismatch!: {t_id} =/= {e_id}\\n\")\n",
        "          \n",
        "\n",
        "# TODO: similarity doesn't seem to work here when saving the visualization\n",
        "def similarity(estimate, target): # Returns the Jaccard Index of estiamte and target (how similar they are)\n",
        "  return (EPSILON+(estimate * target).sum()) / (EPSILON+(estimate + target).sum())\n",
        "\n",
        "def load_image(id, directory=OUTPUT_DIR):\n",
        "  image = {\"record\":None, \"tile_coord\":None, \"feature\":None, \"target\":None, \"estimate\":None, \"visualization\":None}\n",
        "  for dirs in DIRECTORIES.values():\n",
        "    target_path = f\"{dirs['targets']}{id}.{IMG_FORMAT}\"\n",
        "    estimate_path = f\"{dirs['estimates']}{id}.{IMG_FORMAT}\"\n",
        "    feature_path = f\"{dirs['features']}{id}.{IMG_FORMAT}\"\n",
        "    visualization_path = f\"{dirs['visualizations']}{id}.{IMG_FORMAT}\"\n",
        "\n",
        "    image[\"record\"] = int(id.split('_')[1])-1\n",
        "    image[\"tile_coord\"] = find_square_coordinate(image[\"record\"])\n",
        "\n",
        "    if os.path.exists(target_path):\n",
        "      image[\"target\"] = np.asarray(Image.open(target_path).convert(\"L\")) # use 1 instead of L for binary\n",
        "    \n",
        "    if os.path.exists(estimate_path):\n",
        "      image[\"estimate\"] = np.asarray(Image.open(estimate_path).convert(\"L\")) # use 1 instead of L for binary\n",
        "    \n",
        "    if os.path.exists(feature_path):\n",
        "      image[\"feature\"] = np.asarray(Image.open(feature_path).convert(\"RGB\"))\n",
        "    \n",
        "    if os.path.exists(visualization_path):\n",
        "      image[\"visualization\"] = np.asarray(Image.open(visualization_path).convert(\"RGB\"))\n",
        "  return image\n",
        "\n",
        "def load_tile_names():\n",
        "  tile_paths = []\n",
        "  for dirs in DIRECTORIES.values():\n",
        "    for path, _, tile_names in os.walk(dirs['ve_tiles']):\n",
        "      for tile_name in tile_names:\n",
        "        tile_paths.append(path+tile_name)\n",
        "  return tile_paths\n",
        "\n",
        "def make_visualization(id, directory, accuracy):\n",
        "  image = load_image(id)\n",
        "  img = image[\"feature\"]\n",
        "  lbl = cv2.threshold(cv2.convertScaleAbs(image[\"target\"]), 1, 255, cv2.THRESH_BINARY)[1]#cv2.threshold(cv2.convertScaleAbs(cv2.cvtColor(image[\"target\"], cv2.COLOR_BGR2GRAY)), 1, 255, cv2.THRESH_BINARY)[1]\n",
        "  prd = cv2.normalize(cv2.convertScaleAbs(image[\"estimate\"]), np.zeros(img.shape, dtype=np.float32), 0, 255, cv2.NORM_MINMAX)#cv2.normalize(cv2.convertScaleAbs(cv2.cvtColor(image[\"estimate\"], cv2.COLOR_BGR2GRAY)), np.zeros(img.shape, dtype=np.float32), 0, 255, cv2.NORM_MINMAX)\n",
        "  \n",
        "  lbl_count = None\n",
        "  if LOAD_FROM_CSV:\n",
        "    lbl_count = load_csv(id)\n",
        "    if lbl_count is not None and KMEANS_LOCALIZATION:\n",
        "      lbl_kmeans = kmeans_localize(lbl, lbl_count)\n",
        "\n",
        "  overlay = np.zeros(img.shape)\n",
        "  gt_overlay = np.zeros(img.shape)\n",
        "  mask_with_keypoints = prd\n",
        "  \n",
        "  # Adapted from: https://stackoverflow.com/questions/46103731/is-there-a-simple-method-to-highlight-the-mask\n",
        "  contours, hierarchy = cv2.findContours(prd, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  gt_contours, gt_hierarchy = cv2.findContours(lbl, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  \n",
        "  cv2.drawContours(overlay, contours, -1, (1, 0, 0), thickness=cv2.FILLED)\n",
        "  cv2.drawContours(gt_overlay, gt_contours, -1, (1, 0, 1), thickness=cv2.FILLED)\n",
        "\n",
        "  bg = (cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if GRAYSCALE else img)\n",
        "  if GRAYSCALE:\n",
        "    bg = cv2.cvtColor(bg, cv2.COLOR_GRAY2BGR)\n",
        "  \n",
        "  overlay = cv2.normalize(cv2.convertScaleAbs(overlay), np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX)\n",
        "  gt_overlay = cv2.normalize(cv2.convertScaleAbs(gt_overlay), np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX)\n",
        "  overlay = cv2.bitwise_xor(bg, overlay)\n",
        "  gt_overlay = cv2.bitwise_xor(bg, gt_overlay)\n",
        "  overlay = cv2.normalize(overlay, np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX) # set 1 to 255 if this doesn't work?\n",
        "  gt_overlay = cv2.normalize(gt_overlay, np.zeros(img.shape, dtype=np.uint8), 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "  # TODO: Sometimes visualizations lack the background image; maybe it wasn't saved in time or some other issue?\n",
        "  # Might only be a problem on CPU, not sure about GPU yet.\n",
        "\n",
        "  # target_vis_dir = f\"{directory}targets/\"\n",
        "  # if not os.path.exists(target_vis_dir):\n",
        "  #   os.makedirs(target_vis_dir)\n",
        "\n",
        "  # # Create Tile ######\n",
        "  # # TODO: we need to change the colour of the prediction, so that it stands out and highlights\n",
        "  # tile_img = Image.fromarray(img)\n",
        "  # tile_lbl = Image.fromarray(lbl)\n",
        "  # tile_prd = Image.fromarray(prd)\n",
        "  # tile_img.paste(tile_prd, (0,0), mask=tile_prd)\n",
        "  # tile_img.save(f\"{target_vis_dir}{id}-tile.{IMG_FORMAT}\", format='png')\n",
        "\n",
        "  # # TODO: we probbly need to package tiles here to make loading easy\n",
        "  # print(f\"Visualization Tile: {image['tile_coord']}\")\n",
        "  # ####################\n",
        "\n",
        "\n",
        "\n",
        "  # Target Visualization\n",
        "  target_count = f\"(Tents: {lbl_count})\"\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(f\"{id}: Ground Truth {target_count}\")\n",
        "  plt.imshow(gt_overlay, vmin=0, vmax=1)\n",
        "  green_patch = mpatches.Patch(color=\"lightgreen\", label=\"Target Mask\")\n",
        "  if LOAD_FROM_CSV and KMEANS_LOCALIZATION:\n",
        "    kmeans_patch = mpatches.Patch(color=\"red\", label=\"Tent\")\n",
        "    if KMEANS_LOCALIZATION and lbl_kmeans is not None:\n",
        "      kmeans_patch = plt.scatter(lbl_kmeans[:,0], lbl_kmeans[:,1], s=50, marker='x', c=\"red\", alpha=1, label=\"Tent\")\n",
        "    plt.legend(handles=[green_patch, kmeans_patch], loc=\"upper left\")\n",
        "  else:\n",
        "    plt.legend(handles=[green_patch], loc=\"upper left\")\n",
        "  target_vis_dir = f\"{directory}targets/\"\n",
        "  if not os.path.exists(target_vis_dir):\n",
        "    os.makedirs(target_vis_dir)\n",
        "  plt.savefig(f\"{target_vis_dir}{id}.{IMG_FORMAT}\")\n",
        "  plt.close()\n",
        "\n",
        "  # Create Tile ######\n",
        "  target_tile_dir = f\"{target_vis_dir}tiles/\"\n",
        "  if not os.path.exists(target_tile_dir):\n",
        "    os.makedirs(target_tile_dir)\n",
        "  tile_img = Image.fromarray(gt_overlay)\n",
        "  tile_img.save(f\"{target_tile_dir}{id}.{IMG_FORMAT}\", format='png')\n",
        "  ####################\n",
        "\n",
        "\n",
        "  # Estimate Visualization\n",
        "  estimate_count = \"\"\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(f\"{id}: Prediction (Accuracy: {accuracy:.2f} {estimate_count})\")\n",
        "  plt.imshow(overlay, vmin=0, vmax=1)\n",
        "  teal_patch = mpatches.Patch(color=\"cyan\", label=\"Prediction Mask\")\n",
        "  plt.legend(handles=[teal_patch], loc=\"upper left\")\n",
        "  estimate_vis_dir = f\"{directory}estimates/\"\n",
        "  if not os.path.exists(estimate_vis_dir):\n",
        "      os.makedirs(estimate_vis_dir)\n",
        "  plt.savefig(f\"{estimate_vis_dir}{id}.{IMG_FORMAT}\")\n",
        "  plt.close()\n",
        "\n",
        "  # Create Tile ######\n",
        "  # TODO: we need to change the colour of the prediction, so that it stands out and highlights\n",
        "  estimate_tile_dir = f\"{estimate_vis_dir}tiles/\"\n",
        "  if not os.path.exists(estimate_tile_dir):\n",
        "    os.makedirs(estimate_tile_dir)\n",
        "  tile_img = Image.fromarray(overlay)\n",
        "  tile_img.save(f\"{estimate_tile_dir}{id}.{IMG_FORMAT}\", format='png')\n",
        "  ####################\n",
        "\n",
        "\n",
        "  # Keypoint Visualization\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(f\"{id}: Predicted Mask (Accuracy: {accuracy:.2f})\")\n",
        "  plt.imshow(mask_with_keypoints, vmin=0, vmax=1)\n",
        "  keypoint_dir = f\"{directory}keypoints/\"\n",
        "  if not os.path.exists(keypoint_dir):\n",
        "      os.makedirs(keypoint_dir)\n",
        "  plt.savefig(f\"{keypoint_dir}{id}.{IMG_FORMAT}\")\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Maybe combine both tent counts and return a dictionary with target and estimate values\n",
        "def gt_tent_count(id):\n",
        "  with open(TENT_CSV) as csvfile:\n",
        "    for row in list(csv.reader(csvfile)):\n",
        "      if row[0].split(\".\", 1)[0] == id:\n",
        "        return int(row[1])\n",
        "  return None\n",
        "\n",
        "def tent_count(id):\n",
        "  # TODO: implement this (convolutional neural network here?)\n",
        "  return None\n",
        "\n",
        "\n",
        "def get_record(id):\n",
        "  return int(id.split('_')[1])-1\n",
        "\n",
        "def save_all(loaders, model, directory=OUTPUT_DIR, device=DEVICE):\n",
        "  model.eval()\n",
        "  # TODO: do visualizations here too, then save them to a file; we can then just load from file to display them\n",
        "  # also make a Python pickle which contains accuracies and other such information, along with the path to\n",
        "  # the appropriate file, and if it's validation or training data\n",
        "\n",
        "  # TODO: make sure we're loading mask data in binary format (I don't think we are currently)\n",
        "\n",
        "  results = {}\n",
        "  for dataset, loader in loaders.items():\n",
        "    dirs = DIRECTORIES[dataset]\n",
        "\n",
        "    loader_progress = tqdm(loader)\n",
        "    loader_progress.set_description(f\"Saving {dataset} dataset\")\n",
        "    for feature, target, id in loader_progress:\n",
        "      id = id[0]\n",
        "      feature = feature.to(device)\n",
        "      with torch.no_grad():\n",
        "        estimate = torch.sigmoid(model(feature))\n",
        "        estimate = (estimate > 0.5).float()\n",
        "\n",
        "      torchvision.utils.save_image(estimate, f\"{dirs['estimates']}{id}.{IMG_FORMAT}\")\n",
        "      torchvision.utils.save_image(feature, f\"{dirs['features']}{id}.{IMG_FORMAT}\")\n",
        "      torchvision.utils.save_image(target.unsqueeze(1), f\"{dirs['targets']}{id}.{IMG_FORMAT}\") #might not need unsqueeze? or maybe we should use it on estimate too?\n",
        "\n",
        "      accuracy = similarity(estimate.detach().cpu(), target).item()\n",
        "      make_visualization(id, dirs[\"visualizations\"], accuracy)\n",
        "\n",
        "      results[id] = {\n",
        "        \"record\":          get_record(id),\n",
        "        \"tile-coord\":      find_square_coordinate(get_record(id)),\n",
        "        \"dataset\":         dataset,\n",
        "        \"target_count\":    gt_tent_count(id),\n",
        "        \"estimate_count\":  tent_count(id),\n",
        "        \"mask_similarity\": accuracy\n",
        "      }\n",
        "\n",
        "      loader_progress.set_description(f\"Saving [{dataset}]: {id}, {results[id]}\")\n",
        "    \n",
        "  model.train()\n",
        "  with open(f\"{directory}results.pkl\", \"wb\") as output:\n",
        "    pickle.dump(results, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  shutil.make_archive(\"tent_counts\", \"tar\", directory)\n",
        "  print(\"Saved.\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: perhaps we just add to the tile? as in, we put one image in at a time from\n",
        "# other functions, and then the tile just gets bigger and bigger?\n",
        "# def tile_images(tile, tiles):\n",
        "#   #https://note.nkmk.me/en/python-pillow-concat-images/\n",
        "\n",
        "#   img = Image.new('RGB', (img_width + tile_width, img_height))\n",
        "  \n",
        "# def load_tile_names():\n",
        "#   tile_paths = []\n",
        "#   for dirs in DIRECTORIES.values():\n",
        "#     for path, _, tile_names in os.walk(dirs['ve_tiles']):\n",
        "#       for tile_name in tile_names:\n",
        "#         tile_paths.append(path+tile_name)\n",
        "#   return tile_paths\n",
        "\n",
        "# TODO: perhaps we just add to the tile? as in, we put one image in at a time from\n",
        "# other functions, and then the tile just gets bigger and bigger?\n",
        "\n",
        "def tile_images(max_records=256):\n",
        "  output_name = f\"map.{IMG_FORMAT}\"\n",
        "  square_length = int(math.sqrt(max_records))\n",
        "  tile_paths = [[None for x in range(square_length)] for y in range(square_length)]\n",
        "  for dirs in DIRECTORIES.values():\n",
        "    for path, _, tile_names in os.walk(dirs['ve_tiles']):\n",
        "      for tile_name in tile_names:\n",
        "        tile_index = find_square_coordinate(int(tile_name.split('_')[1].split('.')[0])-1)\n",
        "        tile_paths[tile_index[0]][tile_index[1]] = (path+tile_name)\n",
        "  \n",
        "  # img = Image.fromarray(np.empty((0, 0), dtype=np.uint8))\n",
        "  # img.save(output_name)\n",
        "\n",
        "\n",
        "  #   loop = tqdm(loader) # tqdm gives us a progress bar\n",
        "  # loop.set_description(\"Training\")\n",
        "\n",
        "  # for batch_idx, (data, targets, id) in enumerate(loop):\n",
        "\n",
        "  column_loop = tqdm(range(len(tile_paths[0])))\n",
        "\n",
        "  img = None\n",
        "  w = h = 0\n",
        "  for r in range(len(tile_paths)):       # Rows\n",
        "    for c in enumerate(column_loop):  # Columns\n",
        "      column_loop.set_description(f\"Stitching on Index: ({r}, {c})\")\n",
        "\n",
        "      tile = Image.open(tile_paths[r][c]).convert(\"RGB\")\n",
        "\n",
        "      if c == 0:\n",
        "        w = tile.width if img == None else img.width\n",
        "        h += tile.height\n",
        "      else:\n",
        "        w += tile.width\n",
        "        h = img.height\n",
        "\n",
        "      dst = Image.new('RGB', (w, h))\n",
        "      if img == None:\n",
        "        dst.paste(tile, (0, 0))\n",
        "      else:\n",
        "        dst.paste(img, (0, 0))\n",
        "        dst.paste(tile, (0, tile.height) if c == 0 else (tile.width, 0))\n",
        "      img = dst\n",
        "      img.save(output_name)\n",
        "\n",
        "\n",
        "    # tile = Image.open(tile_paths[r+1][0]).convert(\"RGB\")\n",
        "    # dst = Image.new('RGB', (img.width, img.height + tile.height))\n",
        "    # dst.paste(img, (0, 0))\n",
        "    # dst.paste(tile, (0, tile.height))\n",
        "    # img = dst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://github.com/Thundertung/Book-Price-regression-CNNs/blob/main/Judging%20a%20book%20by%20its%20cover.ipynb\n",
        "# and from https://www.youtube.com/watch?v=nU_T2PPigUQ&t=531s\n",
        "# TODO: adapt for use here\n",
        "\n",
        "if False: # TODO: set to True if you want to run this\n",
        "  from sklearn.preprocessing import LabelEncoder    #For encoding categorical variables\n",
        "  from sklearn.model_selection import train_test_split #For splitting of data\n",
        "  #All tensorflow utilities for creating, training and working with a CNN\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
        "  from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "  from tensorflow.keras.losses import categorical_crossentropy\n",
        "  from tensorflow.keras.optimizers import Adam\n",
        "  from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "  from tensorflow.keras.models import load_model\n",
        "\n",
        "  new_train_set = []\n",
        "  new_train_count = np.empty(len(training_set))\n",
        "  for i in range(len(training_set)):\n",
        "    new_train_set.append(np.asarray(training_set[i]['img']))\n",
        "    new_train_count[i] = training_set[i]['num']\n",
        "\n",
        "  new_train_set = np.array(new_train_set)\n",
        "\n",
        "  print(f\"Training Set: {np.shape(new_train_set)}\")\n",
        "  print(f\"Training Count: {np.shape(new_train_count)}\")\n",
        "\n",
        "  new_val_set = []\n",
        "  new_val_count = np.empty(len(validation_set))\n",
        "  for i in range(len(validation_set)):\n",
        "    new_val_set.append(np.asarray(validation_set[i]['img']))\n",
        "    new_val_count[i] = validation_set[i]['num']\n",
        "\n",
        "  new_val_set = np.array(new_val_set)\n",
        "\n",
        "  print(f\"Val Set: {np.shape(new_val_set)}\")\n",
        "  print(f\"Val Count: {np.shape(new_val_count)}\")\n",
        "\n",
        "  # new_train_set.reshape(new_train_set.shape[0],new_train_set.shape[1],new_train_set.shape[2],new_train_set.shape[3])\n",
        "  # new_train_set.reshape(new_train_set.shape[0],new_train_set.shape[1],new_train_set.shape[2],new_train_set.shape[3])\n",
        "\n",
        "\n",
        "  print(f\"Training Set: {np.shape(new_train_set)}\")\n",
        "\n",
        "  # input_shape = np.shape(new_train_set.reshape(np.shape(new_train_set)[1], np.shape(new_train_set)[2]))\n",
        "  input_shape = np.shape(new_train_set[0])\n",
        "  print(f\"Input Shape: {input_shape}\")\n",
        "\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # First conv\n",
        "  model.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "\n",
        "  # Second conv\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Hidden layer\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu', input_shape = input_shape))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # # model.add(MaxPool2D(strides=(2,2)))\n",
        "  # model.add(Dropout(0.25))\n",
        "  # model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  # # model.add(MaxPool2D(strides=(2,2)))\n",
        "  # model.add(Dropout(0.25))\n",
        "\n",
        "  # model.add(Flatten())\n",
        "  # # model.add(Dense(100, activation='relu'))\n",
        "  # model.add(Dropout(0.25))\n",
        "\n",
        "  # # model.add(Dense(1024, activation='relu'))\n",
        "  # model.add(Dropout(0.4))\n",
        "  # model.add(Dense(1, activation='linear'))\n",
        "\n",
        "  learning_rate = 0.001\n",
        "\n",
        "  model.compile(loss = categorical_crossentropy,\n",
        "                optimizer = Adam(learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # TODO: the sets are an array of dictionaries where each dictionary contains the values we want\n",
        "\n",
        "  # new_train_set = []\n",
        "  # new_train_count = np.empty(len(training_set))\n",
        "  # for i in range(len(training_set)):\n",
        "  #   new_train_set.append(np.asarray(training_set[i]['img']))\n",
        "  #   new_train_count[i] = training_set[i]['num']\n",
        "\n",
        "  # print(np.shape(new_train_set))\n",
        "\n",
        "  # new_val_set = []\n",
        "  # new_val_count = np.empty(len(validation_set))\n",
        "  # for i in range(len(validation_set)):\n",
        "  #   new_val_set.append(np.asarray(validation_set[i]['img']))\n",
        "  #   new_val_count[i] = validation_set[i]['num']\n",
        "\n",
        "  #training_set, validation_set, csv_training_set, csv_validation_set = train_test_split(dataset, csv_dataset, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "  model.fit(new_train_set, new_train_count, epochs=15, validation_data=(new_val_set, new_val_count))\n",
        "\n",
        "\n",
        "  # history = model.fit( X_train, Y_train, \n",
        "  #                     epochs = 15, batch_size = 100, \n",
        "  #                     callbacks=[save_best2], verbose=1, \n",
        "  #                    validation_data = (X_val, Y_price_val))\n",
        "else:\n",
        "  print(\"Skipping for now; change if to True if you want to run this block\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC260aihvCus",
        "outputId": "5c29f5a1-7086-47df-a67c-8aa7e779c92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping for now; change if to True if you want to run this block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8NEUSCTRAxr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD1wOKe_Z_eJ"
      },
      "source": [
        "# Model Training\n",
        "# Adapted From https://www.youtube.com/watch?v=IHq1t7NxS8k\n",
        "\n",
        "\n",
        "\n",
        "# This whole function trains one epoch\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "  loop = tqdm(loader) # tqdm gives us a progress bar\n",
        "  loop.set_description(\"Training\")\n",
        "\n",
        "  for batch_idx, (data, targets, id) in enumerate(loop):\n",
        "    loop.set_description(f\"Training on {id[0]}\")\n",
        "    data = data.to(device=DEVICE)\n",
        "    targets = targets.float().unsqueeze(1).to(device=DEVICE) #might not need to make it float since it might already be float? Also, unsqueeze is used cuz we're adding a channel\n",
        "\n",
        "    # Forward\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(data)\n",
        "      loss = loss_fn(predictions, targets)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # Update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "def main():\n",
        "  train_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Rotate(limit=35, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1),\n",
        "    A.Normalize(\n",
        "      mean=[0.0, 0.0, 0.0],\n",
        "      std=[1.0, 1.0, 1.0],\n",
        "      max_pixel_value=255.0 # This basically just divides by 255, so we get a value between 0 and 1\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "  ])\n",
        "\n",
        "  val_transforms = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Normalize(\n",
        "      mean=[0.0, 0.0, 0.0],\n",
        "      std=[1.0, 1.0, 1.0],\n",
        "      max_pixel_value=255.0 # This basically just divides by 255, so we get a value between 0 and 1\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "  ])\n",
        "\n",
        "  model = UNet(in_channels=3, out_channels=1).to(DEVICE) # if we wanted multiclass segmentation we'd change our channels and change our loss function to cross entropy loss\n",
        "  loss_fn = nn.BCEWithLogitsLoss() # We're not doing sigmoid on the output of model which is why we're using this here\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  train_loader, val_loader = get_loaders(\n",
        "      TRAIN_IMG_DIR,\n",
        "      TRAIN_LBL_DIR,\n",
        "      VAL_IMG_DIR,\n",
        "      VAL_LBL_DIR,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      val_transforms,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY\n",
        "  )\n",
        "\n",
        "  if LOAD_MODEL:\n",
        "    if DEVICE == \"cuda\":\n",
        "      load_checkpoint(torch.load(CHECKPOINT), model)\n",
        "    else:\n",
        "      load_checkpoint(torch.load(CHECKPOINT, map_location=lambda storage, loc: storage), model) # From https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/4\n",
        "  # check_accuracy(val_loader, model, device=DEVICE) # Because the val_loader is being passed, we know that we're only visualizing validation images and not the images the model was trained on\n",
        "  scaler = torch.cuda.amp.GradScaler() # I think this is where we get the warning about running on CPU; should try to address this warning\n",
        "  \n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "    # Save Model\n",
        "    checkpoint = {\"state_dict\":model.state_dict(), \"optimizer\":optimizer.state_dict()}\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    # Check Accuracy\n",
        "    # check_accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "    # Save Predictions\n",
        "    # save_predictions_as_imgs(val_loader, model, folder=PREDICT_DIR, device=DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: save all predictions (validation and ground truth in separate folders)\n",
        "    # Also save pictures with overlay, and pictures with overlay plus plot?\n",
        "    # also save tent count (heatmap if possible too)\n",
        "\n",
        "\n",
        "    # TODO: use the commented version later; seems to be a problem with saving the training data though?\n",
        "    save_all({\"validation\":val_loader, \"training\":train_loader}, model)\n",
        "    # save_all({\"validation\":val_loader}, model)\n",
        "    \n",
        "    if LIVE_VISUALIZE:\n",
        "      display_visualizations()\n",
        "\n",
        "\n",
        "\n",
        "  # TODO: after an epoch has run, we need to tile the images and generate the map\n",
        "  # tiles = # TODO: define tiles (we can use our feature sets to get the correct 2D coordinates for each image)\n",
        "  # tile_images(tiles)\n",
        "  # print(\"Loading tile names...\")\n",
        "  # print(load_tile_names())\n",
        "  # print(\"Done.\")\n",
        "  tile_images() # TODO: debug this\n",
        "    # Generate Map View\n",
        "    # Image.fromarray(stitch(f\"{OUTPUT_MASK_DIR}*\")).save(\"map_predicted_mask.png\") # TODO: generate map\n",
        "    # Image.fromarray(stitch(f\"{VAL_IMG_DIR}*\")).save(\"map_image.png\")\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "# This solves issues when running on Windows (issues relating to NUM_WORKERS); probably not so relevant to Colab.\n",
        "# if __name__ == \"__main__\":\n",
        "#   main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj7ekOSNAKHC"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qJ6mESs_4Yp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "ecc11677-ecad-48d2-9b0a-6860ff910770"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training on sarpol_167:   0%|          | 0/179 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "Training on sarpol_200: 100%|██████████| 179/179 [50:20<00:00, 16.87s/it, loss=-31.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving [validation]: sarpol_091, {'record': 90, 'tile-coord': (5, 10), 'dataset': 'validation', 'target_count': 0, 'estimate_count': None, 'mask_similarity': 1.0395346779015824e-19}: 100%|██████████| 77/77 [08:00<00:00,  6.24s/it]\n",
            "Saving [training]: sarpol_230, {'record': 229, 'tile-coord': (14, 5), 'dataset': 'training', 'target_count': 0, 'estimate_count': None, 'mask_similarity': 1.1445597840246972e-18}: 100%|██████████| 179/179 [18:49<00:00,  6.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stitching on Index: (0, (0, 0)):   0%|          | 0/16 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-20a4c88e53ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-e95497fc18d1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;31m# print(load_tile_names())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;31m# print(\"Done.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m   \u001b[0mtile_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: debug this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Generate Map View\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# Image.fromarray(stitch(f\"{OUTPUT_MASK_DIR}*\")).save(\"map_predicted_mask.png\") # TODO: generate map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-200401e6550f>\u001b[0m in \u001b[0;36mtile_images\u001b[0;34m(max_records)\u001b[0m\n\u001b[1;32m    624\u001b[0m       \u001b[0mcolumn_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Stitching on Index: ({r}, {c})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mtile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
          ]
        }
      ]
    }
  ]
}