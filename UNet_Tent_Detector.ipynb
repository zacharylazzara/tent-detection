{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zacharylazzara/tent-detection/blob/main/UNet_Tent_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9fRz0G9QhV7"
      },
      "source": [
        "# Tent Detector\n",
        "This notebook detects tents in satellite images using UNet. You can upload your own data and models under settings. The notebook supports running multiple models, each with multiple metrics, but this capability is not reflected in the settings at present.\n",
        "\n",
        "\n",
        "---\n",
        "Defaults used by the models (both uploaded and otherwise):\n",
        "* Loss: `BCEWithLogitsLoss()`\n",
        "* Optimizer: `Adam()`\n",
        "* Metrics: `BinaryF1Score()` and `BinaryJaccardIndex()`\n",
        "\n",
        "If you want to change these you'll have to do it manually; they are specified within the first 14 lines of the [main](#scrollTo=DNtiwVc4WGl6&line=1&uniqifier=1) function (at the time of writing)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize Notebook\n",
        "\n",
        "Imports needed by settings and removal of Colab's `sample_data`.\n",
        "\n",
        "---\n",
        "*Contents: [Colab Imports](#scrollTo=smiYG2o0SD57&line=1&uniqifier=1), [Cleanup](#scrollTo=1yCUNKesMzh7&line=1&uniqifier=1)*"
      ],
      "metadata": {
        "id": "1fhuzvnFilTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "smiYG2o0SD57"
      },
      "outputs": [],
      "source": [
        "#@title Colab Imports\n",
        "import os\n",
        "from enum import Enum\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import runtime\n",
        "\n",
        "class IO_Format(str, Enum):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cleanup\n",
        "%%bash\n",
        "if [ -d 'sample_data' ]; then\n",
        "  rm -r sample_data\n",
        "fi"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1yCUNKesMzh7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr8GAPTqRD9F"
      },
      "source": [
        "#Settings\n",
        "\n",
        "If you set `DATA_SOURCE` to \"upload\", be sure to use pre-tiled data. The number of tiles is the resolution of the heatmap (so if you want a higher resolution heatmap, you'll need to provide smaller tiles). Furthermore, the number of tiles must be square, otherwise we'll have an error when producing the overviews. Also, take note of the `IMAGE_HEIGHT` and `IMAGE_WIDTH` in config; this is the size the tiles should be (it's 512x512 by default). However, you should be able to use tiles of a different size, but they will be resized to 512x512 (you'll need to train a new model if you want to change the default tile size). Finally, make sure the filenames of the tiles sort correctly before uploading them.\n",
        "\n",
        "If the notebook hangs on the settings code block, it's probably waiting for you to upload flies.\n",
        "\n",
        "---\n",
        "*Contents: [Inputs](#scrollTo=9bxlrwmB8skc&uniqifier=1), [Models](#scrollTo=_g4q4wCWzQW_&uniqifier=1), [Output Format](#scrollTo=uMr1qOc3bzl3&uniqifier=1), [Output](#scrollTo=UFtVNiv06x_g&uniqifier=1), [Config](#scrollTo=Rxo20pIfzlxW&uniqifier=1), [Overrides](#scrollTo=JPn265JbTlUM&uniqifier=1)*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inputs\n",
        "DATA_SOURCE = 'default' #@param [\"default\", \"upload\"]\n",
        "ASSERT_DATA_IS_SQUARE = True # We can't change this setting without causing an error (leaving it in incase I address the error in the future)\n",
        "#@markdown ---\n",
        "USING_DEFAULT_DATASET = DATA_SOURCE == 'default'\n",
        "uploaded_map_filename = None\n",
        "if not USING_DEFAULT_DATASET:\n",
        "  uploaded_data = files.upload()\n",
        "  if uploaded_data == {}:\n",
        "    raise Exception('No files uploaded!')\n",
        "  for filename in uploaded_data.keys():\n",
        "    uploaded_map_filename = filename\n",
        "\n",
        "class I_Format(IO_Format):\n",
        "  model = 'pth' #@param [\"pth\"] {allow-input: true}\n",
        "  spreadsheet = 'csv' #@param [\"csv\"] {allow-input: true}\n",
        "  image = 'png' #@param [\"png\", \"jpg\"] {allow-input: true}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9bxlrwmB8skc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "_g4q4wCWzQW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e01da5d-2c1c-4d71-bafe-976e6bf5034f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 06:15:53--  https://github.com/zacharylazzara/tent-detection/raw/main/cli-tent-detector/models/UNet.pth\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/zacharylazzara/tent-detection/main/cli-tent-detector/models/UNet.pth [following]\n",
            "--2023-05-05 06:15:53--  https://raw.githubusercontent.com/zacharylazzara/tent-detection/main/cli-tent-detector/models/UNet.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 496811 (485K) [application/octet-stream]\n",
            "Saving to: ‘UNet.pth’\n",
            "\n",
            "UNet.pth            100%[===================>] 485.17K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-05 06:15:54 (10.6 MB/s) - ‘UNet.pth’ saved [496811/496811]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Models\n",
        "DOWNLOAD_MODEL = True #@param {type:\"boolean\"}\n",
        "UPLOAD_MODEL = False #@param {type:\"boolean\"}\n",
        "REFRESH_MODEL_ON_RESTART = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "MODEL_NAME = 'UNet' #@param {type:\"string\"}\n",
        "DOWNLOAD_MODEL_URL = 'https://github.com/zacharylazzara/tent-detection/raw/main/cli-tent-detector/models/UNet.pth' #@param {type:\"string\"}\n",
        "MODEL_FILENAME = f'{MODEL_NAME}.{I_Format.model}'\n",
        "\n",
        "uploaded_files = None\n",
        "if REFRESH_MODEL_ON_RESTART and (UPLOAD_MODEL or DOWNLOAD_MODEL):\n",
        "  os.remove(MODEL_FILENAME)\n",
        "if DOWNLOAD_MODEL and not (os.path.exists(MODEL_FILENAME)):\n",
        "  !wget $DOWNLOAD_MODEL_URL\n",
        "  if not (os.path.exists(MODEL_FILENAME)):\n",
        "    raise Exception(f'File \"{MODEL_FILENAME}\" not found!')\n",
        "else:\n",
        "  if UPLOAD_MODEL and not (os.path.exists(MODEL_FILENAME)):\n",
        "    uploaded_files = files.upload()\n",
        "    for filename in uploaded_files.keys():\n",
        "      print(f'Uploaded file \"{filename}\"')\n",
        "      if filename != MODEL_FILENAME:\n",
        "        raise Exception('Filename must match MODEL_FILENAME!')\n",
        "  if UPLOAD_MODEL and uploaded_files == {}:\n",
        "    raise Exception('No files uploaded!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Output Format\n",
        "class O_Format(IO_Format):\n",
        "  model = 'pth' #@param [\"pth\"] {allow-input: true}\n",
        "  spreadsheet = 'csv' #@param [\"csv\"] {allow-input: true}\n",
        "  image = 'png' #@param [\"png\", \"jpg\"] {allow-input: true}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uMr1qOc3bzl3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "UFtVNiv06x_g"
      },
      "outputs": [],
      "source": [
        "#@title Output\n",
        "SAVE_TO_GOOGLE_DRIVE = True #@param {type:\"boolean\"}\n",
        "DOWNLOAD_OUTPUT = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "CONTINUE_ON_GOOGLE_DRIVE_ERROR = True #@param {type:\"boolean\"}\n",
        "PLAY_SOUND_ON_COMPLETE = False #@param {type:\"boolean\"}\n",
        "KILL_RUNTIME_ON_COMPLETE = True #@param {type:\"boolean\"}\n",
        "MINUTES_TO_KILL_RUNTIME = 5 #@param {type:\"slider\", min:0, max:30, step:1}\n",
        "MINUTES_TO_KILL_RUNTIME = MINUTES_TO_KILL_RUNTIME*60\n",
        "#@markdown ---\n",
        "DISPLAY_VISUALIZATIONS = True #@param {type:\"boolean\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "Rxo20pIfzlxW"
      },
      "outputs": [],
      "source": [
        "#@title Config\n",
        "TRAIN_MODEL = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "RESIZE_TILES = True #@param {type:\"boolean\"}\n",
        "IMAGE_HEIGHT = 512 #@param {type:\"number\"}\n",
        "IMAGE_WIDTH = 512 #@param {type:\"number\"}\n",
        "IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "#@markdown ---\n",
        "N_EPOCHS = 200 #@param {type:\"number\"}\n",
        "BATCH_SIZE = 8 #@param {type:\"number\"}\n",
        "INIT_LR = 0.0001 #@param {type:\"number\"}\n",
        "TEST_SPLIT = 0.2 #@param {type:\"number\"}\n",
        "RANDOM_STATE = 42 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Overrides\n",
        "ALLOW_SETTINGS_OVERRIDE = True #@param {type:\"boolean\"}\n",
        "TURN_OFF_WANDB_WARNINGS = True #@param {type:\"boolean\"}\n",
        "DISABLE_TRAINING_ON_CPU = ALLOW_SETTINGS_OVERRIDE\n",
        "\n",
        "if ALLOW_SETTINGS_OVERRIDE:\n",
        "  print('Settings Override is enabled')\n",
        "  # If we download/upload the model we don't need to train it\n",
        "  if DOWNLOAD_MODEL or UPLOAD_MODEL:\n",
        "    TRAIN_MODEL = False\n",
        "    print('Disabled TRAIN_MODEL')\n",
        "\n",
        "  # No need to download if we're saving to Google Drive\n",
        "  if SAVE_TO_GOOGLE_DRIVE:\n",
        "    DOWNLOAD_OUTPUT = False\n",
        "    print('Disabled DOWNLOAD_OUTPUT')\n",
        "\n",
        "  # Killing the runtime on complete is only useful if we save the data somewhere first\n",
        "  if not DOWNLOAD_OUTPUT:\n",
        "    MINUTES_TO_KILL_RUNTIME = 0\n",
        "    print(f'Set MINUTES_TO_KILL_RUNTIME to {MINUTES_TO_KILL_RUNTIME}')\n",
        "    if not SAVE_TO_GOOGLE_DRIVE:\n",
        "      KILL_RUNTIME_ON_COMPLETE = False\n",
        "      print('Disabled KILL_RUNTIME_ON_COMPLETE')\n",
        "\n",
        "if TURN_OFF_WANDB_WARNINGS:\n",
        "  # Turning off warnings as there seems to be a bug in\n",
        "  # wandb https://github.com/wandb/wandb/issues/1994\n",
        "  os.environ['WANDB_CONSOLE'] = 'off'\n",
        "  print('Disabled WandB Warnings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JPn265JbTlUM",
        "outputId": "ffc29dac-9c6e-416f-ae07-775710cb1154"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings Override is enabled\n",
            "Disabled TRAIN_MODEL\n",
            "Disabled DOWNLOAD_OUTPUT\n",
            "Set MINUTES_TO_KILL_RUNTIME to 0\n",
            "Disabled WandB Warnings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um5I5sgxSP31"
      },
      "source": [
        "#Definitions\n",
        "\n",
        "The primary logic.\n",
        "\n",
        "---\n",
        "*Contents: [Initialization](#scrollTo=7Vfi33_GtEj2&uniqifier=1), [Models](#scrollTo=QMaVfs-TuId3&uniqifier=1), [Functions](#scrollTo=m5vpYefL-2Cx&uniqifier=1)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vfi33_GtEj2"
      },
      "source": [
        "##Initialization\n",
        "\n",
        "All the environment setup for the notebook.\n",
        "\n",
        "---\n",
        "*Contents: [Imports](#scrollTo=PMBNizBrtJZS&line=1&uniqifier=1), [Device](#scrollTo=yI4mFY9uMosP&line=1&uniqifier=1), [Paths](#scrollTo=vorAQM6jtdOu&line=1&uniqifier=1), [Mount](#scrollTo=8yL4WjAfIOeb&line=1&uniqifier=1), [Environment](#scrollTo=NMzegdPaUd7Q&line=1&uniqifier=1)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PMBNizBrtJZS",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1887fe56-c0a2-4faf-8b50-e9702dd2a257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "!pip install -q tqdm-thread\n",
        "!pip install -q torchmetrics\n",
        "import time\n",
        "import math\n",
        "import csv\n",
        "import cv2\n",
        "import torch\n",
        "import threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from statistics import mean\n",
        "from tqdm.auto import tqdm\n",
        "from tqdm_thread import tqdm_thread\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.nn import Sequential\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn import ConvTranspose2d\n",
        "from torch.nn import Flatten\n",
        "from torch.nn import functional\n",
        "from torch.nn import BatchNorm2d\n",
        "from torch.nn import Softplus\n",
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from torchmetrics.classification import BinaryJaccardIndex\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "from torch import flatten\n",
        "from torch import cat\n",
        "from torch import randn\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import CenterCrop\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI4mFY9uMosP",
        "outputId": "3120b1c2-76fa-4e2a-bfcc-3e4d8926c413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Warning: Training on CPU is disabled!\n"
          ]
        }
      ],
      "source": [
        "#@title Device\n",
        "DEVICE = None\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'\n",
        "else:\n",
        "    DEVICE = 'cpu'\n",
        "PIN_MEMORY = True if DEVICE != 'cpu' else False\n",
        "print(f'Using device: {DEVICE}')\n",
        "\n",
        "if DISABLE_TRAINING_ON_CPU:\n",
        "  TRAIN_MODEL = TRAIN_MODEL if DEVICE != 'cpu' else False\n",
        "  print('Warning: Training on CPU is disabled!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "vorAQM6jtdOu"
      },
      "outputs": [],
      "source": [
        "#@title Paths\n",
        "OUTPUT_PATH         = os.environ['OUTPUT_PATH']         = f'output'\n",
        "Y_PATH              = os.environ['Y_PATH']              = f'{OUTPUT_PATH}/truths'\n",
        "P_PATH              = os.environ['P_PATH']              = f'{OUTPUT_PATH}/predictions'\n",
        "\n",
        "# Default Dataset Paths\n",
        "SRC_PATH            = os.environ['SRC_PATH']            = f'sarpol-zahab-tents'\n",
        "DATA_PATH           = os.environ['DATA_PATH']           = f'{SRC_PATH}/data'\n",
        "IMAGES_PATH         = os.environ['IMAGES_PATH']         = f'{DATA_PATH}/images'\n",
        "MASKS_PATH          = os.environ['MASKS_PATH']          = f'{DATA_PATH}/labels'\n",
        "LABELS_PATH         = os.environ['LABELS_PATH']         = f'{DATA_PATH}/sarpol_counts.csv'\n",
        "\n",
        "# Google Drive Paths\n",
        "G_DRIVE_MOUNT_POINT = os.environ['G_DRIVE_MOUNT_POINT'] = f'g_drive'\n",
        "G_DRIVE_STORAGE     = os.environ['G_DRIVE_STORAGE']     = f'{G_DRIVE_MOUNT_POINT}/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "8yL4WjAfIOeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca01c684-dded-427a-f78c-a9230fa5ef87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unable to mount Google Drive, data will not be copied and must be downloaded manually!\n"
          ]
        }
      ],
      "source": [
        "#@title Mount\n",
        "if SAVE_TO_GOOGLE_DRIVE:\n",
        "  if not os.path.exists(G_DRIVE_MOUNT_POINT):\n",
        "    os.makedirs(G_DRIVE_MOUNT_POINT)\n",
        "  try:\n",
        "    drive.mount(G_DRIVE_MOUNT_POINT)\n",
        "  except Exception as err:\n",
        "    SAVE_TO_GOOGLE_DRIVE = False\n",
        "    KILL_RUNTIME_ON_COMPLETE = False\n",
        "    if not CONTINUE_ON_GOOGLE_DRIVE_ERROR:\n",
        "      print(f'Unable to mount Google Drive!\\n{err}')\n",
        "      raise\n",
        "    else:\n",
        "      print('Warning: Unable to mount Google Drive, data will not be copied and must be downloaded manually!')\n",
        "  else:\n",
        "    print(f'Mounted Google Drive to {G_DRIVE_MOUNT_POINT}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "NMzegdPaUd7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8496034d-51cf-4148-8e81-92334528abca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sarpol-zahab-tents'...\n",
            "remote: Enumerating objects: 383, done.\u001b[K\n",
            "remote: Counting objects: 100% (383/383), done.\u001b[K\n",
            "remote: Compressing objects: 100% (332/332), done.\u001b[K\n",
            "remote: Total 383 (delta 54), reused 367 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (383/383), 14.33 MiB | 19.59 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Environment\n",
        "if not os.path.exists(SRC_PATH) and USING_DEFAULT_DATASET:\n",
        "  !git clone https://github.com/tofighi/sarpol-zahab-tents.git\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "  os.makedirs(OUTPUT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMaVfs-TuId3"
      },
      "source": [
        "##Models\n",
        "\n",
        "All the classes used in the notebook.\n",
        "\n",
        "---\n",
        "*Contents: [UNet](#scrollTo=2IeM8zsTBq5d&line=1&uniqifier=1), [Dataset](#scrollTo=U00pSVfc0M4f&line=1&uniqifier=1), [Model](#scrollTo=zhQM-Oq8fWS5&line=1&uniqifier=1), [Visualizations](#scrollTo=y2Ts2LPnGt2K&line=1&uniqifier=1)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "2IeM8zsTBq5d"
      },
      "outputs": [],
      "source": [
        "#@title UNet\n",
        "# Adapted from: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n",
        "class Block(Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Block, self).__init__()\n",
        "    self.double_conv2d = Sequential(\n",
        "        Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        BatchNorm2d(out_channels),\n",
        "        ReLU(inplace=True),\n",
        "        Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        BatchNorm2d(out_channels),\n",
        "        ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv2d(x)\n",
        "\n",
        "class Encoder(Module):\n",
        "  def __init__(self, channels=(3, 16, 32, 64)):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.encoder_blocks = ModuleList([Block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n",
        "    self.pool = MaxPool2d(2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    block_outputs = []\n",
        "    for block in self.encoder_blocks:\n",
        "      x = block(x)\n",
        "      block_outputs.append(x)\n",
        "      x = self.pool(x)\n",
        "    return block_outputs\n",
        "\n",
        "class Decoder(Module):\n",
        "  def __init__(self, channels=(64, 32, 16)):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.up_convs = ModuleList([ConvTranspose2d(channels[i], channels[i+1], 2, 2) for i in range(len(channels)-1)])\n",
        "    self.decoder_blocks = ModuleList([Block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n",
        "  \n",
        "  def crop(self, encoder_features, x):\n",
        "    (_, _, H, W) = x.shape\n",
        "    return CenterCrop([H, W])(encoder_features)\n",
        "  \n",
        "  def forward(self, x, encoder_features):\n",
        "    for i in range(len(self.up_convs)):\n",
        "      x = self.up_convs[i](x)\n",
        "      encoder_feature = self.crop(encoder_features[i], x)\n",
        "      x = cat([x, encoder_feature], dim=1)\n",
        "      x = self.decoder_blocks[i](x)\n",
        "    return x\n",
        "\n",
        "class UNet(Module):\n",
        "  def __str__(self) -> str:\n",
        "    return 'UNet'\n",
        "\n",
        "  def __init__(self, encoder_channels=(3, 16, 32, 64), decoder_channels=(64, 32, 16), classes=1, retain_dim=True, output_size=(512, 512)):\n",
        "    super(UNet, self).__init__()\n",
        "    self.encoder = Encoder(encoder_channels)\n",
        "    self.decoder = Decoder(decoder_channels)\n",
        "    self.head = Conv2d(decoder_channels[-1], classes, 1)\n",
        "    self.retain_dim = retain_dim\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoder_features = self.encoder(x)\n",
        "    decoder_features = self.decoder(encoder_features[::-1][0], encoder_features[::-1][1:])\n",
        "    map = self.head(decoder_features)\n",
        "    if self.retain_dim:\n",
        "      map = functional.interpolate(map, self.output_size)\n",
        "    return map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "U00pSVfc0M4f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Dataset\n",
        "# Adapted from: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n",
        "class SegmentationDataset(Dataset):\n",
        "  def __init__(self, dataframe, transformations = None):\n",
        "    self.dataframe = dataframe\n",
        "    self.transformations = transformations\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe.index)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    If there's no mask path, generate a blank mask. This is for the case where\n",
        "    we only want to perform predictions and as such haven't included any masks.\n",
        "    Works as expected when mask paths are included.\n",
        "    \"\"\"\n",
        "    image_paths = self.dataframe.iloc[index]['image_paths']\n",
        "    mask_paths = self.dataframe.iloc[index]['mask_paths']\n",
        "    image = cv2.cvtColor(cv2.imread(image_paths), cv2.COLOR_BGR2RGB)\n",
        "    mask = None\n",
        "    if mask_paths:\n",
        "      mask = cv2.threshold(cv2.imread(mask_paths, cv2.IMREAD_GRAYSCALE), 150, 255, cv2.THRESH_BINARY)[1]\n",
        "    else:\n",
        "      mask = cv2.threshold(np.zeros((image.shape[0], image.shape[1], 1), dtype=np.uint8), 150, 255, cv2.THRESH_BINARY)[1]\n",
        "    if self.transformations:\n",
        "      image = self.transformations(image)\n",
        "      mask = self.transformations(mask)\n",
        "    return (image, mask, self.dataframe.iloc[index]['labels'], self.dataframe.index[index])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "class Model():\n",
        "  \"\"\"Makes it possible to work with models in a more generalized and scalable fashion.\"\"\"\n",
        "\n",
        "  def __init__(self, root_directory:str, model, loss_fn, opt_fn, metric_fns=None, **kwargs) -> None:\n",
        "    self.device = kwargs.get('device', DEVICE)\n",
        "    self.model = model.to(self.device)\n",
        "    self.loss_fn = loss_fn\n",
        "    self.opt_fn = opt_fn(self.model.parameters(), lr=kwargs.get('lr', INIT_LR))\n",
        "    self.metric_fns = metric_fns\n",
        "    self.format = kwargs.get('format', O_Format)\n",
        "    self.dirs = make_directories({'output':       f'{root_directory}', \n",
        "                                  'predictions':  f'{root_directory}/tiles', \n",
        "                                  'history':      f'{root_directory}/metrics'})\n",
        "\n",
        "    # TODO: handle the case when metrics is none (it might break history); might\n",
        "    # want to do the same with loss if we're not interested in training a new model.\n",
        "    self.history = {\n",
        "        't': {str(key):value for (key, value) in zip(['losses', *self.metric_fns], [[] for _ in [*self.metric_fns, '']])},\n",
        "        'v': {str(key):value for (key, value) in zip(['losses', *self.metric_fns], [[] for _ in [*self.metric_fns, '']])}}\n",
        "  \n",
        "  def __str__(self) -> str:\n",
        "    return str(self.model)\n",
        "\n",
        "  def train(self, t_loader:DataLoader, v_loader:DataLoader, epochs:int, pbar:tqdm=None, **kwargs) -> pd.DataFrame:\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    if self.metric_fns:\n",
        "      for metric_fn in self.metric_fns:\n",
        "        metric_fn.to(self.device)\n",
        "\n",
        "    if pbar:\n",
        "      pbar = pbar(range(epochs))\n",
        "      pbar.set_description(f'Training {self.model}')\n",
        "    for e in pbar if pbar else range(epochs):\n",
        "      self.model.train()\n",
        "      losses = []\n",
        "      for (i, (x, y, _, _)) in enumerate(t_loader):\n",
        "        (x, y) = (x.to(self.device), y.to(self.device))\n",
        "\n",
        "        pred = self.model(x)\n",
        "        loss = self.loss_fn(pred, y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if loss.requires_grad:\n",
        "          self.opt_fn.zero_grad()\n",
        "          loss.backward()\n",
        "        self.opt_fn.step()\n",
        "\n",
        "        if self.metric_fns:\n",
        "          for metric_fn in self.metric_fns:\n",
        "            metric_fn.update(pred, y)\n",
        "          \n",
        "      self.history['t']['losses'].append(mean(losses))\n",
        "      if self.metric_fns:\n",
        "        for metric_fn in self.metric_fns:\n",
        "          self.history['t'][f'{metric_fn}'].append(metric_fn.compute().cpu().detach().numpy().item())\n",
        "          metric_fn.reset()\n",
        "      \n",
        "      # Append the prediction metrics into history. We might not actually need a lambda for this.\n",
        "      [(lambda k, v: [self.history['v'][k].append(p) for p in v])(k, v) for k, v in self.predict(v_loader)[2].items()]\n",
        "\n",
        "      if pbar:\n",
        "        pbar.set_description(f'Epoch({e+1}/{epochs}) Training {self.model}, Training Loss: {self.history[\"t\"][\"losses\"][-1]:.4f}, Validation Loss: {self.history[\"v\"][\"losses\"][-1]:.4f}')\n",
        "\n",
        "    if kwargs.get('save_model'): self.save_model()\n",
        "    if kwargs.get('save_history'): self.save_training_history()\n",
        "    return pd.DataFrame(self.history).fillna(np.nan)\n",
        "\n",
        "  def predict(self, loader:DataLoader, pbar:tqdm=None, save_predictions:bool=False) -> tuple[list, pd.DataFrame | None, pd.DataFrame]:\n",
        "    \"\"\"Evaluates the model.\"\"\"\n",
        "    history = {str(key):value for (key, value) in zip(['losses', *self.metric_fns], [[] for _ in [*self.metric_fns, '']])}\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "      self.model.eval()\n",
        "      \n",
        "      losses = []\n",
        "      if pbar:\n",
        "        pbar = pbar(loader)\n",
        "        pbar.set_description(f'Evaluating {self.model}')\n",
        "      for (x, y, _, name) in pbar if pbar else loader:\n",
        "        (x, y) = (x.to(self.device), y.to(self.device))\n",
        "        p = self.model(x)\n",
        "\n",
        "        if self.loss_fn:\n",
        "          loss = self.loss_fn(p, y)\n",
        "          losses.append(loss.item())\n",
        "        if self.metric_fns:\n",
        "          for metric_fn in self.metric_fns:\n",
        "            metric_fn.update(p, y)\n",
        "\n",
        "        for batch, mask in enumerate(p.cpu().detach()):\n",
        "          predictions.append({'name':name[batch], 'mask': mask})\n",
        "\n",
        "      if losses != []:\n",
        "        history['losses'].append(mean(losses))\n",
        "      if self.metric_fns:\n",
        "        for metric_fn in self.metric_fns:\n",
        "          history[f'{metric_fn}'].append(metric_fn.compute().cpu().detach().numpy().item())\n",
        "          metric_fn.reset()\n",
        "\n",
        "    return predictions, self.__save_predictions(predictions) if save_predictions else None, pd.DataFrame(history).fillna(np.nan)\n",
        "\n",
        "  def __save_predictions(self, predictions:list, **kwargs) -> pd.DataFrame:\n",
        "    \"\"\"Saves predictions to disk and outputs a dataframe with the names, paths, and labels.\"\"\"\n",
        "    saved_predictions = []\n",
        "    for prediction in predictions:\n",
        "      out_path = f'{kwargs.get(\"directory_override\", self.dirs.predictions)}/{prediction[\"name\"]}.{kwargs.get(\"format_override\", self.format.image)}'\n",
        "      save_image(prediction['mask'], out_path)\n",
        "      saved_predictions.append({'names':prediction[\"name\"], \n",
        "                                'image_paths': None,\n",
        "                                'mask_paths':out_path, \n",
        "                                'labels':self.__count_contours(prediction['mask'])})\n",
        "    return pd.DataFrame(saved_predictions).set_index('names').fillna(np.nan)\n",
        "\n",
        "  # TODO: either remove save_predictions_to_spreadsheet or set it up so we call it from __save_predictions\n",
        "  # def save_predictions_to_spreadsheet(self, predictions_dataframe, **kwargs):\n",
        "  #   output_path = f'{kwargs.get(\"directory_override\", self.dirs.predictions)}/{kwargs.get(\"filename_override\", f\"labels.{self.format.spreadsheet}\")}'\n",
        "  #   predictions_dataframe.to_csv(output_path)\n",
        "  #   return output_path\n",
        "\n",
        "  def save_model(self, **kwargs) -> None:\n",
        "    if kwargs.get('state_dict', True):\n",
        "      torch.save(self.model.state_dict(), f'{kwargs.get(\"directory_override\", self.dirs.output)}/{kwargs.get(\"filename_override\", f\"{self.model}.{self.format.model}\")}')\n",
        "    else:\n",
        "      torch.save(self.model, f'{kwargs.get(\"directory_override\", self.dirs.output)}/{kwargs.get(\"filename_override\", f\"{self.model}.{self.format.model}\")}')\n",
        "\n",
        "  def save_training_history(self) -> tuple[str, list]:\n",
        "    \"\"\"Saves training history (loss and metric values per epoch) to disk.\"\"\"\n",
        "    model_name = str(self.model).replace('()', '')\n",
        "    loss_fn_name = str(self.loss_fn).replace('()', '')\n",
        "    loss_output_path = f'{self.dirs.history}/{model_name}_loss_{loss_fn_name}.{self.format.image}'\n",
        "    metric_output_paths = []\n",
        "\n",
        "    # Loss\n",
        "    plt.plot(self.history['t']['losses'], label='training')\n",
        "    plt.plot(self.history['v']['losses'], label='validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.ylim([0, 1])\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(f'{model_name} Loss ({loss_fn_name})')\n",
        "    plt.savefig(loss_output_path)\n",
        "\n",
        "    # Cleanup\n",
        "    plt.clf()\n",
        "    plt.cla()\n",
        "    plt.close()\n",
        "\n",
        "    # Metrics\n",
        "    if self.metric_fns: # TODO: just iterate through self.history instead\n",
        "      for metric_fn in self.metric_fns:\n",
        "        metric_fn_name = str(metric_fn).replace('()', '')\n",
        "        metric_output_path = f'{self.dirs.history}/{model_name}_metric_{metric_fn_name}.{self.format.image}'\n",
        "\n",
        "        plt.plot(self.history['t'][f'{metric_fn}'], label='training')\n",
        "        plt.plot(self.history['v'][f'{metric_fn}'], label='validation')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Score')\n",
        "        plt.ylim([0, 1])\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.title(f'{model_name} Metric ({metric_fn_name})')\n",
        "        plt.savefig(metric_output_path)\n",
        "\n",
        "        # Cleanup\n",
        "        plt.clf()\n",
        "        plt.cla()\n",
        "        plt.close()\n",
        "\n",
        "        metric_output_paths.append(metric_output_path)\n",
        "    \n",
        "    return loss_output_path, metric_output_paths\n",
        "\n",
        "  def __save_prediction_performance(self, p_performance) -> str:\n",
        "    \"\"\"Saves the loss and metrics of the prediction if applicable.\"\"\"\n",
        "    model_name = str(self.model).replace('()', '')\n",
        "    output_path = f'{self.dirs.history}/{model_name}_performance.{self.format.image}'\n",
        "    bar_data = {}\n",
        "    if self.loss_fn:\n",
        "      bar_data[str(self.loss_fn).replace('()', '')] = mean(p_performance['losses'])\n",
        "    if self.metric_fns:\n",
        "      for metric_fn in self.metric_fns:\n",
        "        bar_data[str(metric_fn).replace('()', '')] = mean(p_performance[f'{metric_fn}'])\n",
        "    if bar_data:\n",
        "      plt.bar(list(bar_data.keys()), list(bar_data.values()))\n",
        "      plt.ylim([0, 1])\n",
        "      plt.title(f'{model_name} Mean Performance')\n",
        "      plt.savefig(output_path)\n",
        "      plt.close()\n",
        "    return output_path\n",
        "\n",
        "  def __contours(self, p) -> list:\n",
        "    \"\"\"Used to locate blobs in the prediction that correspond to tents.\"\"\"\n",
        "    # Adapted from https://stackoverflow.com/questions/48154642/how-to-count-number-of-dots-in-an-image-using-python-and-opencv\n",
        "    img = p.numpy().T.astype(np.uint8).copy()\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
        "    closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "    cnts = cv2.findContours(closing, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
        "    max_area = 20\n",
        "    xcnts = []\n",
        "    for cnt in cnts:\n",
        "      if cv2.contourArea(cnt) < max_area:\n",
        "        xcnts.append(cnt)\n",
        "    return xcnts\n",
        "\n",
        "  def __count_contours(self, p) -> int:\n",
        "    \"\"\"Count the number of blobs in the prediction mask (i.e., number of tents)\"\"\"\n",
        "    return int(len(self.__contours(p)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zhQM-Oq8fWS5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizations\n",
        "class Visualizations():\n",
        "  \"\"\"Allows us to work with the images while ensuring we stay in the right directory.\"\"\"\n",
        "  def __init__(self, root_directory:str, format:O_Format=O_Format) -> None:\n",
        "    self.format = format\n",
        "    self.dirs = make_directories({'output'    :f'{root_directory}',\n",
        "                                  'tiles'     :f'{root_directory}/tiles',\n",
        "                                  't_overlay' :f'{root_directory}/tiles/overlay'})\n",
        "\n",
        "  def merge(self, tiles:torch.Tensor, tile_row) -> torch.Tensor:\n",
        "    \"\"\"Merges a row of tiles. The total number of tiles must be divisible by 2.\"\"\"\n",
        "    if tiles == None:\n",
        "      tiles = torch.cat(tuple(tile_row), 2)\n",
        "    else:\n",
        "      tiles = torch.cat((tiles, torch.cat(tuple(tile_row), 2)), 1)\n",
        "    return tiles\n",
        "  \n",
        "  def tile(self, loader:DataLoader, output_name_x:str, output_name_y:str, pbar:tqdm=None) -> tuple[str, str]:\n",
        "    \"\"\"Tiles using a loader. Total number of tiles must be divisible by 2.\"\"\"\n",
        "    output_path_x = output_path_y = ''\n",
        "    output_x = output_y = None\n",
        "\n",
        "    if pbar:\n",
        "      pbar = pbar(loader)\n",
        "      pbar.set_description(f'Tiling')\n",
        "    for (x, y, _, _) in pbar if pbar else loader:\n",
        "      if output_name_x:\n",
        "        output_x = self.merge(output_x, x)\n",
        "      if output_name_y:\n",
        "        output_y = self.merge(output_y, y)\n",
        "    \n",
        "    if output_x is not None:\n",
        "      output_path_x = f'{self.dirs.output}/{output_name_x}.{self.format.image}'\n",
        "      save_image(output_x, output_path_x)\n",
        "    if output_y is not None:\n",
        "      output_path_y = f'{self.dirs.output}/{output_name_y}.{self.format.image}'\n",
        "      save_image(output_y, output_path_y)\n",
        "    \n",
        "    return output_path_x, output_path_y\n",
        "  \n",
        "  def overlay_from_path(self, background_path:str, foreground_path:str, bg_opacity:float=1, fg_opacity:float=1):\n",
        "    with Image.open(background_path).convert('RGB') as background_image:\n",
        "      background_image = background_image\n",
        "    with Image.open(foreground_path).convert('RGB') as foreground_image:\n",
        "      foreground_image = foreground_image\n",
        "    return self.overlay_image(background_image, foreground_image, bg_opacity, fg_opacity)\n",
        "  \n",
        "  def save_overlay_from_path(self, background_path:str, foreground_path:str, output_name, bg_opacity:float=1, fg_opacity:float=1, tile:bool=False):\n",
        "    return self.save_overlay(self.overlay_from_path(background_path, foreground_path, bg_opacity, fg_opacity), output_name, tile)\n",
        "  \n",
        "  def overlay_image(self, background_image, foreground_image, bg_opacity:float=1, fg_opacity:float=1):\n",
        "    foreground_image = np.array(foreground_image.resize(background_image.size)) # Make sure foreground image matches background image size\n",
        "    background_image = np.array(background_image)\n",
        "\n",
        "    for channel in range(1, 2):\n",
        "      foreground_image[foreground_image[:,:,channel] > 0, channel] = 0\n",
        "      \n",
        "    overlay = cv2.addWeighted(background_image, bg_opacity, foreground_image, fg_opacity, 0)\n",
        "    return Image.fromarray(overlay)\n",
        "\n",
        "  def save_overlay(self, overlayed_image:Image, output_name:str, tile:bool=False):\n",
        "    output_path = f'{self.dirs.t_overlay if tile else self.dirs.output}/{output_name}.{self.format.image}'\n",
        "    overlayed_image.save(output_path)\n",
        "    return output_path\n",
        "\n",
        "  def save_heatmap(self, image_path:str, dataframe:pd.DataFrame, output_name:str, title:str):\n",
        "    data = [x for x in np.array_split(dataframe['labels'].replace(0, np.nan).tolist(), int(math.sqrt(dataframe.shape[0])))]\n",
        "\n",
        "    sns.set(font_scale=1)\n",
        "    _, ax = plt.subplots(figsize=(15, 15))\n",
        "    ax.set_title(title)\n",
        "    \n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    ax.tick_params(left=False, bottom=False)\n",
        "    sns.heatmap(data, annot=True, square=True, fmt='.5g', alpha=0.3, zorder=2, cbar_kws={'shrink': 0.7}, ax=ax)\n",
        "\n",
        "    with Image.open(image_path).convert(\"RGB\") as image:\n",
        "      ax.imshow(image, aspect=ax.get_aspect(), extent=ax.get_xlim()+ax.get_ylim(), zorder=1)\n",
        "\n",
        "    output_path = f'{self.dirs.output}/{output_name}.{self.format.image}'\n",
        "\n",
        "    plt.savefig(output_path, bbox_inches='tight')\n",
        "    \n",
        "    # Cleanup\n",
        "    plt.clf()\n",
        "    plt.cla()\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n",
        "  def region_box(self, name:str, c:int, w:int, h:int):\n",
        "    \"\"\"Draws the region box with the tent count and name.\"\"\"\n",
        "    shape = [(0, 0), (w-1, h-1)]\n",
        "    \n",
        "    img = Image.new('RGBA', (w, h))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    font = ImageFont.truetype('LiberationMono-Regular.ttf', 50)\n",
        "    draw.text((10, 10), f'{c}', font=font, fill=(255, 0, 0))\n",
        "\n",
        "    _, _, tw, th = draw.textbbox((0, 0), name, font=font)\n",
        "\n",
        "    tw = w-tw\n",
        "    th = h-th\n",
        "\n",
        "    draw.text((tw-20, th-10), name, font=font, fill=(255, 0, 0))\n",
        "\n",
        "    rec = ImageDraw.Draw(img)  \n",
        "    rec.rectangle(shape, fill = None, outline ='red')\n",
        "\n",
        "    return img\n",
        "\n",
        "  def save_region(self, loader:DataLoader, overview_path:str):\n",
        "    \"\"\"\n",
        "    Generates an outline around each image and puts the number of tents\n",
        "    in the upper left corner of the image, with the image name in the bottom right.\n",
        "    Mostly redundant since we're using Seaborn for the heatmap anyway, but may\n",
        "    be useful for troubleshooting.\n",
        "    \"\"\"\n",
        "    region_paths = []\n",
        "    pbar = tqdm(loader)\n",
        "    pbar.set_description(f'Creating region overlays...')\n",
        "    region_overview = None\n",
        "    for (_, y, c, name) in pbar:\n",
        "      regions = []\n",
        "      for batch, img in enumerate(y.cpu().detach()):\n",
        "        out_path = f'{self.dirs.output}/{name[batch]}.{format.image}'\n",
        "        region = self.region_box(name[batch], c[batch], img.shape[1], img.shape[2])\n",
        "        region.save(out_path)\n",
        "        region_paths.append({'names':name[batch], 'region_paths':out_path})\n",
        "        pbar.set_description(f'Saved region overlay for {name[batch]} to {out_path}')\n",
        "\n",
        "        transform = transforms.Compose([transforms.PILToTensor()])\n",
        "        region = transform(region)\n",
        "        regions.append(region)\n",
        "\n",
        "      region_overview = self.merge(region_overview, regions).double()\n",
        "    save_image(region_overview, overview_path)\n",
        "    return pd.DataFrame(region_paths).set_index('names').fillna(np.nan)"
      ],
      "metadata": {
        "id": "y2Ts2LPnGt2K",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5vpYefL-2Cx"
      },
      "source": [
        "##Functions\n",
        "\n",
        "All the functions used in the notebook.\n",
        "\n",
        "---\n",
        "*Contents: [Load](#scrollTo=1bhM-gJ8uLKN&line=1&uniqifier=1), [Make Directories](#scrollTo=tpzX4vj8mNzj&line=1&uniqifier=1), [Save Data](#scrollTo=q18yjjSfS8ya&line=1&uniqifier=1), [Evaluator](#scrollTo=TJ2eEW-qy8Du&line=1&uniqifier=1)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1bhM-gJ8uLKN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load\n",
        "def load_data(x_images_path:str=IMAGES_PATH, y_masks_path:str=MASKS_PATH, **kwargs) -> pd.DataFrame:\n",
        "  if kwargs.get('directory'):\n",
        "    return d_data(kwargs.get('directory'), y_masks_path, **kwargs)\n",
        "  else:\n",
        "    return c_data(x_images_path, y_masks_path, kwargs.get('csv_path', LABELS_PATH), **kwargs)\n",
        "\n",
        "def c_data(x_images_path:str, y_masks_path:str, csv_path:str, **kwargs) -> pd.DataFrame:\n",
        "  return associate_data(x_images_path, y_masks_path, csv_data(csv_path, **kwargs), **kwargs)\n",
        "\n",
        "def d_data(x_images_path:str, y_masks_path:str=None, **kwargs) -> pd.DataFrame:\n",
        "  return associate_data(x_images_path, y_masks_path, directory_data(x_images_path, kwargs.get('format', I_Format)), **kwargs)\n",
        "\n",
        "def associate_data(x_images_path:str, y_masks_path:str, rows:list, **kwargs) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Returns a dataframe with the feature and target paths, along with the image\n",
        "  name and number of tents (i.e., labels).\n",
        "  \"\"\"\n",
        "  if kwargs.get('assert_square', True): assert len(rows) % 2 == 0, f'Number of rows ({len(rows)}) are not even!'\n",
        "  return pd.DataFrame({\n",
        "    'names'        : [row[0].split('.')[0] for row in rows],\n",
        "    'image_paths'  : [str(next(Path(x_images_path).glob(row[0]))) if x_images_path else None for row in rows],\n",
        "    'mask_paths'   : [str(next(Path(y_masks_path).glob(row[0]))) if y_masks_path else None for row in rows],\n",
        "    'labels'       : [int(row[1]) for row in rows]\n",
        "  }).set_index('names').astype({'labels': 'int'})\n",
        "\n",
        "def csv_data(csv_path, **kwargs) -> list[tuple[str, int]]:\n",
        "  rows = pd.read_csv(csv_path, header=kwargs.get('header'))\n",
        "  return list(rows.itertuples(index=False, name=None))\n",
        "\n",
        "def directory_data(directory:str, format:I_Format) -> list[tuple[str, int]]:\n",
        "  names = []\n",
        "  for filepath in Path(directory).glob(f'*.{format.image}'):\n",
        "    names.append((filepath.name, -1))\n",
        "  return names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make Directories\n",
        "def make_directories(directories:dict[str,str]) -> Enum:\n",
        "  for directory in directories.values():\n",
        "    if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "  return Enum('Directories', directories, type=str)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tpzX4vj8mNzj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save Data\n",
        "def save_data(output_dir, data, transformations, pbar=None, **kwargs):\n",
        "  dataset = SegmentationDataset(data, transformations)\n",
        "  loader = DataLoader(dataset, shuffle=False, batch_size=int(math.sqrt(len(dataset))), pin_memory=PIN_MEMORY, num_workers=os.cpu_count(), persistent_workers=True)\n",
        "  vis = Visualizations(output_dir, kwargs.get('format', O_Format))\n",
        "\n",
        "  y = kwargs.get('y', 'y')\n",
        "\n",
        "  print('Saving overviews...')\n",
        "  x_overview_path, y_overview_path = vis.tile(loader,\n",
        "                                              kwargs.get('x_overview_name', 'x_overview' if not kwargs.get('x_overview_path') else None),\n",
        "                                              f'{y}_overview',\n",
        "                                              pbar)\n",
        "  x_overview_path = kwargs.get('x_overview_path', x_overview_path)\n",
        "\n",
        "  overlay_path = heatmap_path = None\n",
        "  if x_overview_path:\n",
        "    print('Saving overlays...')\n",
        "    np.vectorize(vis.save_overlay_from_path)(data['image_paths'], data['mask_paths'], np.vectorize((lambda n: n))(data.index), 0.7, tile=True)\n",
        "    \n",
        "    print('Saving overview overlay...')\n",
        "    overlay_path = vis.save_overlay_from_path(x_overview_path, y_overview_path, f'{y}_overlay', 0.7)\n",
        "\n",
        "    print('Saving heatmap...')\n",
        "    heatmap_path = vis.save_heatmap(overlay_path, data, f'{y}_heatmap', kwargs.get('heatmap_title', 'Number of Tents per Region'))\n",
        "\n",
        "    print('Done.')\n",
        "  return {'x_overview_path': x_overview_path,\n",
        "          'y_overview_path': y_overview_path,\n",
        "          'overlay_path': overlay_path,\n",
        "          'heatmap_path': heatmap_path}"
      ],
      "metadata": {
        "id": "q18yjjSfS8ya",
        "cellView": "form"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluator\n",
        "def evaluator(models:list, data:pd.DataFrame, transformations, train:bool=TRAIN_MODEL, **kwargs) -> None:\n",
        "  dataset = SegmentationDataset(data, transformations)\n",
        "  row_size = int(math.sqrt(len(dataset)))\n",
        "  if row_size < 1:\n",
        "    raise Exception(\"Dataset contains no data. Make sure you set the input format to the correct format!\")\n",
        "  loader = DataLoader(dataset, shuffle=False, batch_size=row_size, pin_memory=PIN_MEMORY, num_workers=os.cpu_count(), persistent_workers=True)\n",
        "\n",
        "  paths = {}\n",
        "  for model in models:\n",
        "    if train:\n",
        "      model.train(t_loader, v_loader, N_EPOCHS, tqdm, save_model=True, save_history=True)\n",
        "    p_data = model.predict(loader, tqdm, save_predictions=True)[1].combine_first(data).sort_index() # Fill in the missing feature paths while preserving the prediction paths\n",
        "    paths[str(model)] = save_data(P_PATH, p_data, transformations, tqdm, y='p', heatmap_title='Detected Tents per Region', **kwargs)\n",
        "\n",
        "  return paths"
      ],
      "metadata": {
        "id": "TJ2eEW-qy8Du",
        "cellView": "form"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwAZOPs5tIwZ"
      },
      "source": [
        "#Output\n",
        "Output will be saved to Colab's default directory (`/content`). When computation is complete, the output will be compressed as a zip file. If Google Drive is connected, the zip file will be copied to the root of your Google Drive with the filename \"`tent_detector_output.zip`\".\n",
        "\n",
        "---\n",
        "*Contents: [Main](#scrollTo=DNtiwVc4WGl6&line=1&uniqifier=1), [Display Heatmap](#scrollTo=9eSPRdD646oW&line=1&uniqifier=1), [Complete](#scrollTo=F3e0Itqf4_Uh&line=1&uniqifier=1)*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Main\n",
        "models = []\n",
        "if UPLOAD_MODEL or DOWNLOAD_MODEL:\n",
        "  model = None\n",
        "  checkpoint = torch.load(MODEL_FILENAME, map_location=DEVICE)\n",
        "  \n",
        "  try:\n",
        "    checkpoint.eval()\n",
        "  except AttributeError:\n",
        "    print('Loading checkpoint')\n",
        "    model = UNet()\n",
        "    model.load_state_dict(checkpoint)\n",
        "  else:\n",
        "    print('Loading model')\n",
        "    model = checkpoint\n",
        "\n",
        "  models.append(Model(P_PATH,\n",
        "                      model,\n",
        "                      BCEWithLogitsLoss(),\n",
        "                      Adam,\n",
        "                      [BinaryF1Score(), BinaryJaccardIndex()]))\n",
        "else:\n",
        "  models.append(Model(P_PATH,\n",
        "                      UNet(),\n",
        "                      BCEWithLogitsLoss(),\n",
        "                      Adam,\n",
        "                      [BinaryF1Score(), BinaryJaccardIndex()]))\n",
        "\n",
        "trans_settings = [transforms.ToPILImage(), transforms.ToTensor()]\n",
        "if RESIZE_TILES:\n",
        "  trans_settings.append(transforms.Resize(IMAGE_SIZE, antialias=True))\n",
        "transformations = transforms.Compose(trans_settings)\n",
        "\n",
        "results = None\n",
        "if USING_DEFAULT_DATASET:\n",
        "  y_data = load_data()\n",
        "\n",
        "  training_data, validation_data = train_test_split(y_data, test_size=TEST_SPLIT, random_state=RANDOM_STATE)\n",
        "  training_dataset = SegmentationDataset(training_data, transformations)\n",
        "  validation_dataset = SegmentationDataset(validation_data, transformations)\n",
        "\n",
        "  t_loader = DataLoader(training_dataset, shuffle=True, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count(), persistent_workers=True)\n",
        "  v_loader = DataLoader(validation_dataset, shuffle=True, batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY, num_workers=os.cpu_count(), persistent_workers=True)\n",
        "\n",
        "  t_count = len(training_data)\n",
        "  v_count = len(validation_data)\n",
        "\n",
        "  t_ratio = 1-v_count/t_count\n",
        "  v_ratio = v_count/t_count\n",
        "\n",
        "  print(f'Training to Validation Ratio\\n')\n",
        "  print(f'Training ({t_count}): \\t{t_ratio*100:>10.2f}%')\n",
        "  print(f'Validation ({v_count}): \\t{v_ratio*100:>10.2f}%')\n",
        "  print(f'Total ({t_count + v_count}): \\t\\t{t_ratio*100 + v_ratio*100:>10.2f}%\\n') \n",
        "\n",
        "  assert t_ratio + v_ratio == 1 # Sanity Check\n",
        "\n",
        "  gt_paths = save_data(Y_PATH, y_data, transformations, tqdm, heatmap_title='Actual Tents per Region')\n",
        "  results = evaluator(models, y_data, transformations, x_overview_path=gt_paths['x_overview_path'])\n",
        "else:\n",
        "  results = evaluator(models, load_data(directory='.', y_masks_path=None, assert_square=ASSERT_DATA_IS_SQUARE), transformations, train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "7df6b4b4a68146ec85f9fd92f168bf85",
            "baea8bb66e174b14a6ca3809bda83478",
            "662be5d39f2f455cb3ac956944513e65",
            "c2d9dec0652243ab99c5afacd5c21384",
            "7f8fc83b51b5415cbe6e50b35a79e15f",
            "608dde58e0ea4cab8a9a53f2ea3a719d",
            "912f407d0d9b4809908cd0ab82536757",
            "07060f27ee4d42008c037fbfac998a71",
            "19db5e424dc44315aa970ab91790779c",
            "738c25fba1f24a95a7b8e23023f0be99",
            "02a1ff0c7a3a4770ad25c63fca512fdf"
          ]
        },
        "id": "DNtiwVc4WGl6",
        "outputId": "727d261b-5c4c-454a-cd4e-7301da7d7745",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint\n",
            "Training to Validation Ratio\n",
            "\n",
            "Training (204): \t     74.51%\n",
            "Validation (52): \t     25.49%\n",
            "Total (256): \t\t    100.00%\n",
            "\n",
            "Saving overviews...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7df6b4b4a68146ec85f9fd92f168bf85"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Heatmap\n",
        "if DISPLAY_VISUALIZATIONS:\n",
        "  for modelname, result in results.items():\n",
        "    print(f'Displaying {modelname} Heatmap ({result[\"heatmap_path\"]})')\n",
        "    with Image.open(result['heatmap_path']) as image:\n",
        "      image.thumbnail((600, 600))\n",
        "      image.show()\n",
        "    print('\\n\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9eSPRdD646oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F3e0Itqf4_Uh"
      },
      "outputs": [],
      "source": [
        "#@title Complete\n",
        "\n",
        "# Since there's seemingly no way to reasonably wait for files.download, we have\n",
        "# to wait a specified period of time before disconnecting the session. This only\n",
        "# applies when we trigger a browser download; copying files to Google Drive can\n",
        "# be done in a blocking way (thus this is the preferred method).\n",
        "\n",
        "def kill_runtime(seconds):\n",
        "  with tqdm_thread(desc='Terminating session...', total=seconds, step_sec=0.5):\n",
        "    time.sleep(seconds)\n",
        "  print('Session terminated automatically.')\n",
        "  time.sleep(1)\n",
        "  runtime.unassign()\n",
        "\n",
        "def end_runtime(seconds):\n",
        "  if seconds > 0:\n",
        "    print(f'Terminating Session in {seconds//60} minutes...')\n",
        "    threading.Thread(target=kill_runtime, args=[seconds]).start()\n",
        "  else:\n",
        "    print('Session terminated automatically.')\n",
        "    time.sleep(1)\n",
        "    runtime.unassign()\n",
        "\n",
        "print(f'Zipping output and finishing up.')\n",
        "!7z a -tzip tent_detector_output.zip $OUTPUT_PATH\n",
        "\n",
        "if SAVE_TO_GOOGLE_DRIVE:\n",
        "  print('\\nSaving to Google Drive\\n')\n",
        "  !rsync -arh --progress tent_detector_output.zip $G_DRIVE_STORAGE\n",
        "if DOWNLOAD_OUTPUT:\n",
        "  print('\\nDownloading to Local Storage\\n')\n",
        "  files.download('tent_detector_output.zip')\n",
        "if PLAY_SOUND_ON_COMPLETE:\n",
        "  output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "print('Done.\\n\\n')\n",
        "if KILL_RUNTIME_ON_COMPLETE:\n",
        "  end_runtime(MINUTES_TO_KILL_RUNTIME)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Vfi33_GtEj2"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4pCOZzwsqa3kehZwAdwZY",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7df6b4b4a68146ec85f9fd92f168bf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_baea8bb66e174b14a6ca3809bda83478",
              "IPY_MODEL_662be5d39f2f455cb3ac956944513e65",
              "IPY_MODEL_c2d9dec0652243ab99c5afacd5c21384"
            ],
            "layout": "IPY_MODEL_7f8fc83b51b5415cbe6e50b35a79e15f"
          }
        },
        "baea8bb66e174b14a6ca3809bda83478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_608dde58e0ea4cab8a9a53f2ea3a719d",
            "placeholder": "​",
            "style": "IPY_MODEL_912f407d0d9b4809908cd0ab82536757",
            "value": "Tiling: 100%"
          }
        },
        "662be5d39f2f455cb3ac956944513e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07060f27ee4d42008c037fbfac998a71",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19db5e424dc44315aa970ab91790779c",
            "value": 16
          }
        },
        "c2d9dec0652243ab99c5afacd5c21384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738c25fba1f24a95a7b8e23023f0be99",
            "placeholder": "​",
            "style": "IPY_MODEL_02a1ff0c7a3a4770ad25c63fca512fdf",
            "value": " 16/16 [00:17&lt;00:00,  1.38s/it]"
          }
        },
        "7f8fc83b51b5415cbe6e50b35a79e15f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608dde58e0ea4cab8a9a53f2ea3a719d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "912f407d0d9b4809908cd0ab82536757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07060f27ee4d42008c037fbfac998a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19db5e424dc44315aa970ab91790779c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "738c25fba1f24a95a7b8e23023f0be99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02a1ff0c7a3a4770ad25c63fca512fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}