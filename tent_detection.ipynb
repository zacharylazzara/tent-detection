{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tent-detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDUrXzfAEtdq7aHpFKk59w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zacharylazzara/tent-detection/blob/main/tent_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4j6s1vEvIYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e2eba32-51bf-4b14-daed-396b9a11dcb0"
      },
      "source": [
        "%%bash\n",
        "if [ ! -d \"./unet\" ]; then\n",
        " git clone https://github.com/tofighi/sarpol-zahab-tents.git\n",
        " git clone https://github.com/zhixuhao/unet.git\n",
        "fi\n",
        "\n",
        "# For some reason line 55 of model.py needs to be replaced by model = Model(inputs, conv10)\n",
        "sed -i '55s/.*/    model = Model(inputs, conv10)/' unet/model.py\n",
        "\n",
        "mkdir unet/data/tents\n",
        "cp -r sarpol-zahab-tents/data unet/data/tents/train\n",
        "\n",
        "cd unet/data/tents/train\n",
        "\n",
        "mv images image\n",
        "mv labels label\n",
        "\n",
        "cd image\n",
        "for file in .\n",
        "do\n",
        "  rename 's/.{7}(.*)/$1/' *\n",
        "done\n",
        "\n",
        "cd ../label\n",
        "for file in .\n",
        "do\n",
        "  rename 's/.{7}(.*)/$1/' *\n",
        "done\n",
        "\n",
        "cd ..\n",
        "mkdir aug # TODO: we need to augment the data then load it as our training data\n",
        "\n",
        "\n",
        "cd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sarpol-zahab-tents'...\n",
            "Cloning into 'unet'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "id": "7j1j_koefcPv",
        "outputId": "43ea6c88-60d5-41e4-d479-68bf15fee394"
      },
      "source": [
        "%cd unet/\n",
        "# data_root = \"data/membrane/\"\n",
        "data_root = \"data/tents/\"\n",
        "\n",
        "from model import *\n",
        "from data import *\n",
        "\n",
        "data_gen_args = dict(rotation_range=0.2,\n",
        "                    width_shift_range=0.05,\n",
        "                    height_shift_range=0.05,\n",
        "                    shear_range=0.05,\n",
        "                    zoom_range=0.05,\n",
        "                    horizontal_flip=True,\n",
        "                    fill_mode='nearest')\n",
        "myGene = trainGenerator(2, data_root + 'train','image','label',data_gen_args,save_to_dir = None)\n",
        "model = unet()\n",
        "model_checkpoint = ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
        "model.fit_generator(myGene,steps_per_epoch=2000,epochs=5,callbacks=[model_checkpoint])\n",
        "\n",
        "\n",
        "imgs_train,imgs_mask_train = geneTrainNpy(data_root + \"train/aug/\",data_root + \"train/aug/\")\n",
        "model.fit(imgs_train, imgs_mask_train, batch_size=2, epochs=10, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
        "\n",
        "\n",
        "testGene = testGenerator(data_root + \"test\")\n",
        "model = unet()\n",
        "model.load_weights(\"unet_membrane.hdf5\")\n",
        "results = model.predict_generator(testGene,30,verbose=1)\n",
        "saveResult(data_root + \"test\",results)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/unet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 256 images belonging to 1 classes.\n",
            "Found 256 images belonging to 1 classes.\n",
            "Epoch 1/5\n",
            "2000/2000 [==============================] - 374s 160ms/step - loss: 0.6692 - accuracy: 0.9951\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.64575, saving model to unet_membrane.hdf5\n",
            "Epoch 2/5\n",
            "2000/2000 [==============================] - 327s 164ms/step - loss: 0.5794 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00002: loss improved from 0.64575 to 0.55897, saving model to unet_membrane.hdf5\n",
            "Epoch 3/5\n",
            "2000/2000 [==============================] - 328s 164ms/step - loss: 0.5006 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00003: loss improved from 0.55897 to 0.48259, saving model to unet_membrane.hdf5\n",
            "Epoch 4/5\n",
            "2000/2000 [==============================] - 328s 164ms/step - loss: 0.4312 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00004: loss improved from 0.48259 to 0.41525, saving model to unet_membrane.hdf5\n",
            "Epoch 5/5\n",
            "2000/2000 [==============================] - 328s 164ms/step - loss: 0.3701 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00005: loss improved from 0.41525 to 0.35616, saving model to unet_membrane.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aabee4fd817f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mimgs_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimgs_mask_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneTrainNpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train/aug/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train/aug/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_mask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m       (x, y, sample_weight), validation_data = (\n\u001b[1;32m   1094\u001b[0m           data_adapter.train_validation_split(\n\u001b[0;32m-> 1095\u001b[0;31m               (x, y, sample_weight), validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;34m\"`validation_split={validation_split}`. Either provide more data, or a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \"different value for the `validation_split` argument.\" .format(\n\u001b[0;32m-> 1465\u001b[0;31m             batch_dim=batch_dim, validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
          ]
        }
      ]
    }
  ]
}